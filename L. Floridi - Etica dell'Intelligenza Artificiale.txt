Scienza e idee


			Collana fondata da Giulio Giorello

			Consulenza scientifica

			Telmo Pievani, Corrado Sinigaglia





www.raffaellocortina.it


			Titolo originale

			The Ethics of Artificial Intelligence.

			Principles, Challenges, and Opportunities

			© Luciano Floridi, 2022

			International Rights Management:

			Susanna Lea Associates

			(la traduzione è una versione ridotta che comprende:

			Preface, Chapters 1-9, 15-16, 18-20)

			Traduzione

			Massimo Durante

			ISBN 978-88-3285-444-2

			© 2022 Raffaello Cortina Editore

			Milano, via Rossini 4

			Prima edizione: 2022





Indice


			Prefazione

			Prima parte

			Comprendere l’intelligenza artificiale

			1. Passato: l’origine dell’intelligenza artificiale

			1.1 Introduzione: la rivoluzione digitale e l’intelligenza artificiale

			1.2 Il potere di scissione del digitale: tagliare e incollare la modernità

			1.3 Nuove forme dell’agire

			1.4 ia: un ambito di ricerca in cerca di una definizione

			1.5 Conclusione: etica, governance e design

			2. Presente: ia come nuova forma dell’agire e non dell’intelligenza

			2.1 Introduzione: che cos’è l’ia? “La riconosco quando la vedo”

			2.2 ia come controfattuale

			2.3 Le due anime dell’ia: ingegneristica e cognitiva

			2.4 ia: un divorzio riuscito nell’infosfera

			2.5 L’uso umano degli esseri umani e delle interfacce

			2.6 Conclusione: chi si adatterà a chi?

			3. Futuro: lo sviluppo prevedibile dell’ia

			3.1 Introduzione: scrutare nei semi del tempo

			3.2 Dati storici, ibridi e sintetici e il bisogno di ludicizzazione

			3.3 Problemi difficili, problemi complessi e il bisogno di avvolgimento

			3.4 Il design come futuro dell’ia

			3.5 Conclusione: l’ia e le sue stagioni

			Seconda parte

			Valutare l’intelligenza artificiale

			4. Un quadro unificato di principi etici per l’ia

			4.1 Introduzione: troppi principi?

			4.2 Un quadro unificato di cinque principi per l’ia etica

			4.3 Beneficenza: promuovere il benessere, preservare la dignità e sostenere il pianeta

			4.4 Non maleficenza: privacy, sicurezza e “cautela della capacità”

			4.5 Autonomia: il potere di “decidere di decidere”

			4.6 Giustizia: promuovere la prosperità, preservare la solidarietà, evitare l’iniquità

			4.7 Esplicabilità: rendere possibili gli altri principi tramite l’intelligibilità e la responsabilità

			4.8 Una visione sinottica

			4.9 L’etica dell’ia: da dove e per chi?

			4.10 Conclusione: dai principi alle pratiche

			5. Dai principi alle pratiche: i rischi di comportamenti contrari all’etica

			5.1 Introduzione: traduzioni rischiose

			5.2 Lo shopping etico

			5.3 Il “bluewashing” etico

			5.4 Il lobbismo etico

			5.5 Il dumping etico

			5.6 L’elusione dell’etica

			5.7 Conclusione: l’importanza di conoscere meglio

			6. Etica soft e governance dell’ia

			6.1 Introduzione: dall’innovazione digitale alla governance del digitale

			6.2 Etica, regolazione e governance

			6.3 La compliance nei confronti delle norme: necessaria ma insufficiente

			6.4 Etica hard e soft

			6.5 L’etica soft come quadro etico

			6.6 Analisi dell’impatto etico

			6.7 Preferibilità digitale e cascata normativa

			6.8 Il duplice vantaggio dell’etica digitale

			6.9 Conclusione: l’etica come strategia

			7. La mappatura dell’etica degli algoritmi

			7.1 Introduzione: una definizione operativa di algoritmo

			7.2 La mappa dell’etica degli algoritmi

			7.3 Prove inconcludenti che portano ad azioni ingiustificate

			7.4 Prove imperscrutabili che portano all’opacità

			7.5 Prove fuorvianti che portano a pregiudizi (bias) non voluti

			7.6 Risultati ingiusti che portano alla discriminazione

			7.7 Effetti trasformativi che sollevano sfide per l’autonomia e la privacy informativa

			7.8 Tracciabilità come presupposto della responsabilità morale

			7.9 Conclusione: l’uso buono e cattivo degli algoritmi

			8. Cattive pratiche: l’uso improprio dell’ia per il male sociale

			8.1 Introduzione: l’uso criminale dell’ia

			8.2 Preoccupazioni

			8.3 Minacce

			8.4 Possibili soluzioni

			8.5 Sviluppi futuri

			8.6 Conclusione: dagli usi malvagi dell’ia all’ia socialmente buona

			9. Buone pratiche: l’uso dell’ia per il bene sociale

			9.1 Introduzione: l’idea di ia per il bene sociale

			9.2 Una definizione di ai4sg

			9.3 Sette fattori essenziali per il successo dell’ai4sg

			9.4 Conclusione: fattori di bilanciamento per l’ia per il bene sociale

			10. Macchine ultraintelligenti, singolarità e altre distrazioni fantascientifiche

			10.1 Introduzione: l’aggiornata paura ancestrale dei mostri

			10.2 Credenti e miscredenti nella vera ia: un dibattito sulla fede

			10.3 Gli adepti della singolarità: la fine è vicina, la vera ia sta arrivando

			10.4 ia-teismo dell’ia: quello che i computer non possono fare, presumibilmente

			10.5 Adepti della singolarità e atei dell’ia: una diatriba inutile

			10.6 Conclusione: il problema non è hal ma l’umanità nel suo complesso

			11. La società per la buona ia

			11.1 Introduzione: quattro modi per realizzare una società per la buona ia

			11.2 Chi possiamo diventare: rendere possibile l’umana realizzazione di sé, senza svalutare le capacità umane

			11.3 Cosa possiamo fare: migliorare l’agire umano, senza rimuovere la responsabilità umana

			11.4 Cosa possiamo conseguire: incrementare le capacità della società, senza ridurre il controllo umano

			11.5 Come possiamo interagire: coltivare la coesione sociale, senza erodere l’autodeterminazione umana

			11.6 Raccomandazioni per una società della buona ia

			11.7 Conclusione: la necessità di politiche concrete e costruttive

			12. Il gambetto: l’impatto dell’ia sul cambiamento climatico

			12.1 Introduzione: il potere duplice dell’ia

			12.2 L’ia e le “transizioni gemelle” dell’Unione Europea

			12.3 ai e cambiamento climatico: sfide etiche

			12.4 ia e cambiamento climatico: l’impronta ecologica

			12.5 Tredici raccomandazioni a favore dell’ia contro il cambiamento climatico

			12.6 Conclusione: una società più sostenibile e una biosfera più sana

			13. L’ia e gli obiettivi di sviluppo sostenibile delle Nazioni Unite

			13.1 L’ia per il bene sociale e gli obiettivi di sviluppo sostenibile delle Nazioni Unite

			13.2 Valutare le evidenze dell’iaxoss

			13.3 L’ia per promuovere l’“azione per il clima”

			13.4 Conclusione: un programma di ricerca per l’iaxoss

			14. Conclusione: il verde e il blu

			14.1 Introduzione: dal divorzio tra agire e intelligenza al matrimonio tra il verde e il blu

			14.2 Il ruolo della filosofia come design concettuale

			14.3 Il bellissimo errore di natura

			Ringraziamenti

			Bibliografia





		 			Prefazione

			Istruzione, affari e industria, viaggi e logistica, banche, vendita al dettaglio e shopping, intrattenimento, welfare e sanità, politica e relazioni sociali, in breve la vita stessa per come la conosciamo oggi è diventata inconcepibile senza la presenza di pratiche, prodotti, servizi e tecnologie digitali. Chiunque non sia stupito di fronte a una tale rivoluzione digitale non ne ha afferrato la portata. Stiamo parlando di un nuovo capitolo della storia umana. Naturalmente, molti altri capitoli l’hanno preceduto. Erano tutti ugualmente significativi. L’umanità ha sperimentato un mondo prima e dopo la ruota, la lavorazione del ferro, l’alfabeto, la stampa, il motore, l’elettricità, la televisione o il telefono. Ogni trasformazione è unica. Alcune di queste hanno cambiato in maniera irreversibile il modo in cui comprendiamo noi stessi, la nostra realtà e l’esperienza che ne facciamo, con implicazioni complesse e di lungo periodo. Stiamo ancora scoprendo nuovi modi per sfruttare la ruota, basti pensare alla ghiera cliccabile dell’iPod. Al contempo, è inimmaginabile ciò che l’umanità potrà ottenere grazie alle tecnologie digitali. Nessuno nel 1964 (vedi capitolo 1) avrebbe potuto immaginare come sarebbe stato il mondo solo cinquant’anni dopo. I futurologi sono i nuovi astrologi. Eppure, è anche vero che la rivoluzione digitale accade una volta sola, e cioè adesso. Questa particolare pagina della storia umana è stata voltata ed è iniziato un nuovo capitolo. Le generazioni future non sapranno mai com’era una realtà esclusivamente analogica, offline, predigitale. Siamo l’ultima generazione che l’avrà vissuta.

			Il prezzo di un posto così speciale nella storia lo si paga con incertezze che destano preoccupazioni. Le trasformazioni indotte dalle tecnologie digitali sono sorprendenti. Giustificano un po’ di confusione e di apprensione. Basta guardare i titoli dei giornali. Tuttavia, il nostro posto speciale in questo spartiacque storico, tra una realtà completamente analogica e una sempre più digitale, porta con sé anche straordinarie opportunità. Proprio perché la rivoluzione digitale è appena iniziata, abbiamo la possibilità di plasmarla in modi positivi che possono fare progredire sia l’umanità sia il nostro pianeta. Come disse una volta Winston Churchill, “prima siamo noi a dare forma agli edifici; poi sono questi a dare forma a noi”. Siamo nella primissima fase di costruzione delle nostre realtà digitali. Possiamo costruirle bene, prima che inizino a influenzare e modellare noi e le generazioni future nel modo sbagliato. La discussione sul bicchiere mezzo vuoto o mezzo pieno è inutile perché la questione davvero interessante è come possiamo riempirlo.

			Per individuare la strada migliore da percorrere nello sviluppo delle nostre tecnologie digitali, il primo, fondamentale passo è cercare di averne una maggiore e migliore comprensione. Non dovremmo sonnecchiare nella creazione di un mondo sempre più digitale. L’insonnia della ragione è vitale, perché il suo sonno genera errori mostruosi. Comprendere le trasformazioni tecnologiche in atto sotto i nostri occhi è cruciale, se vogliamo guidare la rivoluzione digitale in una direzione che sia preferibile (equa) dal punto di vista sociale e sostenibile da quello ambientale. Ciò può tradursi solo in uno sforzo collaborativo. Pertanto, in questo libro, offro il mio contributo condividendo alcune idee su un particolare tipo di tecnologia digitale, l’intelligenza artificiale (ia), e un problema specifico, la sua etica.

			Il libro fa parte di un più ampio progetto di ricerca, che investe le trasformazioni della capacità di agire indotte dalla rivoluzione digitale. Inizialmente avevo pensato di poter lavorare sia sull’intelligenza artificiale – intesa come forma dell’agire artificiale, il tema di questo libro – sia sull’agire politico, inteso come forma dell’agire collettivo sostenuta e influenzata dalle interazioni digitali. In effetti, quando sono stato invitato nel 2018 a tenere le Ryle Lectures, ho tentato di fare proprio questo, affrontando entrambi gli argomenti come due aspetti di una medesima e più profonda trasformazione. Organizzatori e partecipanti hanno detto (forse per gentilezza) che l’esperimento non era fallito. Personalmente, non l’ho trovato un grande successo. Non perché approcciarsi all’etica dell’ia e alla politica dell’informazione da un unico punto di vista, basato sulla capacità di agire, sia un errore, ma perché tale approccio funziona bene solo se si è disposti a rinunciare ai dettagli e a scambiare la profondità con l’ampiezza. Questo può andare bene in una serie di conferenze, ma trattare entrambi i temi in un’unica monografia di ricerca avrebbe prodotto un fermaporta di fascino ancora inferiore a quello che potrebbe avere questo libro. Perciò, ho deciso di dividere il progetto in due parti, questo libro sull’etica dell’ia, e il prossimo, sulla politica dell’informazione.

			Possiamo, ora, dare un rapido sguardo ai suoi contenuti. Il compito di questo volume è quello di contribuire allo sviluppo di una filosofia del nostro tempo per il nostro tempo, come ho scritto più volte. Tale sforzo è attuato in modo sistematico (l’architettura concettuale è una caratteristica preziosa del pensiero filosofico) piuttosto che in modo esauriente, perseguendo due obiettivi.

			Il primo obiettivo è metateorico ed è soddisfatto dalla prima parte. Questa comprende i primi tre capitoli, in cui offro un’interpretazione del passato (capitolo 1), del presente (capitolo 2) e del futuro dell’ia (capitolo 3). Questa prima parte non è un’introduzione all’ia in senso tecnico, o una sorta di ia per principianti. Ci sono tanti ottimi libri a questo scopo. È piuttosto un’interpretazione filosofica dell’ia come tecnologia. La tesi principale che sviluppo nel libro consiste nel dire che l’ia costituisce un divorzio senza precedenti tra l’intelligenza e la capacità di agire.

			Sulla base di questa prima parte, la seconda svolge una disamina non metateorica ma teorica delle conseguenze di tale divorzio analizzate nella prima parte.

			Nel capitolo 4, offro una prospettiva unificata sui molti principi che sono stati proposti per inquadrare l’etica dell’ia. Ciò porta a esaminare, nel capitolo 5, i potenziali rischi che possono pregiudicare l’applicazione di tali principi e quindi, nel capitolo 6, a un’analisi della relazione tra principi etici e norme giuridiche e alla definizione di etica soft come etica post-compliance. Dopo questi tre capitoli, analizzo le sfide etiche poste dallo sviluppo e dall’uso dell’ia nel capitolo 7, gli usi cattivi dell’ia nel capitolo 8 e le buone pratiche nell’applicazione dell’ia nel capitolo 9. L’ultimo gruppo di capitoli è dedicato a una serie di questioni rilevanti dal punto di vista del dibattito etico sull’ia e, in particolare, del design, sviluppo e implementazione dell’ia per il bene sociale. Pertanto, nel capitolo 10, getto luce su una questione attuale ma fuorviante, quella della singolarità. Nel capitolo 11, esamino più da vicino la natura e le caratteristiche dell’ia per il bene sociale. Nel capitolo 12, ricostruisco l’impatto positivo e negativo che l’ia ha sull’ambiente e in che modo l’ia può essere una forza positiva nella lotta ai cambiamenti climatici, ma non senza rischi e costi, che devono essere evitati o minimizzati. Nel capitolo 13, approfondisco l’analisi presentata nel capitolo 9 e discuto la possibilità di avvalersi dell’ia a sostegno dei 17 obiettivi di sviluppo sostenibile delle Nazioni Unite. In questo contesto, presento l’iniziativa di ricerca dell’Università di Oxford sull’ia per gli obiettivi di sviluppo sostenibile che ho diretto a Oxford. Infine, nel capitolo 14, concludo sostenendo l’esigenza di un nuovo matrimonio tra il verde di tutti i nostri habitat e il blu di tutte le nostre tecnologie digitali, per sostenere e sviluppare una società migliore e una biosfera più sana. Il libro si chiude con alcuni richiami a concetti che saranno centrali nel prossimo libro, La politica dell’informazione, dedicato, come accennato sopra, all’impatto delle tecnologie digitali sull’agire sociopolitico.

			Tutti i capitoli sono strettamente correlati, per cui ho aggiunto riferimenti interni ogni volta che risultava utile. I capitoli potrebbero essere letti in un ordine leggermente diverso, come ha osservato uno dei revisori anonimi. Sono d’accordo. Ho creato la struttura che mi sembrava più utile, ma alcuni lettori potrebbero, per esempio, voler leggere insieme i capitoli 9-12-13.

			In termini di radici filosofiche, come per altri libri in precedenza, anche questo è un libro tedesco, scritto da una prospettiva post-analitico-continentale, che ho l’impressione stia svanendo. Il lettore più attento collocherà facilmente quest’opera nella tradizione che collega il pragmatismo, in particolare Charles Sanders Peirce, con la filosofia della tecnologia, in particolare Herbert Simon.1 Questo volume è meno neokantiano, platonico e cartesiano di quanto mi aspettassi. In sintesi, scriverlo mi ha reso consapevole che sto uscendo dal cono d’ombra dei miei tre eroi filosofici. Non c’è niente di programmatico, ma questo è ciò che accade quando segui il tuo ragionamento ovunque ti porti. Amici Plato, Cartesius et Kant, sed magis amica veritas. In The Ethics of Information ho osservato che “alcuni libri scrivono i loro autori”. Adesso ho l’impressione che soltanto i brutti libri siano totalmente controllati dai loro autori. Si chiamano romanzi d’aeroporto e telenovele.

			La differenza principale rispetto ai libri passati è che ora sono sempre più convinto che la filosofia sia nella sua forma migliore design concettuale, e il design concettuale offre progetti mirati – comprendere il mondo per migliorarlo – e semantizzazione – dare senso e significato all’Essere, e prendersi cura del capitale semantico dell’umanità. Tutto è iniziato rendendosi conto di qualcosa di ovvio, in un caso specifico riguardante un celeberrimo filosofo di Oxford: la vera eredità di Locke è il suo pensiero politico, non la sua epistemologia. Forse Kant non voleva indurci a credere che l’epistemologia e l’ontologia fossero le regine del regno filosofico, ma è così che sono stato educato a pensare la filosofia moderna. E forse né Wittgenstein né Heidegger pensavano che la logica, il linguaggio e la sua filosofia dovessero sostituire le due regine come loro unici eredi legittimi, ma è così che sono stato anche educato a pensare la filosofia contemporanea. A ogni modo, oggi non metto più al centro dell’impresa filosofica queste discipline, ma piuttosto l’etica, la filosofia politica e del diritto. Ricerca, comprensione, formazione, realizzazione e negoziazione di ciò che è moralmente buono e giusto sono il nucleo della riflessione filosofica. Tutto il resto è il viaggio necessario per raggiungere un luogo simile, ma non va confuso con esso.

			Riguardo allo stile e alla struttura di questo libro, ripeto qui quanto ho scritto nella prefazione ai miei precedenti libri. Resto dolorosamente consapevole che anche questo non è un libro che si legge tutto d’un fiato, per utilizzare un eufemismo, nonostante i miei tentativi di renderlo il più interessante e di facile lettura possibile. Resto convinto che la ricerca esoterica (in senso tecnico) in filosofia sia l’unico modo per sviluppare nuove idee. Ma la filosofia essoterica ha il suo posto cruciale. È come la punta più accessibile e visibile rispetto alla parte più oscura e tuttavia necessaria dell’iceberg sotto la superficie della vita quotidiana. Il lettore interessato a una lettura molto più leggera potrebbe fare riferimento a La quarta rivoluzione. Come l’infosfera sta trasformando il mondo (Floridi, 2014a) o forse al testo ancora più accessibile Information: A Very Short Introduction (Floridi, 2010b).

			Anche questo libro richiede non solo una conoscenza di livello universitario della filosofia, un po’ di pazienza e del tempo, ma pure una mente aperta. Queste ultime tre sono risorse scarse. Negli ultimi due decenni circa di dibattiti, mi sono reso pienamente conto, talora attraverso confronti molto meno amichevoli di quanto avrei desiderato, che talune delle idee che sostengo in questo e nei volumi precedenti sono controverse. Negli ultimi anni ho anche notato lo scetticismo dei colleghi riguardo alla possibilità che qualcuno possa costruire una ricerca che si estende al di là di un unico, circoscritto tema di specializzazione, come se si potesse fare buona filosofia solo se questa diviene scolastica. Ricordo un incontro in cui un collega chiese, con visibile scetticismo, se qualcuno potesse affrontare più di una manciata di temi quando si fa ricerca filosofica. Non sono d’accordo. Sono grato, però, a tutte le persone che mi hanno più volte sfidato, per avermi fatto capire che preferisco essere un esploratore piuttosto che un colonizzatore. Il loro scetticismo è stato rinvigorente.

			Mi sono reso conto anche di quanto spesso si commettano errori facendo affidamento su “attrattori sistemici”: se una nuova idea assomiglia un po’ a una vecchia idea che già possediamo, allora quella vecchia idea diventa una calamita da cui la nuova è fortemente attratta, in modo pressoché irresistibile. Finiamo per pensare che “il nuovo” sia proprio come “il vecchio”, e se non ci piace “quel vecchio” allora non ci piace neanche “quel nuovo”. Cattiva filosofia indubbiamente, ma ci vogliono esercizio e forza mentale per resistere a un cambiamento così forte. Nel caso di questo libro, temo che alcuni lettori possano essere tentati dall’idea che si tratti di un libro contro la tecnologia o di un libro in cui indico i limiti dell’ia, ciò che “l’ia non può fare”, o addirittura dall’idea opposta, per cui questo libro sarebbe troppo ottimista riguardo alla tecnologia, troppo innamorato della rivoluzione digitale e dell’intelligenza artificiale come se fosse una sorta di panacea. Hanno torto entrambi. Il libro è un tentativo di stare nel mezzo, né l’inferno né il paradiso, ma il laborioso purgatorio degli sforzi umani. Naturalmente, sarei deluso nel sentirmi dire che il mio tentativo è fallito, ma sarei frustrato se il tentativo dovesse essere frainteso. Ci sono molti modi di comprendere la tecnologia: uno di questi consiste nel concepirla in termini di buon design e governance etica. In realtà credo che sia l’approccio migliore. Il lettore non deve concordare con me su tutto. Ma non vorrei essere frainteso sulla direzione che sto prendendo.

			Ho pensato che due caratteristiche possono aiutare il lettore ad accedere più facilmente ai contenuti di questo libro: i sommari e le conclusioni all’inizio e alla fine di ciascun capitolo, e qualche ridondanza. Per quanto riguarda la prima caratteristica, so che è poco ortodossa, ma la soluzione, già adottata in altri volumi, di iniziare ogni capitolo con un “In precedenza nel capitolo x…” dovrebbe consentire al lettore di navigare nel testo o passare a capitoli successivi senza perderne la trama essenziale.

			Per quanto riguarda la seconda caratteristica, in fase di redazione della versione finale del libro ho deciso di lasciare nei capitoli alcune ripetizioni e qualche riformulazione di temi ricorrenti, laddove ritenevo che il punto in cui era stato introdotto il contenuto originale fosse troppo distante, in termini di pagine o di contesto teorico. Se talora il lettore sperimenta un senso di déjà vu, spero che ciò vada a vantaggio della chiarezza, in quanto caratteristica saliente e non in quanto difetto.

			Un’ultima breve considerazione su quello che il lettore non troverà nelle pagine seguenti. Non si tratta di un’introduzione all’ia o all’etica dell’ia. Né cerco di fornire un esame esaustivo di tutte le questioni che possono essere rubricate sotto l’etichetta di “etica dell’ia”. Vi sono temi che saranno investigati e approfonditi in futuro, perché ancora poco esplorati; e altri, che investono considerazioni geopolitiche sulle politiche dell’ia, che intendo affrontare in termini più specifici in La politica dell’informazione. Il lettore interessato potrebbe voler vedere Cath e coautori (2018) sugli approcci all’ia di Stati Uniti, Unione Europea e Regno Unito, e Roberts e coautori (2021) sulla Cina, per esempio. Non è neppure un libro sugli aspetti statistici e computazionali dei temi cosiddetti fat (fairness, accountability, and transparency) o xai (explainable ai) o sulla normativa che li concerne. Questi sono tutti argomenti solo accennati nei capitoli che seguono (per maggiori informazioni al loro riguardo si vedano Watson, Floridi, 2020; Lee, Floridi, 2020; Lee, Floridi, Denev, 2020). Si tratta di un libro filosofico sulle radici di alcuni dei problemi digitali del nostro tempo, non sulle loro foglie. Un libro che investe e mette a tema una nuova forma dell’agire, la sua natura, la sua portata e le sue sfide, e il modo in cui sfruttarla a beneficio dell’umanità e dell’ambiente.



* * *





			 				 					1. Il lettore interessato ad approfondire queste connessioni può consultare Allo (2010), Demir (2012) e Durante (2017).





prima Parte


			Comprendere l’intelligenza artificiale





		 			Questa prima parte del libro può essere letta come una breve introduzione filosofica al passato, presente e futuro dell’intelligenza artificiale (ia). Si compone di tre capitoli. Unitariamente considerati, forniscono la cornice concettuale necessaria per comprendere la seconda parte, dove affronterò alcune delle più urgenti questioni etiche sollevate dall’ia. Nel primo capitolo, ricostruisco l’origine dell’ia, non da un punto di vista storico o tecnologico, ma da un punto di vista concettuale, concentrando l’attenzione sulle trasformazioni che hanno condotto ai sistemi di ia che usiamo oggi. Nel secondo capitolo, articolo un’interpretazione dell’ia contemporanea in termini di riserva di capacità di agire, che è stata resa possibile da due fattori: 1) il divorzio tra la capacità di risolvere problemi e di portare a termine con successo compiti, al fine di raggiungere un obiettivo, e la necessità di essere intelligenti nel farlo; e 2) la progressiva trasformazione del nostro ambiente in un’infosfera adattata all’ia, che rende tale divorzio non solo possibile ma anche efficace. Nel terzo capitolo, completerò questa prima parte osservando i possibili sviluppi dell’ia nel prossimo futuro, ancora una volta non dal punto di vista tecnico o tecnologico, ma da quello concettuale, in relazione alle tipologie preferibili di dati richiesti dai sistemi di ia e di problemi che l’ia è più facilmente in grado di risolvere.





1


			Passato: l’origine dell’intelligenza artificiale

			Sommario Nel primo paragrafo, intendo fornire una rapida panoramica di come gli sviluppi della digitalizzazione abbiano creato le condizioni per l’attuale diffusione e per il successo dei sistemi di ia. Nel secondo paragrafo, sostengo che l’impatto dirompente di tecnologie, scienze, pratiche, prodotti e servizi digitali – in breve, del digitale – sia dovuto alla loro capacità di scindere e ricomporre realtà e idee che abbiamo ereditato dalla modernità. Definisco questo aspetto potere di scissione del digitale. Tale potere di scissione è illustrato tramite esempi concreti ed è utilizzato per interpretare l’ia come una nuova forma di agire intelligente, determinata dal disallineamento digitale tra azione e intelligenza, un fenomeno senza precedenti che ha causato alcuni malintesi. Nel terzo paragrafo, presento una breve disamina dell’agire politico, che costituisce l’altra importante tipologia di azione che è stata trasformata dal potere di scissione del digitale, e spiego brevemente perché tale aspetto è importante e pertinente in questo contesto d’esame, sebbene in generale ecceda lo scopo del libro. Nel quarto paragrafo, torno sul tema principale di un’interpretazione concettuale dell’ia e introduco il secondo capitolo, ricordando al lettore quanto risulti difficile definire e tratteggiare con esattezza cosa sia precisamente l’ia e cosa conti come ia. Nell’ultimo paragrafo, sostengo che il design sia la controparte del potere di scissione del digitale e anticipo alcuni dei temi discussi nella seconda parte del libro.





1.1 Introduzione: la rivoluzione digitale e l’intelligenza artificiale


			Nel 1964, la Paramount Pictures distribuì Robinson Crusoe su Marte. Il film descriveva le avventure del comandante Christopher “Kit” Draper (Paul Mantee), un astronauta statunitense naufragato su Marte. Se lo guardiamo su YouTube anche solo per pochi minuti, ci rendiamo conto di quanto radicalmente sia cambiato il mondo in pochi decenni. In particolare, il computer che compare all’inizio del film sembra un motore di epoca vittoriana, con leve, ingranaggi e quadranti. Un pezzo di archeologia che avrebbe potuto usare il dottor Frankenstein. Eppure, verso la fine della storia, Friday (Victor Lundin) viene rintracciato da un’astronave aliena attraverso i suoi braccialetti. Un pezzo di futurologia che sembra inquietantemente preveggente.

			Robinson Crusoe su Marte apparteneva a un’epoca diversa, tecnologicamente e culturalmente più vicina al secolo scorso che al nostro. Descrive una realtà moderna, non contemporanea, basata sull’hardware e non sul software. Computer portatili, Internet, servizi web, touch screen, smartphone, orologi intelligenti, social media, shopping online, video e musica in streaming, automobili a guida autonoma, tosaerba robotizzati o assistenti virtuali non esistono ancora. L’intelligenza artificiale è più un progetto che una realtà. Il film mostra una tecnologia fatta di dadi, bulloni e meccanismi che seguono le goffe leggi della fisica newtoniana. È una realtà del tutto analogica, basata sugli atomi piuttosto che sui byte, di cui i Millennials non hanno esperienza, essendo nati dopo i primi anni Ottanta. Per loro un mondo senza tecnologie digitali assomiglia a ciò che era per me (nato nel 1964) un mondo senza automobili: qualcosa di cui ho sentito parlare solo da mia nonna.

			Si fa spesso osservare che uno smartphone racchiude molta più potenza di calcolo in pochissimo spazio di quanto la nasa potesse mettere insieme quando Armstrong atterrò sulla Luna cinque anni dopo Robinson Crusoe su Marte, nel 1969, e tutto questo a un costo pressoché trascurabile. Molti articoli hanno tracciato alcuni raffronti precisi nel 2019, per il cinquantesimo anniversario dello sbarco sulla Luna, facendo emergere alcuni fatti sorprendenti. L’Apollo Guidance Computer (agc) a bordo dell’Apollo 11 aveva 32.768 bit di ram (random access memory) e 589.824 bit (72 kb) di rom (read only memory): uno spazio di memoria su cui non saremmo stati in grado di salvare questo libro. Cinquant’anni dopo, un cellulare possiede, in media, 4 gb di ram e 512 gb di rom: vale a dire, un milione di volte in più di ram e sette milioni di volte in più di rom. Per quanto riguarda il processore, l’agc funzionava a 0,043 mhz. Un processore per iPhone funziona, in media, a circa 2490 mhz: vuol dire che è circa 58.000 volte più veloce. Per afferrare meglio il senso di questa accelerazione, forse un paragone può esserci di aiuto. In media, una persona cammina alla velocità di 5 km/h. Oggi, alcuni jet supersonici possono viaggiare alla velocità di 6100 km/h, vale a dire a una velocità più di cinque volte superiore a quella del suono (1235 km/h). Eppure è soltanto poco più di mille volte più veloce che camminare. Immaginiamo di moltiplicarlo per 58.000.

			Dove sono andate a finire tutta questa velocità e questa potenza di calcolo? La risposta è duplice: fattibilità e usabilità. Possiamo fare sempre di più, in termini di applicazioni, e possiamo farlo in modi sempre più semplici, non solo per ciò che concerne la programmazione, ma soprattutto per ciò che riguarda l’esperienza dell’utente. I video, per esempio, sono avidi di potenza di calcolo, così come i sistemi operativi. L’ia oggi è possibile anche perché abbiamo la potenza di calcolo necessaria per far funzionare il suo software.

			Grazie a questa crescita sbalorditiva delle capacità di archiviazione ed elaborazione, a costi sempre più contenuti, oggi miliardi di persone sono connessi e trascorrono molte ore online ogni giorno. Nel Regno Unito, per esempio, “nel 2018, il tempo medio trascorso utilizzando Internet è stato di 25,3 ore a settimana. Si è trattato di un incremento di 15,4 ore rispetto al 2005”.1 Ciò è tutt’altro che insolito. E la pandemia ha generato un incremento ulteriore. Tornerò su questo punto nel prossimo capitolo, ma un altro dei motivi per cui l’ia è possibile oggi è proprio il fatto che noi esseri umani trascorriamo sempre più tempo in contesti digitali e, pertanto, adattati all’ia.

			Più memoria, più velocità e più ambienti e interazioni digitali hanno generato più dati, in quantità immense. Tutti abbiamo osservato diagrammi con curve esponenziali, che indicavano quantità che non sappiamo neppure come rappresentare. Secondo la società di analisi di mercato idc,2 nel 2018 abbiamo raggiunto 18 zettabyte di dati creati, catturati o riprodotti, e questa sorprendente crescita di dati non mostra segni di rallentamento: a quanto pare, diventeranno 175 zettabyte nel 2025. Si tratta di un aspetto difficile da cogliere in termini di quantità, ma due conseguenze meritano un momento di riflessione (sono entrambe esaminate in Floridi, 2014a). La velocità e la memoria delle nostre tecnologie digitali non crescono alla stessa velocità dell’universo dei dati. Ne consegue che stiamo rapidamente passando da una cultura della registrazione a una della cancellazione: la questione non è più che cosa salvare ma che cosa eliminare per fare spazio ai nuovi dati. La maggior parte dei dati disponibili è stata creata a partire dagli anni Novanta, anche se contiamo ogni parola pronunciata, scritta o stampata nella storia dell’umanità e ogni biblioteca o archivio mai esistiti. Per rendersene conto, basta osservare uno dei tanti diagrammi disponibili online che illustrano l’esplosione dei dati: l’aspetto sorprendente non sta solo sulla parte destra, dove la freccia della crescita sale, ma anche sulla parte sinistra, dove la crescita ha avuto inizio: si tratta solo di una manciata di anni fa. Poiché tutti questi dati sono stati creati dalla generazione attuale, stanno anche invecchiando insieme, in termini di supporti e tecnologie obsolete. Per questo motivo la loro conservazione costituirà una questione sempre più urgente.

			Una maggiore potenza di calcolo e una maggiore quantità di dati hanno reso possibile il passaggio dalla logica alla statistica. Le reti neurali che erano interessanti solo da un punto di vista teorico (ne ho discusse alcune in un libro precedente già alla fine degli anni novanta: Floridi, 1999) sono diventate strumenti ordinari nell’ambito dell’apprendimento automatico. La vecchia ia era per lo più simbolica e poteva essere interpretata come una branca della logica matematica, ma la nuova ia è principalmente connessionista e potrebbe essere interpretata come una branca della statistica. Il principale cavallo di battaglia dell’ia non è più la deduzione logica ma l’inferenza e la correlazione statistica.

			La potenza e la velocità di calcolo, le dimensioni della memoria, la quantità di dati, i potenti algoritmi, gli strumenti statistici e le interazioni online sono tutti fattori che stanno crescendo in modo incredibilmente rapido. Ciò accade anche perché (e la relazione causale procede in entrambe le direzioni) il numero di dispositivi digitali che interagiscono tra loro è già notevolmente superiore alla popolazione umana. Perciò, la maggior parte delle comunicazioni avviene da macchina a macchina, senza coinvolgimento umano. Ci sono robot computerizzati su Marte controllati a distanza dalla Terra. Il comandante Christopher “Kit” Draper li avrebbe trovati assolutamente fantastici.

			Tutte le tendenze precedenti continueranno a crescere, inarrestabilmente, nel prossimo futuro. Queste tendenze hanno modificato il modo in cui impariamo, giochiamo, lavoriamo, amiamo, odiamo, scegliamo, decidiamo, produciamo, vendiamo, compriamo, consumiamo, pubblicizziamo, ci divertiamo, ci preoccupiamo di qualcosa e ce ne prendiamo cura, socializziamo, comunichiamo e così via. Sembra impossibile trovare un qualsiasi aspetto della nostra vita che non sia stato influenzato dalla rivoluzione digitale. Nell’ultimo mezzo secolo circa, la nostra realtà è diventata sempre più digitale, fatta di zero e uno, gestita da software e dati, piuttosto che da hardware e atomi. Un numero crescente di persone vive sempre più diffusamente onlife, sia online sia offline, e nell’infosfera, sia digitalmente sia analogicamente. Questa rivoluzione digitale ha anche influito sul modo in cui concepiamo e comprendiamo le nostre realtà, che sono sempre più interpretate in termini computazionali e digitali. Basti pensare alla “vecchia” analogia tra il nostro dna e il nostro “codice”, che ora diamo per scontata. Tutto ciò ha anche alimentato lo sviluppo dell’ia, dal momento che condividiamo le nostre esperienze onlife e gli ambiti dell’infosfera con agenti artificiali, siano essi algoritmi, bot o robot. Per capire in che cosa effettivamente consista l’ia – la mia tesi è che si tratti di una nuova forma di agire, e non di intelligenza – occorre dunque dire qualcosa di più sull’impatto della rivoluzione digitale: è quanto mi propongo di fare nella parte restante di questo capitolo. È solo comprendendo la traiettoria concettuale delle sue implicazioni che possiamo avere una corretta prospettiva sulla natura dell’ia (secondo capitolo), i suoi probabili sviluppi (terzo capitolo) e le sue sfide etiche (seconda parte).





1.2 Il potere di scissione del digitale: tagliare e incollare la modernità


			Le tecnologie, le scienze, le pratiche, i prodotti e i servizi digitali, in breve il digitale come fenomeno globale sta profondamente trasformando la realtà. Tutto questo è piuttosto ovvio e pacifico. Le vere domande consistono casomai nel chiedersi perché, come e con quali conseguenze, soprattutto in relazione all’ia. In ciascuno di questi casi, la risposta è tutt’altro che banale e sicuramente aperta al dibattito. Per chiarire quali siano le risposte che ritengo più convincenti e introdurre nel prossimo capitolo un’interpretazione dell’ia come una riserva crescente di capacità di agire intelligente, conviene entrare in medias res e partire dal “come”. Diventerà allora più facile fare un passo indietro, per comprendere il “perché”, e poi procedere in avanti, per affrontare le “conseguenze” e collegare le risposte all’emergere dell’ia.

			Il digitale “taglia e incolla” le nostre realtà sia ontologicamente sia epistemologicamente. Con questo intendo dire che incolla, scolla o rincolla certi aspetti del mondo – e quindi le nostre corrispondenti ipotesi su di essi – che pensavamo fossero immutabili. Separa e riunisce, per così dire, gli “atomi” delle nostre esperienza e cultura “moderne”. Modifica il letto del fiume, per usare la metafora di Wittgenstein. Alcuni esempi lampanti possono chiarire l’idea in modo più diretto.

			Consideriamo, anzitutto, uno dei casi più significativi di incollamento. La nostra identità e i nostri dati personali non sono mai stati incollati insieme così indistinguibilmente come accade oggi, allorché si parla di identità personale dei soggetti interessati.3 I conteggi dei censimenti sono molto datati (Alterman, 1969). L’invenzione della fotografia ha avuto un forte impatto sulla privacy (Warren, Brandeis, 1890). I governi europei hanno reso obbligatorio viaggiare con il passaporto durante la Prima guerra mondiale, per motivi di migrazione e sicurezza, estendendo così il controllo dello Stato sui mezzi di mobilità (Torpey, 2000). Ma è soltanto il digitale, con il suo enorme potere di registrare, monitorare, condividere e processare quantità illimitate di dati su Alice, che ha saldato insieme chi è Alice, la sua identità e il suo profilo individuali, con le informazioni personali su di lei. La privacy è diventata una questione urgente anche, se non principalmente, a causa di tale incollamento, e oggi, almeno nella normativa dell’Unione Europea, la protezione dei dati personali è discussa in termini di dignità umana (Floridi, 2016c) e identità personale (Floridi, 2005a, 2006), con i cittadini descritti come soggetti interessati.

			L’esempio successivo riguarda la posizione e la presenza, e il loro scollamento. In un mondo digitale, è ovvio che uno può trovarsi fisicamente in un posto, diciamo un bar, ed essere presente interattivamente in un altro, diciamo una pagina su Facebook. Eppure tutte le generazioni passate che vivevano in un mondo esclusivamente analogico hanno concepito e sperimentato posizione e presenza come due lati inseparabili della stessa situazione umana: l’essere situati nello spazio e nel tempo, qui e ora. L’azione a distanza e la telepresenza appartenevano a mondi magici o alla fantascienza. Oggi, questo scollamento è un semplice tratto dell’esperienza comune in qualsiasi società dell’informazione (Floridi, 2005b). Siamo la prima generazione per la quale “Dove sei?” non è soltanto una domanda retorica. Ovviamente, tale scollamento non ha interrotto tutti i collegamenti. La geolocalizzazione funziona solo se è possibile monitorare la telepresenza di Alice. E la telepresenza di Alice è possibile solo se si trova all’interno di un ambiente fisicamente connesso. Ma questi due aspetti sono ora totalmente distinti e in effetti il loro scollamento ha almeno in parte declassato la posizione a favore della presenza. Perché se tutto ciò di cui Alice ha bisogno e si preoccupa è di essere presente digitalmente e interattiva in un particolare angolo dell’infosfera, non importa in quale parte del mondo si trovi analogicamente, se a casa, in treno o nel suo ufficio. Questo è il motivo per cui banche, librerie, biblioteche e negozi di vendita al dettaglio sono tutti luoghi disegnati per la localizzazione oggi alla ricerca di un riutilizzo in termini di presenza. Quando un negozio apre una caffetteria, sta cercando di ricongiungere la presenza e la localizzazione dei clienti che è stata separata dall’esperienza digitale.

			Consideriamo, inoltre, lo scollamento tra legge e territorialità. Per secoli, all’incirca dalla pace di Westfalia (1648), la geografia politica ha fornito alla giurisprudenza una pronta risposta alla questione relativa all’ambito di applicazione di una sentenza: la decisione del giudice si applica nel perimetro dei confini nazionali entro cui opera l’autorità della legge. Tale connessione potrebbe essere riassunta nei seguenti termini: “Il mio posto le mie regole, il tuo posto le tue regole”. Ora può sembrare scontato, ma ci sono voluti molto tempo ed enorme fatica per raggiungere un approccio così semplice, che funziona ancora oggi molto bene, purché si operi solo all’interno di uno spazio fisico, analogico. Tuttavia, Internet non è uno spazio fisico, e il problema della territorialità si profila a partire dallo scollamento ontologico tra lo spazio normativo del diritto, lo spazio fisico della geografia e lo spazio logico del digitale. Si tratta di una nuova “geometria” variabile che stiamo ancora imparando a gestire. Per esempio, lo scollamento tra legge e territorialità è diventato tanto palese quanto problematico durante il dibattito sul cosiddetto diritto all’oblio (Floridi, 2015a). I motori di ricerca operano all’interno di uno spazio logico online fatto di nodi, collegamenti, protocolli, risorse, servizi, url, interfacce ecc. Ciò significa che qualsiasi cosa è a portata di clic. Perciò è difficile dare piena attuazione al diritto all’oblio, richiedendo a Google di rimuovere i collegamenti alle informazioni personali di qualcuno dalla sua versione .com negli Stati Uniti in forza di una decisione presa dalla Corte di Giustizia dell’Unione Europea, dal momento che tale decisione può risultare inutile se i collegamenti non sono rimossi da tutte le versioni del motore di ricerca. Si noti però che un tale disallineamento degli spazi non genera solo problemi, ma fornisce anche soluzioni. La non territorialità del digitale fa miracoli per la libera circolazione delle informazioni. In Cina, per esempio, il governo deve compiere uno sforzo costante e sostenuto per controllare le informazioni online. Parimenti, il Regolamento generale sulla protezione dei dati personali deve essere ammirato per la sua capacità di sfruttare la “saldatura” tra identità personale e informazioni personali per aggirare lo “scollamento” tra legge e territorialità, fondando la protezione dei dati personali sul legame tra persona e dato (che è ora fondamentale) piuttosto che sulla geografia (il luogo dove tali dati personali sono trattati non è più rilevante).

			Infine, possiamo considerare un incollamento che risulta essere più precisamente un ri-incollamento. Nel suo libro The Third Wave (La terza ondata) Alvin Toffler ha coniato il termine prosumer in riferimento all’affievolirsi della differenza e alla progressiva fusione tra il ruolo di produttore e quello di consumatore (Toffler, 1980). Egli ha attribuito questa tendenza a un mercato altamente saturo e alla produzione di massa di prodotti standardizzati, che ha stimolato un processo di personalizzazione di massa e quindi un crescente coinvolgimento dei consumatori come produttori dei propri prodotti personalizzati. Questa idea era stata anticipata da Marshall McLuhan e Barrington Nevitt (1972), che hanno attribuito il fenomeno alle tecnologie basate sull’elettricità. In seguito, si è fatto riferimento con ciò al consumo di informazioni prodotte dalla stessa popolazione di produttori, per esempio su YouTube. Ignaro di questi precedenti, quasi vent’anni dopo Toffler, io ho introdotto la parola produmer per cogliere lo stesso fenomeno.4 Tuttavia, in tutti questi casi, la posta in gioco non è tanto un nuovo incollamento, quanto piuttosto, per essere precisi, un ri-incollamento. Per la maggior parte della nostra storia (circa il 90% [Lee, Daly, 1999]) abbiamo vissuto in società di cacciatori-raccoglitori, che cercano cibo per sopravvivere. Durante questo lungo periodo, produttori e consumatori normalmente hanno coinciso. I prosumer che cacciavano animali selvatici e raccoglievano piante selvatiche erano, in altri termini, la normalità, non l’eccezione. È solo dopo lo sviluppo delle società agrarie, circa 10.000 anni fa, che abbiamo assistito a una separazione completa, e nel tempo culturalmente palese, tra produttori e consumatori. In alcuni angoli dell’infosfera, quella disgiunzione è ricongiunta. Su Instagram o TikTok, per esempio, consumiamo ciò che produciamo. Si può quindi rimarcare che, in alcuni casi, questa parentesi stia volgendo al termine e che i prosumer siano tornati, ricongiunti dal digitale. Di conseguenza, è perfettamente coerente che il comportamento umano in rete sia stato comparato e studiato in termini di modelli di foraggiamento sin dagli anni Novanta (Pirolli, Card, 1995, 1999; Pirolli, 2007).

			Il lettore può facilmente elencare ulteriori casi di incollamento, scollamento e ri-incollamento. Si pensi, per esempio, alla differenza tra realtà virtuale (scollamento) e realtà aumentata (incollamento); la consueta disgiunzione tra uso e proprietà nella share economy; la ricongiunzione di autenticità e memoria grazie alla blockchain; o l’attuale dibattito su un reddito di base universale, un caso di scollamento tra stipendio e lavoro. Ma è giunto il momento di procedere dalla domanda relativa al “come” a quella relativa al “perché”. Perché il digitale ha questo potere di scissione, vale a dire di incollare, scollare o ri-incollare il mondo? Perché altre innovazioni tecnologiche sembrano non avere un impatto simile? La risposta, suppongo, sta nella combinazione di due fattori.

			Da un lato, il digitale è una tecnologia di terzo ordine (Floridi, 2014a). Non è solo una tecnologia che sta tra noi e la natura, come un’ascia (primo ordine); o una tecnologia che sta tra noi e un’altra tecnologia, come un motore (secondo ordine). È piuttosto una tecnologia che sta tra una tecnologia e un’altra tecnologia, come un sistema computerizzato che controlla un robot che dipinge un’automobile (terzo ordine). A causa dell’autonoma potenza di calcolo del digitale, potremmo anche non avere controllo sul (per non parlare di essere parte del) processo.

			Dall’altro, il digitale non è semplicemente qualcosa che potenzia o aumenta una realtà, ma qualcosa che la trasforma radicalmente, perché crea nuovi ambienti che abitiamo e nuove forme di agire con cui interagiamo. Non c’è un termine specifico per descrivere questa profonda trasformazione, perciò in passato (Floridi, 2010b) ho utilizzato l’espressione re-ontologizzazione per fare riferimento a una radicale forma di re-ingegnerizzazione, che non consiste soltanto nel disegnare, costruire o strutturare un sistema (come una società, una macchina o un qualche artefatto) in modo nuovo, ma nel trasformare fondamentalmente la sua natura intrinseca, vale a dire la sua ontologia. In questo senso, per esempio, le nanotecnologie e le biotecnologie stanno non semplicemente re-ingegnerizzando, ma re-ontologizzando il nostro mondo. Attraverso la re-ontologizzazione della modernità, per dirlo in breve, il digitale sta anche ridefinendo dal punto di vista epistemologico la mentalità moderna, cioè molte delle nostre concezioni e idee consolidate.

			Considerati unitariamente, tutti questi fattori suggeriscono che il digitale deve il suo potere di scissione al suo essere una tecnologia di terzo ordine re-ontologizzante e ri-epistemologizzante. Questo è il motivo per cui fa ciò che fa e per cui nessun’altra tecnologia neppure si avvicina ad avere un effetto similare.





1.3 Nuove forme dell’agire


			Se quanto detto fin qui è a grandi linee corretto, allora ci può aiutare a dare un senso ad alcuni fenomeni attuali riguardanti la trasformazione della morfologia dell’agire nell’era digitale e, in tal modo, a rendere conto di alcune forme dell’agire che sono solo apparentemente non correlate. La loro trasformazione dipende dal potere di scissione del digitale, ma la loro interpretazione potrebbe essere dovuta a un implicito fraintendimento di tale potere e delle sue conseguenze ulteriori, profonde e durature. Mi riferisco all’agire politico in quanto democrazia diretta e all’agire artificiale in quanto ia. In sostanza, la re-ontologizzazione dell’agire non è stata ancora accompagnata da un’adeguata ri-epistemologizzazione della sua interpretazione. O per dirlo in modo meno preciso ma forse più intuitivo: il digitale ha cambiato la natura dell’agire, ma stiamo ancora interpretando l’esito di tali cambiamenti attraverso una mentalità moderna, e ciò genera qualche profondo malinteso. Dirò soltanto qualche parola sull’agire politico e la democrazia diretta perché, come anticipato, in questo libro mi concentrerò esclusivamente sull’agire artificiale, senza mettere a tema l’agire sociopolitico che è modellato e sostenuto dalle tecnologie digitali.

			Negli attuali dibattiti sulla democrazia diretta, talora siamo indotti erroneamente a credere che il digitale dovrebbe (si noti l’approccio normativo opposto a quello descrittivo) ricongiungere sovranità (il potere politico che può essere legittimamente delegato) e governance (il potere politico che è legittimamente delegato, temporaneamente, condizionatamente e responsabilmente, e che può essere in modo altrettanto legittimo ripreso: Floridi, 2016e). La democrazia rappresentativa è comunemente (benché erroneamente) concepita come un compromesso dovuto a vincoli pratici di comunicazione. La vera democrazia sarebbe quella diretta, in quanto basata sulla partecipazione immediata, costante e universale di tutti i cittadini alle questioni politiche. Purtroppo, così si è soliti argomentare, siamo troppo numerosi, e quindi la delega del potere politico è il male minore ma necessario (Mill, 1861, p. 69). È il mito della città-Stato, in particolare di Atene. Il compromesso a favore di una democrazia rappresentativa è sembrato inevitabile per secoli, fino all’avvento del digitale. Secondo taluni, il digitale promette ora di congiungere (o di ricongiungere, se si crede in qualche mitico tempo antico) sovranità e governance per offrire un nuovo tipo di democratica agorà digitale, che potrebbe infine rendere possibile il costante coinvolgimento diretto di ogni cittadino interessato. È la stessa promessa formulata dallo strumento referendario (soprattutto se vincolante, invece che consultivo). In entrambi i casi, agli elettori viene chiesto direttamente cosa si dovrebbe fare. Il solo compito lasciato alla classe politica, amministrativa e tecnica sarebbe di attuare la decisione popolare. I politici sarebbero funzionari delegati (e non rappresentanti) in un senso molto letterale. Eppure questo è un errore, perché la democrazia indiretta è sempre stata il vero progetto da realizzare. La disgiunzione è una caratteristica e non un difetto, per dirlo in modo esplicito. E ciò perché un regime democratico è prima di tutto caratterizzato non da talune procedure o da alcuni valori (elementi da cui pure è caratterizzato), ma da una chiara e netta separazione – cioè disgiunzione – tra coloro a cui appartiene il potere politico (sovranità) che delegano legittimamente con il voto (di tutti i cittadini che vi hanno diritto) e coloro a cui è affidato questo potere politico (governance) che esercitano in forza di tale mandato, governando in modo trasparente e responsabile, fintanto che vi sono legittimamente autorizzati. Per dirlo in termini più chiari, un regime democratico non è semplicemente un modo di esercitare e gestire il potere in base a determinate procedure e/o conformemente a determinati valori, ma prima di tutto un modo di strutturarlo: chi detiene il potere non lo esercita ma lo affida a chi lo esercita ma non lo detiene. La commistione tra queste due parti conduce a instabili forme di dittatura o di dominio delle masse. In tal senso la democrazia rappresentativa non è un compromesso ma in realtà la migliore forma di democrazia. Usare il digitale per congiungere (o, più miticamente, ricongiungere) sovranità e governance sarebbe un errore che si paga a caro prezzo. Brexit, Trump, Lega Nord e altri disastri populisti generati dalla “tirannia della maggioranza” (Adams, 1787) ne sono una prova sufficiente. Dobbiamo considerare quale sia il modo migliore per trarre vantaggio dal programmato disallineamento rappresentativo tra sovranità e governance, e non come cancellarlo. Il consenso è il problema, ma non è il tema di questo libro. Ciò che volevo suggerire con l’analisi precedente era solo un saggio del tipo di considerazioni sulle forme dell’agire che mettono in relazione l’impatto del digitale sulla politica con il modo in cui esaminiamo e valutiamo l’ia, come vedremo nel prossimo paragrafo.





1.4 ia: un ambito di ricerca in cerca di una definizione


			Alcune persone (forse molte) sembrano credere che l’ia riguardi la congiunzione tra agire artificiale e comportamento intelligente in nuovi artefatti. Questo è un malinteso. Come spiegherò più diffusamente nel prossimo capitolo, in realtà è vero il contrario: la rivoluzione digitale ha reso l’ia non solo possibile ma sempre più utile separando la capacità di risolvere un problema o di portare a termine un compito con successo dall’esigenza di essere intelligenti nel farlo. L’ia ha successo proprio quando è possibile realizzare tale separazione. Quindi, la consueta lamentela nota come “effetto dell’ia”5 – per cui non appena l’ia può eseguire un compito particolare, per esempio la traduzione automatica o il riconoscimento vocale, il bersaglio si sposta e quel compito non è più definito intelligente se eseguito dall’ia – è in realtà il riconoscimento corretto di come funzioni il processo in questione. L’ia esegue con successo un compito solo se può slegare la sua esecuzione dall’esigenza di essere intelligente nell’eseguirlo. Pertanto, se l’ia ha successo, allora la separazione ha avuto luogo e in effetti il compito si è dimostrato dissociabile dall’intelligenza che sembrava richiesta (per esempio in un essere umano) per portarlo a termine con successo. Ciò è meno sorprendente di quanto possa parere, e nel prossimo capitolo vedremo che è perfettamente coerente con le definizioni classiche, e probabilmente ancora tra le migliori, di ia, fornite da McCarthy, Minsky, Rochester e Shannon nella loro “Proposta per il progetto estivo di ricerca sull’intelligenza artificiale di Dartmouth”, il documento fondante e il successivo evento che hanno gettato le basi dei primi studi sull’ia nel 1955. Mi limito a citarlo di seguito, rimandandone la trattazione al prossimo capitolo:

			Per il presente scopo il problema dell’intelligenza artificiale è quello di far sì che una macchina agisca con modalità che sarebbero definite intelligenti se un essere umano si comportasse allo stesso modo. (Citazione dalla riedizione del 2006 in McCarthy, Minsky, Rochester, Shannon, 2006)

			Le conseguenze derivanti dal comprendere l’ia come un divorzio tra l’agire e l’intelligenza sono profonde, così come lo sono le sfide etiche a cui dà origine tale divorzio. Il resto del libro si concentra sulla loro analisi. Per concludere questo capitolo introduttivo, resta ancora da dare una risposta finale alla domanda “con quali conseguenze”, di cui parlavo all’inizio. Questo è il compito del prossimo paragrafo.





1.5 Conclusione: etica, governance e design


			Supponendo che le risposte precedenti alle domande “perché” e “come” siano plausibili, che differenza fa comprendere il potere del digitale come un potere di tagliare e incollare il mondo e la sua rappresentazione in modi senza precedenti? Un’analogia può aiutare a introdurre la risposta. Se qualcuno ha solo una pietra e nient’altro, nemmeno un’altra pietra da mettere accanto a essa, allora non c’è altro che si possa fare se non godersi la pietra stessa, magari guardandola o giocandoci. Ma se si può tagliare la pietra in due parti, ci sono già diverse possibilità di combinarle insieme. Due pietre forniscono più possibilità e meno vincoli di una singola pietra, e numerose pietre molte di più. Ma tagliare e incollare le pietre della modernità è, per così dire, proprio ciò che il digitale sa fare meglio, quando si tratta del mondo. Trarre vantaggio da tali possibilità e vincoli in vista della risoluzione di alcuni problemi è ciò che possiamo definire design. Quindi, la risposta ora dovrebbe essere chiara. Il potere di scissione del digitale riduce enormemente i vincoli della realtà e ne aumenta le possibilità. In tal modo, rende il design – l’arte di risolvere un problema sfruttando vincoli e possibilità, per soddisfare alcuni requisiti in vista di un obiettivo – l’attività innovativa che definisce la nostra epoca.

			Il nostro viaggio è adesso completo. Ogni epoca ha innovato la propria cultura, società e ambiente facendo affidamento su almeno tre elementi principali: la scoperta, l’invenzione e il design. Questi tre tipi di innovazione sono strettamente correlati, anche se l’innovazione è stata spesso più inclinata in una direzione rispetto alle altre, come uno sgabello con tre gambe, di cui una sia più lunga e pertanto più prominente rispetto alle altre. L’età post-rinascimentale e la prima modernità possono essere qualificate come l’epoca delle scoperte, soprattutto geografiche. La tarda modernità è ancora un’epoca di scoperte ma, con le sue innovazioni industriali e meccaniche, è forse in misura maggiore un’epoca di invenzioni. E, naturalmente, tutte le epoche sono state anche età del design, almeno in quanto le scoperte e le invenzioni richiedono modi ingegnosi per collegare e dare forma a nuove e vecchie realtà. Ma se è corretto quello che ho sostenuto finora, allora è davvero la nostra epoca che è in modo peculiare, e più di ogni altra, l’età del design. Perché il digitale sta riducendo i vincoli e aumentando le possibilità a nostra disposizione, offrendoci un’immensa e crescente libertà di strutturare e organizzare il mondo in una moltitudine di modi, per risolvere una varietà di problemi vecchi e nuovi. Naturalmente, ogni design richiede un progetto. E, nel nostro caso, si tratta di un progetto umano per la nostra era digitale che ancora ci manca. Tuttavia, non dovremmo lasciare che la forza di scissione del digitale plasmi il mondo senza un piano. Dobbiamo intraprendere ogni sforzo per decidere in quale direzione desideriamo sfruttarlo, per garantire che le società dell’informazione che stiamo costruendo grazie a esso siano aperte, tolleranti, eque, giuste e favorevoli all’ambiente e allo sviluppo umano. La conseguenza più rilevante del potere di scissione del digitale dovrebbe essere un design migliore del nostro mondo. E questo riguarda la definizione dell’ia come nuova forma dell’agire, come vedremo nel capitolo seguente.



* * *





			 				 					1. Fonte: https://www.statista.com/statistics/300201/hours-of-internet-use-per-week-per-person-in-the-uk/.



				 					2. Fonte: https://www.seagate.com/gb/en/our-story/data-age-2025/.



				 					3. Il soggetto interessato è, per il Regolamento generale sulla protezione dei dati personali (gdpr), la persona fisica a cui si riferiscono i dati personali. [NdT]



				 					4. In Floridi (1999); vedi anche Floridi (2004, 2014b). Avrei dovuto conoscerlo meglio e usare il termine prosumer di Toffler.



				 					5. https://en.wikipedia.org/wiki/AI_effect.





2


			Presente: ia come nuova forma dell’agire e non dell’intelligenza

			Sommario In precedenza, nel primo capitolo, abbiamo visto come la rivoluzione digitale abbia tagliato e incollato la nostra realtà e le nostre idee sulla realtà, re-ontologizzando e ri-epistemologizzando la modernità. Ciò ha portato allo sviluppo dell’ia come nuova forma dell’agire, che può avere successo senza essere intelligente. Nel presente capitolo analizzo questa interpretazione. Nel primo paragrafo, mostro come l’assenza di una definizione di ia sia la prova che l’espressione non è un termine scientifico ma un’utile scorciatoia per fare riferimento a una famiglia di scienze, metodi, paradigmi, tecnologie, prodotti e servizi. Nel secondo paragrafo, faccio riferimento alla classica descrizione controfattuale dell’ia fornita da McCarthy, Minsky, Rochester e Shannon nella loro “Proposta per il progetto estivo di ricerca sull’intelligenza artificiale di Dartmouth”. L’abbiamo già incontrata nel capitolo precedente, ed è la descrizione che adotterò nel resto del libro. Esamino anche la celebre domanda di Turing: “Le macchine possono pensare?”. Nel terzo paragrafo, sulla base della precedente analisi delineo l’approccio ingegneristico e cognitivo all’ia, sostenendo che il primo si è tradotto in un grande successo mentre il secondo in un totale fallimento. L’interpretazione dell’ia come nuova forma dell’agire che non deve essere intelligente per avere successo si basa sulla tradizione ingegneristica: nel paragrafo seguente, il quarto, suggerisco che tale forma dell’agire può avere successo perché abbiamo trasformato il mondo (avvolgendolo) in un ambiente sempre più adattato al funzionamento dell’ia. In conclusione, sottolineo come tale processo generi il rischio di spingere l’umanità a adattarsi alle sue tecnologie intelligenti.





2.1 Introduzione: che cos’è l’ia? “La riconosco quando la vedo”


			L’ia è stata definita in molti modi, ma non esiste una sua definizione unitaria su cui tutti concordino. Un rapporto datato (Legg, Hutter, 2007) elencava già 53 definizioni di “intelligenza”, ciascuna delle quali in linea di principio può essere “artificiale”, e 18 definizioni di ia. Il numero è in crescita (Russell, Norvig, 2016). Di fronte a una sfida simile, Wikipedia risolve il problema optando per una tautologia:

			L’intelligenza artificiale (ia) […] è l’intelligenza mostrata dalle macchine, in contrasto con l’intelligenza naturale mostrata dagli esseri umani. (Wikipedia, “Artificial Intelligence”, 17 gennaio 2020)

			Ciò è al contempo assolutamente vero e totalmente inutile. Mi ricorda l’inattaccabile “Brexit significa Brexit”, meccanicamente ripetuto da Theresa May quando rivestiva il ruolo di primo ministro del Regno Unito.

			L’assenza di una definizione standard di ia può essere un problema perché, quasi inevitabilmente, in un seminario sull’etica dell’ia prima o poi qualche brillante partecipante non può fare a meno di chiedersi, pensieroso: “Ma cosa si intende veramente per ia?” (l’enfasi è presente nell’originale, poiché alcune persone riescono a parlare in corsivo). Di solito segue una discussione senza fine, che non raggiunge mai alcun consenso (né potrebbe essere altrimenti) e lascia tutti (inevitabilmente) frustrati. Il rischio è che, dopo una tale perdita di tempo, qualcuno possa concludere che non si possa avere un dibattito sull’etica di qualcosa che è indefinito e apparentemente indefinibile. Questo non ha senso. Certamente la mancanza della definizione di qualcosa di così importante solleva qualche sospetto. Tuttavia ciò non è dovuto al fatto che tutte le cose importanti nella vita siano sempre definibili. Spesso molte di esse non lo sono affatto. Per esempio, abbiamo un’idea assolutamente chiara di cosa sia l’amicizia, pur non disponendo delle condizioni necessarie e sufficienti per coglierne la natura. Un dibattito filosofico sulla sua definizione può facilmente terminare in un vicolo cieco, ma possiamo certamente discuterne in modo molto ragionevole e avere intuizioni preziose sull’etica dell’amicizia, sulla sua natura online, nonché sui suoi vantaggi o svantaggi. Lo stesso vale per “essere in buona salute” o “essere innamorati”. È così perché “la riconosciamo quando la vediamo”, per usare una frase diventata celebre nel 1964 (un anno piuttosto speciale, a quanto pare). Come il lettore probabilmente sa, questa espressione è stata utilizzata dal giudice della Corte Suprema degli Stati Uniti, Potter Stewart, nella decisione del caso Jacobellis v. Ohio, che verteva su ciò che poteva essere considerato osceno. Spiegando perché avesse deciso che il materiale in esame potesse essere considerato alla stregua di un discorso protetto dal Primo emendamento, da non reputarsi osceno e pertanto da non censurare, si era espresso in questi termini:

			Oggi non cercherò di definire ulteriormente i tipi di materiale che, a quanto mi risulta, devono essere inclusi in tale sintetica descrizione [pornografia hard-core], e forse non potrei mai riuscire a farlo in modo intellegibile. Ma la riconosco quando la vedo, e il film oggetto di questo caso non vi rientra. (378 Stati Uniti, p. 197; giudice Stewart, opinione concorrente [corsivo mio])

			L’amicizia, l’ia e molte altre cose nella vita sono come la pornografia: non sono definibili (nel senso stretto in cui l’acqua è definibile e definita come h2o) ma le riconosciamo quando le incontriamo. Ciò va bene nella quotidianità. Tuttavia riconosco che l’assenza di una definizione sia un po’ sospetta perché, nella scienza, anche cose insignificanti dovrebbero essere definibili con precisione, soprattutto dopo lunghi decenni di dibattiti. La conclusione è che probabilmente ia non è un termine scientifico, come “triangolo”, “pianeta” o “mammifero”, ma un’espressione generica, proprio come amicizia o pornografia. È una scorciatoia, usata per riferirsi approssimativamente a diverse discipline, servizi, prodotti tecnoscientifici talora solo genericamente correlati. L’ia è una famiglia in cui la somiglianza, e talvolta solo per pochi tratti, è il criterio di appartenenza.

			Alcuni membri della famiglia ia sono illustrati nella Figura 2.1.



			La mappa è utile ma alquanto controversa. Come mi ha fatto notare David Watson, sembra strano dedicare lo stesso spazio all’ia simbolica e probabilistica (“Sarebbe come tenere un corso di fisica nucleare che dedichi tanto tempo a Democrito quanto alla meccanica quantistica”); è fonte di confusione includere il deep learning (l’apprendimento profondo) in una sezione a sé stante distinta dalla visione artificiale e dall’elaborazione del linguaggio naturale, quando queste ultime due applicazioni sono condotte quasi esclusivamente, oggi, utilizzando il deep learning. Non è chiaro il motivo per cui le reti neurali vengono inserite in una colonna relativa all’apprendimento non supervisionato, quando sono più notoriamente associate a problemi supervisionati. Molti importanti metodi non supervisionati (come, per esempio, raggruppamento, proiezione, rilevamento di valori anomali) non sono nemmeno menzionati; e la terminologia del calcolo “sub-simbolico” non è standard, almeno se intende riferirsi a procedure di ottimizzazione come gli algoritmi evolutivi. David ha ragione. Ma mi piace ancora, perché aiuta a mettere a fuoco la questione ed è molto meglio di niente. Non mostro questa mappa perché è perfetta, ma perché, con tutti i suoi limiti, copre un bel po’ di territorio e mostra che anche due esperti possono facilmente essere in disaccordo pur appartenendo allo stesso ambito e condividendo lo stesso approccio. Come due esperti di etica che non siano d’accordo su che cosa sia veramente l’amicizia o la pornografia.

			Questo ci conduce alla domanda seguente: se l’ia non può essere definita elencando condizioni necessarie e sufficienti incontroverse, c’è qualcosa che queste o simili discipline, ambiti, paradigmi, metodi, tecniche o tecnologie riconducibili nell’alveo dell’ia hanno in comune? Direi di sì, e risale al 1955. È la definizione che abbiamo incontrato nel capitolo precedente.





2.2 ia come controfattuale


			Iniziamo richiamando tale definizione, così non occorre ricercarla:

			Per il presente scopo il problema dell’intelligenza artificiale è quello di far sì che una macchina agisca con modalità che sarebbero definite intelligenti se un essere umano si comportasse allo stesso modo. (Citazione dalla riedizione del 2006 in McCarthy, Minsky, Rochester, Shannon, 2006)

			Questo è chiaramente un controfattuale e non ha nulla a che vedere con il pensiero ma esclusivamente con il comportamento: se un essere umano si comportasse in quel modo, quel comportamento sarebbe definito intelligente. Non significa che la macchina sia intelligente o che addirittura stia pensando. La comprensione controfattuale dell’ia è alla base anche del test di Turing (Turing, 1950) e del premio Loebner (Floridi, Taddeo, Turilli, 2009). Turing comprese molto bene che non vi era modo di rispondere alla domanda se una macchina fosse in grado di pensare, perché, come ammise, entrambi i termini sono privi di definizione scientifica:

			Propongo di considerare la domanda: “Possono le macchine pensare?”. Questa indagine dovrebbe iniziare definendo il significato dei termini “macchina” e “pensare”. […] La domanda originaria, “Possono le macchine pensare?”, credo sia troppo insensata per meritare di essere discussa. (Turing, 1950, pp. 433, 442, corsivo mio)

			Per questo, ha elaborato invece un test: un po’ come decidere che il miglior modo di valutare se qualcuno sia in grado di guidare consista nel verificare le sue prestazioni su strada. Nel caso di Turing, il test verifica la capacità di una macchina di rispondere a domande in modo tale che il risultato sia indistinguibile, quanto alla sua fonte, dal risultato di un agente umano che si adopera nello stesso compito (ibidem). Questo è perfettamente ragionevole, ma riflettiamo su un punto: solo perché una lavastoviglie pulisce bene i piatti o meglio di quanto lo faccia io, non significa che li pulisca come me o che abbia bisogno di intelligenza (non importa se del mio tipo o di qualsiasi altro) nello svolgimento del suo compito. Ciò equivarrebbe a sostenere che, poiché (a) il fiume raggiunge il mare seguendo il miglior percorso possibile, rimuovendo gli ostacoli sul suo cammino; e (b) se ciò dovesse essere fatto da un essere umano, lo considereremmo un comportamento intelligente; allora (c) il comportamento del fiume è intelligente. Quest’ultimo scenario è fallace e sa di superstizione. Il solo aspetto rilevante è eseguire un compito con successo in modo tale che il risultato sia altrettanto buono o migliore di quello che l’intelligenza umana sarebbe stata in grado di ottenere. Il come non è in discussione, lo è solo il risultato. Questo punto è cruciale, logicamente e storicamente.

			Logicamente, persino l’identità (lasciando da parte la somiglianza) di un risultato non dice nulla circa l’identità dei processi che l’hanno generato e delle fonti dei processi stessi. Questo sembra indiscutibile, eppure Turing la pensava diversamente. In una trasmissione radiofonica della bbc, ha affermato:

			[…] [1.20] l’opinione che io stesso sostengo, che non è del tutto irragionevole descrivere i computer digitali come cervelli. […] [4.36-4.47] Se ora una macchina in particolare può essere descritta come un cervello non ci resta che programmare il nostro computer digitale per imitarlo e sarà anche un cervello. Se si accetta l’idea che il vero cervello, che si trova negli animali e in particolare negli esseri umani, sia una sorta di macchina, ne consegue che il nostro computer digitale opportunamente programmato si comporterà come un cervello. (Turing, 1951)

			Proviamo a dimenticare per un momento che la persona che sta parlando è un genio. Immaginiamo che sia scritto in un articolo di giornale, come il Daily Mail. Prendiamo la penna rossa. Prima cosa da sottolineare: “non è del tutto irragionevole”. Anche considerando l’inglese dell’epoca, questa doppia negazione costituisce la più debole premessa possibile che si possa immaginare per sostenere una qualsiasi tesi. In base allo stesso standard, “non del tutto irragionevole” è anche descrivere gli animali come automi che non possono ragionare o provare dolore, come ha sostenuto un altro genio, Cartesio. Naturalmente, pensiamo che Cartesio avesse torto. La successiva parola da sottolineare è “descrivere”. Qualsiasi cosa può essere descritta nei termini di qualsiasi altra cosa, a un certo livello di astrazione. La questione è se tale livello di astrazione sia quello corretto. Non è del tutto irragionevole descrivere una partita a scacchi come una battaglia tra due nemici. E se si tratta della finale del campionato mondiale del 1972 tra Spassky e Fischer, la descrizione potrebbe persino ricordare che le battaglie appartengono alle guerre, in questo caso la Guerra fredda. Ma se cerchiamo in essa sofferenza e morte, violenza e sangue, rimarremo delusi. È un “tiiipo” (le tre “i” sono importanti) di battaglia, ma non in senso letterale. Pertanto, consideriamo l’ipotesi che il cervello sia una macchina. Ciò pare o banalmente vero dal punto di vista metaforico o fattualmente sbagliato dal punto di vista non metaforico, cioè in termini di osservazione scientifica. Nell’articolo, Turing aveva ammesso che abbiamo una vaga idea di ciò che possiamo considerare macchina. Anche una burocrazia può essere descritta “non del tutto irragionevolmente” come una macchina, per dirla con un altro genio, Kafka. Un bollitore, un frigorifero, un treno, un computer, un tostapane, una lavastoviglie… sono tutte macchine, in un senso o nell’altro. Anche il nostro corpo è una macchina. Così il nostro cuore. Perché non il cervello? Assolutamente sì. Il vero problema è l’enorme margine di manovra. Perché tutto dipende da quanto rigorosamente interpretiamo “tipo di”. Se quasi tutto può qualificarsi come un tipo di macchina, allora sicuramente anche un cervello è un tipo di macchina. Ma non stiamo inserendo il piolo giusto nel foro giusto, abbiamo solo reso il foro così grande che qualsiasi piolo vi si adatterà. Finora tutto questo ha riguardato un modo di esprimersi approssimativo, ma il problema che segue è un vero errore. Se A e B producono entrambi lo stesso risultato R dato lo stesso input I (per usare l’esempio preferito di Turing, cioè fornire alle stesse domande risposte identiche o equivalenti per qualità, e comunque indistinguibili quanto alla loro fonte; cosa che, non dimentichiamolo, implica già la riduzione del cervello a un computer), anche in tal caso ciò non significa che (a) si comportino entrambi allo stesso modo o (b) siano la stessa cosa. Immaginiamo che Alice si rechi a casa di Roberto e trovi dei piatti puliti sul tavolo. Non sarebbe in grado di evincere dal risultato (piatti puliti) se sia stato utilizzato lo stesso procedimento (lavaggio meccanico o manuale), né quale agente li abbia effettivamente puliti (la lavastoviglie o Roberto), o quali capacità siano state adoperate per ottenere il risultato. Tuttavia, sarebbe sbagliato inferire da tale irreversibilità e opacità che Bob e la lavastoviglie siano, dunque, la stessa cosa o si comportino allo stesso modo, anche solo in termini di capacità di lavare i piatti. Il fatto è che a Alice probabilmente non importerebbe nulla, purché i piatti siano puliti.

			Temo che Turing fosse, metaforicamente, nel giusto ma, sostanzialmente, nel torto, o forse è stata la bbc che ha richiesto un abbassamento del livello di precisione (ci sono stato e l’ho fatto, per cui parlo da peccatore): non è del tutto irragionevole descrivere computer digitali come cervelli, o addirittura viceversa; ma non è utile perché tutto resta troppo metaforico e vago, e una volta che iniziamo a essere precisi le somiglianze scompaiono e tutte le differenze rilevanti diventano sempre più evidenti. Cervello e computer non sono la stessa cosa e non si comportano allo stesso modo. Entrambe le tesi non sono confutate dalla posizione di Turing. Tuttavia, ciò non costituisce un argomento positivo a loro favore. Temo che sostenere interamente tali tesi richiederebbe un diverso tipo di libro.1 In questo contesto, voglio solo rendere esplicita la prospettiva con cui può essere interpretata in modo più accurato la seconda parte di questo libro. I lettori che preferiscono pensarla come Turing non saranno d’accordo con me, ma spero che siano comunque in grado di convenire sulla seguente distinzione. Storicamente, la rappresentazione controfattuale dell’ia contiene i germi di un approccio ingegneristico, opposto a quello cognitivo, all’ia, come vedremo nel prossimo paragrafo.





2.3 Le due anime dell’ia: ingegneristica e cognitiva


			È un fatto risaputo, anche se talora sottostimato, che le ricerche sull’ia aspirino sia a riprodurre i risultati o l’esito positivo del nostro comportamento intelligente (o almeno di qualche tipo di comportamento animale) con mezzi non biologici, sia a produrre l’equivalente non biologico della nostra intelligenza, cioè la fonte di tale comportamento.

			Da un lato, come settore dell’ingegneria interessata alla riproduzione del comportamento intelligente, l’ia ha avuto un successo sbalorditivo, ben oltre le più rosee aspettative. Prendiamo un esempio piuttosto celebre, anche se un po’ vecchio. Deep Q-network (un sistema di algoritmi software) appartiene a questo genere di ia riproduttiva. Nel 2015, Deep Q-network ha imparato a giocare a 49 classici videogiochi vintage Atari da zero, basandosi solo sui dati relativi ai pixel su uno schermo e sul metodo di punteggio (Mnih, Kavukcuoglu, Silver et al., 2015). Impressionante? Sì, da un punto di vista ingegneristico. Non molto, per ciò che concerne avvicinarsi a una vera forma di intelligenza artificiale. In fin dei conti, ci vuole meno “intelligenza” per vincere giocando a Space Invaders o Breakout che per essere un campione di scacchi. Per questo era solo una questione di tempo prima che alcuni esseri umani ingegnosi trovassero il modo di rendere una macchina di Turing abbastanza intelligente da giocare con abilità ai giochi Atari. Oggi, facciamo sempre più affidamento su applicazioni basate sull’ia (definite talvolta come tecnologie smart, una terminologia che userò anch’io, sebbene l’espressione abbia una portata più ampia) per eseguire compiti che sarebbero semplicemente impossibili per un’intelligenza umana non aiutata o non aumentata. L’ia riproduttiva ottiene regolarmente risultati migliori e sostituisce l’intelligenza umana in un numero sempre maggiore di contesti. Per le tecnologie smart il cielo è il limite e Deep Q-network ha solo cancellato un altro ambito in cui gli esseri umani erano migliori delle macchine. La prossima volta che sperimenteremo un atterraggio un po’ accidentato, ricordiamoci che probabilmente ciò è accaduto perché al comando c’era un pilota e non un computer. Ciò non significa che un drone autonomo guidato dall’ia voli come un uccello. Il celebre commento attribuito a Edsger Wybe Dijkstra per cui “la questione se un computer possa pensare non è più interessante della questione se un sottomarino possa nuotare” illustra bene l’approccio pratico condiviso dall’ia riproduttiva.

			D’altro lato, come settore della scienza cognitiva interessata alla produzione di intelligenza, l’ia rimane fantascienza ed è stata una triste delusione. L’ia produttiva non si limita a prestazioni inferiori rispetto all’intelligenza umana; non ha ancora preso parte alla competizione. Il fatto che Watson, il sistema ibm in grado di rispondere alle domande poste in linguaggio naturale, possa sconfiggere i suoi avversari umani giocando a Jeopardy! dice di più sugli ingegneri umani, le loro incredibili capacità e competenze, e il gioco stesso, che sull’intelligenza biologica di qualsiasi tipo, inclusa quella di un topo. Il lettore non è costretto a credermi. John McCarthy – che, come detto, ha coniato l’espressione “intelligenza artificiale” ed era un autentico sostenitore della possibilità di creare una forma di ia nel senso cognitivo osservato poco sopra2 – aveva ben compreso tutto ciò. Le sue osservazioni deluse sulla vittoria di Deep Blue contro il campione del mondo Garry Kasparov nel 1997 (McCarthy, 1997) sono sintomatiche del tipo di ia produttiva e cognitiva che disapprova l’ia riproduttiva e ingegneristica. Questo è il motivo per cui non ha mai smesso di lamentarsi del fatto che il gioco degli scacchi fosse considerato un caso di autentica ia. Aveva ragione. Non lo è. Ma aveva torto nel ritenere che non fosse una buona alternativa. Lo stesso vale per AlphaGo (di cui dirò di più a breve).

			Le due anime dell’ia, quella ingegneristica (tecnologie intelligenti) e quella cognitiva (tecnologie realmente intelligenti), hanno spesso ingaggiato lotte fratricide per il primato intellettuale, il potere accademico e le risorse finanziarie. Ciò è in parte dovuto al fatto che entrambe rivendicano antenati comuni e un’unica eredità intellettuale: un evento fondante, la già citata conferenza estiva di ricerca di Dartmouth sull’ia nel 1956, e un padre fondatore, Turing, con la sua macchina, i suoi limiti computazionali e il suo celebre test. Non è di aiuto il fatto che una simulazione possa essere usata per verificare tanto se la fonte simulata (cioè l’intelligenza umana) sia stata prodotta, quanto se il comportamento o la prestazione della fonte che si hanno di mira (vale a dire, ciò che l’intelligenza umana consegue) siano stati riprodotti o addirittura superati. Le due anime dell’ia sono state denominate in modo diverso e non sempre coerente. Talora le distinzioni tra ia debole o forte, tradizionale o nuova, sono state utilizzate per cogliere tale differenza. Oggi, l’espressione intelligenza artificiale generale sembra più di moda, invece di quella di ia completa. In passato, ho preferito utilizzare la distinzione meno marcata tra ia leggera e forte (Floridi, 1999). Non importa. Il disallineamento dei loro caratteri, obiettivi e risultati ha causato infinite e per lo più inutili diatribe. I difensori dell’ia indicano i risultati impressionanti dell’ia riproduttiva e ingegneristica, che è un’ia davvero debole o leggera in termini di obiettivi. Mentre i detrattori dell’ia indicano i risultati deludenti dell’ia produttiva e cognitiva, che è un’ia veramente forte o generale in termini di obiettivi. Molte delle attuali speculazioni sulla cosiddetta questione della singolarità (di cui dirò di più nel decimo capitolo) e sugli effetti catastrofici di alcune presunte superintelligenze hanno le loro radici in tale confusione. A volte non posso fare a meno di sospettare che ciò sia stato fatto intenzionalmente, non per motivi maliziosi, ma perché la confusione è così intellettualmente piacevole. Alcune persone amano le diatribe inutili.

			I grandi campioni sanno come concludere la carriera al culmine del loro successo. Nel 2017, DeepMind, il laboratorio di ia di Alphabet (precedentemente Google), ha deciso che il suo programma per computer AlphaGo non si sarebbe più concentrato sulla vittoria del gioco Go. Invece, il ceo di DeepMind, Demis Hassabis, e il capo programmatore di AlphaGo, David Silver, hanno rivelato che l’attenzione sarebbe stata rivolta a

			sviluppare algoritmi generali avanzati che un giorno potrebbero aiutare gli scienziati nell’affrontare alcuni dei nostri problemi più complessi, come trovare nuove cure per le malattie, ridurre drasticamente il consumo di energia o inventare nuovi materiali rivoluzionari.3

			L’ambizione era giustificata. Tre anni dopo, nel 2020, il sistema di ia di DeepMind, AlphaFold 2, ha risolto il “problema del ripiegamento delle proteine”, una grande sfida della biologia che aveva tormentato gli scienziati per cinquant’anni:

			La capacità di prevedere con precisione le strutture proteiche a partire dalla loro sequenza di amminoacidi sarebbe un enorme vantaggio per le scienze della vita e la medicina. Velocizzerebbe notevolmente gli sforzi per comprendere gli elementi costitutivi delle cellule e per consentire una scoperta di farmaci più rapida e avanzata. (Callaway, 2020)

			L’ia porterà a nuove scoperte e svolte potenzialmente considerevoli, specialmente nelle mani di persone brillanti e riflessive. Sosterrà, inoltre, la gestione e il controllo di sistemi sempre più complessi. Tuttavia, tutti questi straordinari sviluppi potranno realizzarsi più facilmente se viene eliminato un malinteso. Abbiamo visto che l’ia di successo non riguarda la produzione ma la sostituzione dell’intelligenza umana. Una lavastoviglie non pulisce i piatti come lo facciamo noi, ma alla fine del processo i suoi piatti puliti sono indistinguibili dai nostri, anzi possono essere anche più puliti (efficacia), utilizzando meno risorse (efficienza). Lo stesso vale per l’ia. AlphaGo non ha giocato come il grande maestro cinese di Go, Ke Jie, numero uno al mondo, ma ha vinto comunque. Parimenti, le automobili autonome non sono auto guidate da robot umanoidi seduti al volante al nostro posto, ma sono modi per reinventare completamente l’auto e il suo ambiente. Nell’ia, è il risultato che conta, non se l’agente o il suo comportamento sia intelligente. Per questo, l’ia non concerne la capacità di riprodurre l’intelligenza umana, ma in realtà la capacità di farne a meno. Le macchine attuali hanno l’intelligenza di un tostapane e non abbiamo davvero la più pallida idea di come fare un passo avanti (Floridi, Taddeo, Turilli, 2009). Quando l’avviso “stampante non trovata” è visualizzato sullo schermo del computer, può risultare fastidioso ma non sorprendente, nonostante la stampante in questione sia effettivamente lì, accanto al computer. Ma, soprattutto, ciò non è un ostacolo perché gli artefatti possono essere smart senza essere intelligenti, e questo è il risultato davvero straordinario dell’ia riproduttiva, che è la continuazione riuscita dell’intelligenza umana con altri mezzi, per parafrasare Carl von Clausewitz.

			Oggi, l’ia scinde la risoluzione efficace dei problemi e l’esecuzione corretta dei compiti dal comportamento intelligente, ed è proprio grazie a tale scissione che può incessantemente colonizzare lo spazio sterminato di problemi e compiti, ogni volta che questi possono essere conseguiti senza comprensione, consapevolezza, acume, sensibilità, preoccupazioni, sensazioni, intuizioni, semantica, esperienza, bio-incorporazione, significato, persino saggezza e ogni altro ingrediente che contribuisca a creare l’intelligenza umana. In breve, è proprio quando smettiamo di cercare di produrre intelligenza umana che possiamo sostituirla con successo in un numero crescente di compiti. Se avessimo aspettato anche solo una scintilla di vera intelligenza artificiale, del tipo che troviamo in Star Wars, AlphaGo non sarebbe mai diventato più abile di chiunque altro nel giocare a Go. In effetti, avrei comunque vinto giocando a scacchi contro il mio smartphone.

			Se si comprende appieno il senso di questa scissione, si prospettano tre ovvi sviluppi. Li discuterò più in dettaglio nel prossimo capitolo, ma possono essere delineati qui. L’ia dovrebbe smettere di vincere i giochi e imparare a ludicizzare. Man mano che l’ia migliora nel giocare, tutto ciò che può essere trasformato in gioco rientra nel suo ambito. Se fossi DeepMind, assumerei un team di esperti che progettano giochi. In secondo luogo, in contesti ludificati, l’ia sarà abbinata soltanto all’ia e le sue interazioni interne potrebbero diventare troppo complesse per poter essere integralmente comprese da ammiratori esterni come noi. Potremmo allietarci nel guardare l’ia giocare così come ci allietiamo nell’ascoltare Bach. E, infine, possiamo aspettarci che l’intelligenza umana abbia un ruolo diverso ovunque l’ia sia il giocatore migliore. Perché si tratterà meno di risolvere alcuni problemi e più di decidere quali problemi valga la pena di risolvere, perché, per quali finalità, e con quali costi, trade-off e conseguenze accettabili.





2.4 ia: un divorzio riuscito nell’infosfera


			La classica definizione controfattuale e l’interpretazione dell’ia come divorzio, e non come matrimonio, tra l’agire e l’intelligenza, consentono di concepire l’ia come una risorsa crescente di capacità di agire interattiva, autonoma e spesso autoapprendente (nel senso dell’apprendimento automatico, di cui alla Figura 2.1), che può affrontare un numero sempre più elevato di problemi e attività che richiederebbero altrimenti l’intelligenza e l’intervento umani (e possibilmente una quantità illimitata di tempo) per essere eseguiti con successo. In breve, l’ia è definita sulla base di risultati e azioni ingegnerizzati e quindi, nel resto di questo libro, tratterò l’ia come una riserva di capacità di agire a portata di mano. Questa definizione è sufficientemente generale per cogliere i molti modi in cui l’ia è discussa in letteratura. Tuttavia, prima di esaminare il possibile sviluppo dell’ia nel prossimo capitolo, è necessario affrontare in questo contesto un’ultima questione. Il divorzio tra l’agire e l’intelligenza è problematico, in quanto è una delle fonti delle sfide etiche poste dall’ia, dal momento che gli agenti artificiali sono

			sufficientemente informati, “smart”, autonomi e in grado di compiere azioni moralmente rilevanti indipendentemente dagli esseri umani che li hanno creati […]. (Floridi, Sanders, 2004)

			Tuttavia, bisogna chiedersi, in primo luogo, come possa un divorzio tra l’agire e l’intelligenza avere successo in termini di efficacia. Non è, dunque, necessaria l’intelligenza perché un qualsiasi tipo di comportamento abbia successo? Ho già offerto l’esempio del fiume per illustrare una possibile fallacia. Occorre, però, fornire una spiegazione più articolata per rispondere alla domanda precedente, che può richiedere uno sforzo supplementare poiché costituisce un’obiezione ragionevole. Il successo dell’ia è in gran parte dovuto al fatto che stiamo costruendo un ambiente adattato a essa, in cui le tecnologie intelligenti si trovano a casa mentre noi siamo più simili a sommozzatori. È il mondo che si sta adattando all’ia e non viceversa. Vediamo cosa significa.

			L’ia non può essere ridotta a una “scienza della natura” o a una “scienza della cultura” (Ganascia, 2010) perché è una “scienza dell’artificiale”, per dirla con Herbert Simon (1996). In quanto tale, l’ia persegue un approccio al mondo che non è descrittivo né prescrittivo: indaga, piuttosto, le condizioni vincolanti che rendono possibile costruire e incorporare artefatti nel mondo, che sono in grado di interagire con esso con successo. In altre parole, inscrive il mondo, poiché questi artefatti sono nuovi pezzi di codice logico-matematico, cioè nuovi testi, scritti nel libro matematico della natura di Galileo:

			La filosofia è scritta in questo grande libro – intendo l’universo – che è continuamente aperto al nostro sguardo, ma non può essere compresa se non si impara prima a comprendere la lingua in cui è scritta. È scritto nel linguaggio della matematica e i suoi caratteri sono triangoli, cerchi e altre figure geometriche, senza le quali è umanamente impossibile comprenderne una sola parola; senza questi, si vaga in un labirinto oscuro. (Galileo, Il Saggiatore, 1623).

			Fino a poco tempo fa, l’impressione diffusa era che tale processo di addizione al libro matematico della natura (iscrizione) richiedesse la fattibilità di un’ia produttiva e cognitiva. Dopotutto, sviluppare anche una forma rudimentale di intelligenza non biologica può sembrare non solo il migliore ma forse l’unico modo per implementare tecnologie sufficientemente adattive e flessibili per affrontare efficacemente un ambiente complesso, in continua evoluzione e spesso imprevedibile, quando non ostile. Ciò che Cartesio, per esempio, riconosceva come un segno essenziale di intelligenza – la capacità di adattarsi a circostanze diverse e sfruttarle a proprio vantaggio – sarebbe una caratteristica inestimabile di qualsiasi dispositivo che cercasse di essere qualcosa di più che semplicemente smart.

			Questa impressione non è sbagliata, ma è sviante. Abbiamo osservato che il digitale sta re-ontologizzando la natura stessa (e quindi il significato) del nostro ambiente, l’infosfera, la quale al contempo sta progressivamente diventando il mondo in cui viviamo. Quindi, mentre stavamo perseguendo senza successo l’iscrizione dell’ia produttiva nel mondo, stavamo effettivamente modificando (re-ontologizzando) il mondo per adattarlo all’ia ingegneristica e riproduttiva. Il mondo sta diventando un’infosfera sempre meglio adattata alle delimitate capacità dell’ia (Floridi, 2003, 2014a). Per comprendere meglio come ciò accade, consideriamo brevemente l’industria automobilistica. Tale settore è stato in prima linea nella rivoluzione digitale e nell’ia fin dal principio, prima con la robotica industriale e ora con le automobili a guida autonoma basata sull’ia. I due fenomeni sono correlati e possono anche insegnarci una lezione molto importante allorché si tratti di comprendere come gli agenti umani e artificiali coabiteranno nei nostri ambienti.

			Consideriamo prima la robotica industriale: per esempio, un robot che dipinge il componente di un veicolo in una fabbrica. Lo spazio tridimensionale che definisce i confini entro i quali tale robot può lavorare con successo è definito l’involucro del robot. Alcune delle nostre tecnologie, come le lavastoviglie o le lavatrici, assolvono i loro compiti perché i loro ambienti sono strutturati (avvolti) attorno alle capacità elementari del robot al loro interno. Lo stesso vale, per esempio, per gli scaffali robotici nei magazzini di Amazon che sono “avvolti” attorno a loro. È l’ambiente che è progettato in modo tale da essere compatibile con i robot, non il contrario. Pertanto, non costruiamo droidi come il c-3po di Star Wars per lavare i piatti nel lavello esattamente come lo faremmo noi. Invece, avvolgiamo microambienti attorno a robot semplici per adattarli a essi e sfruttare le loro capacità limitate, in modo tale da ottenere comunque il risultato desiderato.

			L’avvolgimento era un fenomeno a sé stante (si poteva acquistare il robot con l’involucro richiesto, come una lavastoviglie o una lavatrice) o implementato all’interno delle mura di edifici industriali, attentamente ritagliati sui loro abitanti artificiali. Al giorno d’oggi, la pratica di avvolgere l’ambiente in un’infosfera adatta all’ia ha iniziato a pervadere tutti gli aspetti della realtà e sta prendendo piede quotidianamente ovunque, in casa, in ufficio e per strada. Quando parliamo di città smart, facciamo riferimento anche al fatto che stiamo trasformando gli habitat sociali in luoghi in cui i robot possono operare con successo. Da decenni avvolgiamo il mondo intorno alle tecnologie digitali in modo invisibile e senza rendercene interamente conto. Come vedremo nel terzo capitolo, il futuro dell’ia risiede anche in un maggiore avvolgimento, per esempio, in termini di 5G e Internet delle Cose (IoT), e in una crescente dimensione onlife, vale a dire nel fatto che siamo tutti costantemente connessi e trascorriamo sempre più tempo nell’infosfera, mentre tutte le nuove informazioni nascono sempre più digitali. Tornando all’industria automobilistica, le auto a guida autonoma diventeranno una merce il giorno in cui potremo avvolgere l’ambiente che le circonda.

			Negli anni Quaranta e Cinquanta, il computer era una stanza e vi camminavamo dentro per lavorare con esso e al suo interno. Programmare significava usare un cacciavite. L’interazione uomo-computer era una relazione somatica o fisica. Ricordiamoci del computer mostrato in Robinson Crusoe su Marte. Negli anni Settanta, siamo usciti dal computer, per sederci di fronte a esso. L’interazione uomo-computer divenne una relazione semantica, resa in seguito più facile dal sistema operativo per dischi, dalle righe di testo, dall’interfaccia utente grafica e dalle icone. Oggi, siamo entrati di nuovo nel computer, sotto forma di un’intera infosfera che ci circonda, spesso in modo impercettibile. Stiamo costruendo l’involucro definitivo in cui le interazioni uomo-computer sono diventate di nuovo somatiche, attraverso i touch screen, i comandi vocali, i dispositivi di ascolto, le applicazioni sensibili ai gesti, i dati di geolocalizzazione e così via. Come al solito, intrattenimento, sanità e applicazioni militari stanno guidando l’innovazione, ma il resto del mondo non è molto indietro. Se droni, veicoli a guida autonoma, tagliaerba robotici, ma anche bot e algoritmi di ogni tipo possono spostarsi “in giro” e interagire con i nostri ambienti con problemi decrescenti, non è perché è stata finalmente realizzata l’ia produttiva e cognitiva (di tipo hollywoodiano), ma perché ciò che sta “intorno” e gli ambienti con cui i nostri artefatti ingegnerizzati devono negoziare sono diventati sempre più adattati all’ia riproduttiva e ingegnerizzata e alle sue limitate capacità. In una tale infosfera adattata all’ia, l’assunto di base è che un agente può essere artificiale: questo è il motivo per cui ci viene chiesto regolarmente di dimostrare che non siamo robot, cliccando sul cosiddetto captcha, il test di Turing pubblico e completamente automatico, per distinguere computer e umani. Il test è rappresentato da stringhe di lettere leggermente alterate, eventualmente mescolate con altri segni grafici, che dobbiamo decifrare per dimostrare che siamo un umano e non un agente artificiale, per esempio, quando ci registriamo per un nuovo account online. Si tratta di un test banale per un essere umano ma realmente insormontabile per l’ia: ecco quanti pochi progressi ci sono stati nell’area cognitiva di produzione dell’intelligenza non biologica.

			Ogni giorno assistiamo all’incremento di potenza di calcolo, di dati, dispositivi (IoT), sensori, tag, satelliti, attuatori, servizi digitali, esseri umani connessi che vivono sempre di più onlife: in una parola, a un maggiore avvolgimento. Un numero crescente di lavori e attività sta diventando di natura digitale: giocare, educare, uscire con qualcuno, incontrarsi, litigare, prendersi cura, spettegolare, fare pubblicità. Facciamo tutto questo e molto di più in un’infosfera avvolta in cui siamo più ospitati analogici che ospiti digitali. Non c’è da stupirsi che i nostri agenti artificiali si comportino sempre meglio. È il loro ambiente. Come vedremo nella seconda parte, questa profonda trasformazione ontologica solleva importanti sfide etiche.





2.5 L’uso umano degli esseri umani e delle interfacce


			Avvolgere il mondo trasformando un ambiente ostile in un’infosfera adattata digitalmente significa che condivideremo i nostri habitat non solo con forze e fonti di azione naturali, animali e sociali, ma anche e talvolta principalmente con agenti artificiali. Questo non vuol dire che un vero agire artificiale di tipo intelligente sia prossimo. Non disponiamo di macchine competenti dal punto di vista semantico e realmente intelligenti che comprendono le cose, si preoccupano per esse, hanno preferenze o si appassionano a qualcosa. Abbiamo strumenti statistici così sofisticati che tecnologie puramente sintattiche possono aggirare i problemi di significato, pertinenza, comprensione, verità, intelligenza, intuizione, esperienza e così via, e fornire comunque ciò di cui abbiamo bisogno: una traduzione, la giusta immagine di un luogo, il ristorante preferito, un libro interessante, un biglietto a un prezzo migliore, un affare decisamente scontato, la canzone adatta alle nostre preferenze musicali, un film che ci piace, una soluzione più economica, una strategia più efficace, informazioni essenziali per nuovi progetti, il design per nuovi prodotti, la lungimiranza necessaria per anticipare i problemi, una migliore diagnosi, l’elemento inaspettato di cui non sapevamo neppure di aver bisogno, il supporto necessario per una scoperta scientifica o una cura medica e così via. Sono stupide come un vecchio frigorifero, eppure le nostre tecnologie smart giocano a scacchi, parcheggiano un’automobile o interpretano le scansioni mediche meglio di noi. La loro memoria (per ciò che concerne dati e algoritmi) supera l’intelligenza in un numero crescente e illimitato di compiti e problemi. Il cielo o meglio la nostra immaginazione su come sviluppare e distribuire tali tecnologie smart è il limite.

			Alcuni dei problemi che stiamo affrontando oggi, per esempio, nella sanità digitale o nei mercati finanziari, sorgono già in ambienti altamente avvolti in cui tutti i dati rilevanti (e talora gli unici disponibili) sono leggibili da macchine, cosicché decisioni e azioni possono essere compiute automaticamente da applicazioni e attuatori in grado di eseguire comandi e completare le corrispondenti procedure: dall’avvertire o esaminare un paziente all’acquistare o vendere obbligazioni. Gli esempi potrebbero facilmente moltiplicarsi. Le conseguenze dell’avvolgere il mondo per trasformarlo in un luogo adattato all’ia sono molte, e il resto del libro ne esplorerà alcune. Ma un esempio in particolare è molto significativo e ricco di conseguenze, e può essere discusso qui a titolo di conclusione: gli esseri umani possono diventare inavvertitamente parte del meccanismo. Questo è proprio ciò che Kant raccomandava di non fare mai: trattare gli esseri umani solo come mezzi anziché come fini. Eppure si sta già verificando, principalmente in due modi. Entrambi sono casi di un “uso umano degli esseri umani” (Wiener, 1954).

			In primo luogo, gli esseri umani stanno diventando nuovi mezzi di produzione digitale. Il punto è semplice: talvolta l’ia ha bisogno di capire e interpretare ciò che sta accadendo, per cui ha bisogno di motori semantici come noi per svolgere tale compito. Questa tendenza abbastanza recente è nota come computazione basata sull’umano. Un classico esempio è fornito da Amazon Mechanical Turk. Il nome deriva da un celebre automa capace di giocare a scacchi costruito da Wolfgang von Kempelen (1734-1804) alla fine del xviii secolo. L’automa divenne famoso battendo personaggi del calibro di Napoleone Bonaparte e Benjamin Franklin e ingaggiando una bella partita contro un campione come François-André Danican Philidor (1726-1795). Tuttavia, si trattava di un falso perché albergava al suo interno uno scomparto in cui si nascondeva un giocatore umano che ne controllava le operazioni meccaniche. Il Mechanical Turk adotta un trucco simile. Amazon lo descrive come un sistema di “intelligenza artificiale artificiale”. Il doppio “artificiale” è nell’originale. Si tratta di un servizio web di crowdsourcing che consente ai cosiddetti “richiedenti” di sfruttare l’intelligenza di lavoratori umani, noti come “fornitori” o, più informalmente, “turchi”, per realizzare compiti definiti hit (human intelligence tasks, compiti che richiedono intelligenza umana) che i computer non sono tuttora in grado di svolgere. Un richiedente pubblica un hit, come trascrivere registrazioni audio o taggare i contenuti negativi di un film (due esempi reali). I turchi possono sfogliare una lista, scegliere tra gli hit disponibili e completarli per una ricompensa stabilita dal richiedente. I richiedenti possono verificare se i turchi soddisfano alcune qualifiche specifiche prima di affidare loro un hit. Possono anche accettare o rifiutare il risultato inviato da un turco e questo incide sulla sua reputazione. “L’umano dentro” sta diventando il nuovo slogan. La formula vincente è semplice: macchina smart + intelligenza umana = sistema ingegnoso.

			Il secondo modo in cui gli esseri umani stanno diventando parte del meccanismo è come clienti influenzabili. Per il settore pubblicitario un cliente è un’interfaccia tra un fornitore e un conto bancario (più precisamente si dovrebbe parlare di “limite di credito”, che non coincide con il reddito disponibile, poiché i clienti possono spendere più di quanto hanno, per esempio utilizzando le loro carte di credito). Più il rapporto tra i due è fluido e privo di attriti e meglio è: per questo conviene manipolare l’interfaccia. Per manipolarla, il settore pubblicitario ha bisogno di avere quante più informazioni possibili sul cliente-interfaccia. Tuttavia, tali informazioni non possono essere ottenute a meno di fornire qualcosa in cambio al cliente. Fanno così il loro ingresso i servizi “gratuiti” online. Questi sono le valute con cui sono “acquistate” le informazioni sui clienti-interfaccia. L’obiettivo finale è quindi quello di fornire quel tanto che basta di “servizi gratuiti”, che sono costosi, per ottenere tutte le informazioni sul cliente-interfaccia che sono necessarie per garantire quel grado di manipolazione che fornisce, in relazione all’offerta, un accesso illimitato e non vincolato al conto bancario. A causa delle regole della concorrenza, un tale obiettivo non può essere conseguito da un singolo operatore. Tuttavia, lo sforzo congiunto dell’industria della pubblicità e dei fornitori fa sì che i clienti siano sempre più concepiti come un mezzo in direzione di un fine: come le interfacce di conti bancari da spingere e tirare, indirizzare e allettare. L’ia gioca un ruolo cruciale in questo contesto, ritagliando, ottimizzando e decidendo molti processi attraverso sistemi di raccomandazione (Milano, Taddeo, Floridi, 2019, 2020), un tema che sarà ulteriormente discusso nel settimo capitolo.





2.6 Conclusione: chi si adatterà a chi?


			I sistemi di ia saranno esponenzialmente più utili ed efficaci nella misura in cui ci inoltreremo nel percorso di digitalizzazione dei nostri ambienti e di espansione dell’infosfera. L’avvolgimento è una tendenza robusta, cumulativa e che si perfeziona progressivamente. Non ha nulla a che fare con una singolarità futura, perché non si basa su qualche speculazione su super ia che conquisteranno il mondo nel prossimo futuro (vedi capitolo 10). Nessuno Spartaco artificiale guiderà una rivolta digitale. Tuttavia, avvolgere il mondo e in tal modo agevolare l’emergere di agenti artificiali e il successo dei loro comportamenti è un processo che solleva sfide concrete e urgenti, che discuterò nella seconda metà di questo libro. Qui, mi sia concesso di illustrarne alcune affidandomi a una parodia.

			Immaginiamo due persone, A e H. Sono sposate e desiderano davvero far funzionare la loro relazione. A, che fa sempre di più in casa, è persona inflessibile, testarda, insofferente agli errori e restia a cambiare. H invece è esattamente l’opposto, ma sta anche diventando progressivamente più pigra e dipendente da A. Il risultato è una situazione squilibrata, in cui A finisce per plasmare la relazione e distorcere i comportamenti di H praticamente, se non intenzionalmente. Se la relazione funziona, è perché è attentamente ritagliata attorno a A. La relazione diviene interpretabile nei termini della dialettica hegeliana Servo-Padrone (Hegel, 1807). Ora, le tecnologie smart svolgono il ruolo di A nell’analogia precedente, mentre i loro utenti umani sono chiaramente H. Il rischio che corriamo è che, avvolgendo il mondo, le nostre tecnologie e in particolare l’ia possano plasmare i nostri ambienti fisici e concettuali e costringerci a adattarci a essi perché questo è il modo più semplice o migliore, e talvolta l’unico, per far funzionare le cose. In fin dei conti, dato che l’ia è il coniuge stupido ma laborioso e l’umanità quello intelligente ma pigro, chi si adatterà a chi? Il lettore probabilmente ricorderà molti episodi della vita reale in cui qualcosa non poteva assolutamente essere fatto, o doveva essere fatto in modo scomodo o sciocco, perché quello era il solo modo per far fare al sistema computerizzato quello che doveva fare. “Il computer dice no”, come risponderebbe il personaggio Carol Beer nella commedia britannica Little Britain a qualsiasi richiesta del cliente. Ecco un esempio più concreto, per quanto banale. Il rischio è che potremmo finire per costruire case con pareti rotonde e mobili con gambe abbastanza alte per adattarle alle capacità di Roomba4 in modo molto più efficace. Vorrei certamente che la nostra casa fosse più adatta a Roomba. Abbiamo adattato il nostro giardino per assicurarci che Ambrogio, un robot tagliaerba, possa lavorare con successo. Gli esempi sono utili per illustrare non solo il rischio ma anche l’opportunità rappresentata dal potere delle tecnologie digitali di re-ontologizzare e avvolgere il mondo.

			Sono tanti i luoghi “rotondi” in cui viviamo, dagli igloo alle torri medievali, dai bovindo agli edifici pubblici dove gli angoli delle stanze sono arrotondati per motivi sanitari. Se trascorriamo la maggior parte del nostro tempo all’interno di luoghi squadrati, è per un altro insieme di tecnologie legate alla produzione in serie di mattoni e infrastrutture in calcestruzzo e alla facilità dei tagli diritti del materiale da costruzione. È la sega circolare meccanica che, paradossalmente, genera un mondo ad angolo retto. In entrambi i casi, luoghi squadrati e rotondi sono stati costruiti seguendo le tecnologie predominanti, piuttosto che in forza delle scelte dei loro potenziali abitanti. Sulla base di questo esempio, è facile percepire come l’opportunità rappresentata dal potere di re-ontologizzazione del digitale si presenti in tre forme: rifiuto, accettazione critica e design proattivo. Diventando più criticamente consapevoli del potere re-ontologizzante dell’ia e delle applicazioni smart, potremmo essere in grado di evitare le peggiori forme di distorsione (rifiuto) o almeno essere coscientemente tolleranti nei loro confronti (accettazione), specialmente quando non è importante (penso alla lunghezza delle gambe del divano in casa nostra compatibili con Roomba) o quando si tratta di una soluzione temporanea, in attesa di un design migliore. In quest’ultimo caso, essere in grado di immaginare come sarà il futuro e quali esigenze di adattamento saranno poste dall’ia e dal digitale più in generale ai loro utenti umani può aiutarci a escogitare soluzioni tecnologiche capaci di diminuire i loro costi antropologici e accrescere i loro benefici ambientali. In breve, il design umano intelligente (il gioco di parole è voluto) dovrebbe svolgere un ruolo maggiore nel plasmare il futuro delle nostre interazioni con gli artefatti smart attuali e futuri, e gli ambienti che condividiamo con loro. Dopotutto, è un segno di intelligenza far lavorare la stupidità per noi.5 È giunto il momento di esaminare il presente e il futuro prevedibile dell’ia.



* * *





			 				 					1. Il dibattito su questo tema è infinito e così la bibliografia relativa: per una ricostruzione breve e molto leggibile, da una prospettiva neuroscientifica, del perché il cervello non è un computer, Epstein (2016) è un buon punto di partenza. Per una ricostruzione più ampia e dettagliata ma ugualmente piacevole vedi Cobb (2020a), o l’estratto (Cobb, 2020b).



				 					2. Ho incontrato John quando era sulla settantina in diversi incontri e poi più regolarmente per un progetto di libro a cui entrambi abbiamo contribuito. Penso che non abbia mai cambiato idea sulla reale fattibilità della vera ia come equivalente non biologico (o forse come versione migliore) dell’intelligenza umana.



				 					3. https://deepmind.com/blog/article/alphagos-next-move.



				 					4. http://www.irobot.com/.



				 					5. Ho discusso la natura della filosofia applicata dell’informazione in Floridi (2002) e Floridi (2004), e la rilevanza e trasparenza dell’informazione in Floridi (2008c) e Turilli e Floridi (2009).





3


			Futuro: lo sviluppo prevedibile dell’ia

			Sommario In precedenza, nel secondo capitolo, ho sostenuto che l’ia non dovrebbe essere interpretata come un matrimonio tra un’intelligenza di tipo biologico e artefatti ingegnerizzati, ma come un divorzio tra l’agire e l’intelligenza, cioè una scissione tra la capacità di affrontare problemi e compiti con successo in vista di uno scopo e l’esigenza di essere intelligenti nel farlo. Nel presente capitolo, utilizzo questa interpretazione dell’ia come una nuova forma di agire efficace ma non-intelligente per scrutare il suo futuro. Dopo una breve introduzione nel primo paragrafo, relativa alle difficoltà che investono qualsiasi esercizio di previsione, nei paragrafi secondo e terzo sostengo che i probabili sviluppi e le possibili sfide dell’ia dipenderanno dalla spinta verso i dati sintetici, dalla crescente traduzione di problemi difficili in problemi complessi, dalla tensione tra regole regolative e costitutive alla base delle aree di applicazione dell’ia, e quindi dal progressivo adattamento dell’ambiente all’ia piuttosto che dell’ia all’ambiente (ciò che ho definito nel capitolo precedente come avvolgimento). Nel quarto paragrafo, ritorno sull’importanza del design e della responsabilità nel produrre il corretto tipo di ia per trarre vantaggio dagli sviluppi di cui sopra. Nella conclusione, discuto le stagioni dell’ia, e in particolare i suoi inverni, per sottolineare le lezioni che avremmo dovuto apprendere, e possiamo ancora assimilare e applicare, per sfruttare al meglio questa straordinaria tecnologia. Il capitolo conclude la prima parte del libro, con una breve introduzione filosofica a passato, presente e futuro dell’ia.





3.1 Introduzione: scrutare nei semi del tempo


			L’ia ha dominato i titoli dei giornali recenti, con le sue promesse e sfide, i suoi rischi, successi e fallimenti. Quale futuro possiamo prevedere per l’ia? Naturalmente, le previsioni più accurate vengono effettuate con il senno di poi. Ma se qualche trucco non è accettabile, allora le persone in gamba scommettono su ciò che non è controverso o non può essere verificato. Sul lato non controverso, si può menzionare la maggiore pressione che proverrà dai legislatori per garantire che le applicazioni di ia siano in linea con aspettative socialmente accettabili. Si veda, per esempio, la normativa proposta in merito dall’Unione Europea (Proposta di regolamento del Parlamento e del Consiglio che stabilisce regole armonizzate sull’intelligenza artificiale [com (2021) 206 definitivo]). Sul lato non verificabile, alcune persone continueranno a vendere previsioni catastrofiche, con scenari distopici che hanno luogo in un futuro sufficientemente distante da garantire che tali Geremia non saranno più in circolazione per essere smentiti. La paura vende sempre bene, come i film sui vampiri o sugli zombi. Perciò, dobbiamo aspettarci di più. Ciò che è difficile, e potrebbe risultare piuttosto imbarazzante in seguito, è cercare di “scrutare nei semi del tempo, e dire quali chicchi germoglieranno, e quali no” (Macbeth, atto i, scena iii), cioè tentare di capire in che direzione è più probabile che l’ia stia andando o dove potrebbe non andare, dato il suo stato attuale, e su questa base provare a tracciare la mappa delle sfide etiche che bisognerebbe prendere sul serio. È ciò che cercherò di fare in questo capitolo, dove sarò cauto nell’individuare i percorsi più probabili, ma non così cauto da evitare ogni rischio di essere smentito da qualcuno che leggerà questo libro tra pochi anni.

			Parte della difficoltà è individuare il corretto livello di astrazione (Floridi, 2008b, 2008c), vale a dire identificare l’insieme di osservabili rilevanti (“i semi del tempo”) su cui concentrarsi, poiché sono tali osservabili che faranno la vera, significativa differenza. Nel nostro caso, sosterrò che i migliori osservabili sono forniti da un’analisi della natura:

			a)	dei dati utilizzati dall’ia per realizzare le proprie prestazioni;

			b)	dei problemi che è ragionevole attendersi che l’ia sia in grado di risolvere.1

			Esaminiamo prima (a) e poi (b) nei prossimi due paragrafi.





3.2 Dati storici, ibridi e sintetici e il bisogno di ludicizzazione


			Dicono che i dati siano il nuovo petrolio. Non la penso così. I dati sono durevoli, riutilizzabili, rapidamente trasportabili, facilmente duplicabili e simultaneamente condivisibili (non rivali) senza fine, mentre il petrolio non ha alcuna di queste proprietà. Disponiamo di enormi quantità di dati che continuano a crescere, laddove il petrolio è, invece, una risorsa limitata e in diminuzione. Il petrolio ha un prezzo ben distinto, mentre la monetizzazione degli stessi dati dipende quantomeno da chi li utilizza e per quale scopo, per non parlare di circostanze come quando, dove e così via. E tutto questo ancor prima di introdurre le questioni giuridiche ed etiche che emergono quando sono in gioco i dati personali, o l’intero dibattito sulla proprietà dei dati (“i miei dati” è un’espressione molto più simile alle “mie mani” che non al “mio petrolio”: Floridi, 2013). Dunque, l’analogia è a dir poco forzata. Ciò non significa che sia totalmente inutile. Perché è vero che i dati, come il petrolio, sono una risorsa preziosa e devono essere raffinati per estrarne il valore. In particolare, senza dati, gli algoritmi – inclusa l’ia – non vanno da nessuna parte, come un motore con un serbatoio vuoto. L’ia ha bisogno di dati per essere addestrata e pertanto di dati per applicare il suo addestramento. Naturalmente, l’ia può essere estremamente flessibile: sono i dati che determinano il suo ambito di applicazione e grado di successo. Per esempio, nel 2016 Google ha utilizzato il sistema di apprendimento automatico di DeepMind per ridurre il proprio consumo energetico:

			Poiché l’algoritmo è uno strumento concettuale che si applica a finalità diverse per comprendere dinamiche complesse, prevediamo di applicarlo ad altre sfide nell’ambiente dei data center e oltre nei prossimi mesi. Possibili applicazioni di questa tecnologia includono il miglioramento dell’efficienza di conversione delle centrali elettriche (ottenendo più energia dalla stessa unità di input), la riduzione dell’energia di produzione di semiconduttori e dell’utilizzo di acqua o l’ottimizzazione degli impianti di produzione per aumentare la produttività.2

			È noto che l’ia, intesa come Machine Learning (apprendimento automatico), apprende dai dati che riceve e migliora progressivamente i suoi risultati. Se mostriamo un grandissimo numero di foto di cani a una rete neurale, alla fine imparerà a riconoscere i cani in modo sempre migliore, compresi i cani che non ha mai visto prima. Per ottenere tale risultato, di solito sono necessarie enormi quantità di dati, e di regola quanti più sono i dati, tanto migliore è il risultato. Per esempio, in test recenti un team di ricercatori dell’Università della California a San Diego ha addestrato un sistema di ia su 101,6 milioni di punti dati di cartelle cliniche elettroniche (incluso il testo scritto da medici e i risultati dei test di laboratorio) sulla base di 1.362.559 visite di pazienti pediatrici in un importante centro medico a Guangzhou, in Cina. Una volta addestrato, il sistema ia è stato in grado di mostrare:

			[…] un’elevata accuratezza diagnostica su più sistemi di organi ed è paragonabile a pediatri esperti nella diagnosi di malattie infantili comuni. Il nostro studio fornisce una verifica di funzionamento per l’implementazione di un sistema basato su ia come strumento di ausilio per aiutare i medici ad affrontare grandi quantità di dati, incrementare le valutazioni diagnostiche e fornire supporto decisionale clinico in casi di incertezza o complessità diagnostica. Sebbene questo impatto possa risultare più evidente nelle aree in cui gli operatori sanitari sono relativamente carenti, è probabile che i vantaggi di un tale sistema di ia siano universali. (Liang, Tsui, Ni et al., 2019)

			Tuttavia, recentemente l’ia è talmente migliorata che, in taluni casi, si sta passando da un’enfasi sulla quantità di grandi masse di dati, a volte impropriamente chiamati Big Data (Floridi, 2012a), a un’enfasi sulla qualità di insiemi di dati ben curati. Per esempio, nel 2018, DeepMind, in collaborazione con l’ospedale Moorfields Eye di Londra, ha addestrato un sistema di ia per identificare sintomi di malattie degli occhi pericolose per la vista utilizzando i dati della tomografia a coerenza ottica, una tecnica di elaborazione di immagini che genera immagini 3D della parte posteriore dell’occhio. Alla fine, il team è riuscito a

			dimostrare una capacità di prestazione nel formulare raccomandazioni ai pazienti equivalente o superiore a quella degli esperti su una gamma di malattie retiniche pericolose per la vista a seguito di un addestramento su solo 14.884 scansioni. (De Fauw, Ledsam, Romera-Paredes et al., 2018, p. 1342, corsivo mio)

			Sottolineo “solo 14.884 scansioni” perché i “piccoli dati” di alta qualità costituiscono uno degli scenari futuri dell’ia. L’ia avrà maggiori possibilità di successo ogni volta che insiemi di dati ben curati, aggiornati e completamente affidabili saranno disponibili e accessibili per addestrare un sistema in un’area specifica di applicazione. Ciò è piuttosto ovvio e a stento rappresenta una nuova previsione. Ma è un passo avanti concreto, che ci aiuta a guardare più lontano, oltre la narrativa dei “Big Data”. Se la qualità è importante, la provenienza è fondamentale. Da dove provengono i dati? Nell’esempio precedente, sono stati forniti dall’ospedale. Tali dati sono talvolta noti come storici, autentici o provenienti dalla vita reale (d’ora in poi li chiamerò semplicemente storici). Ma sappiamo anche che l’ia può generare i propri dati. Non parlo di metadati o dati secondari sui loro utilizzi (Floridi, 2010b). Faccio riferimento al loro input principale. Chiamerò sintetici i dati interamente generati dall’ia. Purtroppo, il termine ha un’etimologia ambigua, dal momento che ha iniziato a essere utilizzato negli anni Novanta in riferimento a dati storici resi anonimi prima di essere utilizzati, di regola per proteggere privacy e riservatezza. Questi dati sono sintetici solo nel senso che sono stati sintetizzati da dati storici, per esempio attraverso il “mascheramento”.3 Hanno una risoluzione inferiore, ma non sono generati da una fonte artificiale. La distinzione tra i dati storici e quelli da essi sintetizzati è utile, ma non è ciò che intendo in questo contesto, dove voglio sottolineare la provenienza interamente ed esclusivamente artificiale dei dati in questione. È una distinzione ontologica, che può avere importanti implicazioni in termini epistemologici, soprattutto quando è in gioco la nostra capacità di spiegare i dati sintetici prodotti e l’addestramento raggiunto dall’ia che li utilizza (Watson, Krutzinna, Bruce et al., 2019). Un celebre esempio può aiutare a spiegare la differenza.

			In passato, giocare a scacchi contro un computer significava giocare contro i migliori giocatori umani che avessero mai preso parte al gioco. Perciò, una delle caratteristiche di Deep Blue, il programma di scacchi della ibm che aveva sconfitto il campione del mondo Garry Kasparov, consisteva in

			un uso efficace di un database delle partite di un grande maestro. (Campbell, Hoane Jr, Hsu, 2002, p. 57)

			Ma AlphaZero, l’ultima versione del sistema di ia sviluppato da DeepMind, ha imparato a giocare meglio di chiunque altro, e in effetti di qualsiasi altro software, facendo affidamento soltanto sulle regole del gioco, senza alcun input di dati da alcuna fonte esterna. Non aveva alcuna memoria storica:

			Il gioco degli scacchi ha rappresentato l’apice della ricerca sull’intelligenza artificiale per diversi decenni. I programmi all’avanguardia si basano su potenti motori che ricercano molti milioni di posizioni, sfruttando competenze proprie dell’ambito degli scacchi e sofisticati adattamenti a esso [corsivo mio, questi sono i dati non sintetici]. AlphaZero è un generico algoritmo di ricerca e di apprendimento per rinforzo, ideato originariamente per il gioco Go, che ha ottenuto risultati superiori in poche ore […] senza alcuna conoscenza dell’ambito eccetto le regole degli scacchi. (Silver, Hubert, Schrittwieser et al., 2018, p. 1144, corsivo mio)

			AlphaZero ha imparato giocando contro se stesso, generando così i propri dati sintetici relativi agli scacchi. Non sorprende che il grande maestro di scacchi Matthew Sadler e la maestra internazionale Natasha Regan,

			che hanno analizzato migliaia di partite di scacchi di AlphaZero per il loro libro Game Changer (gennaio 2019), affermano che il suo stile è diverso da qualsiasi motore scacchistico tradizionale. “È come scoprire i taccuini segreti di un grande giocatore del passato”, dice Matthew.4

			AlphaZero ha generato i propri dati sintetici, e questo è stato sufficiente per il suo addestramento. Questo è ciò che intendo per dati sintetici.

			I dati realmente sintetici, nel senso in cui li ho definiti in questo contesto, hanno alcune straordinarie proprietà. Non solo condividono quelle elencate all’inizio del paragrafo (sono durevoli, riutilizzabili, rapidamente trasportabili, facilmente duplicabili, simultaneamente condivisibili senza fine ecc.). Sono anche puliti e affidabili (in termini di accuratezza), non violano privacy o riservatezza nella fase di sviluppo (sebbene i problemi persistano nella fase di implementazione, a causa di possibili violazioni della privacy su base predittiva (Crawford, Schultz, 2014), non sono immediatamente sensibili (la sensibilità durante la fase di implementazione è tuttora rilevante), se vengono persi non è un disastro perché possono essere ricreati e sono perfettamente formattati per essere utilizzati dal sistema che li genera. Con i dati sintetici l’ia non è mai costretta ad abbandonare il suo spazio digitale, dove può esercitare il controllo completo su qualsiasi input e output dei suoi processi. In termini più epistemologici, con i dati sintetici l’ia gode della posizione privilegiata della conoscenza del costruttore, che conosce la natura intrinseca e il funzionamento di qualcosa perché lo ha costruito (Floridi, 2018). Ciò spiega perché sono così popolari, per esempio, nei contesti di sicurezza, dove l’ia viene impiegata per testare i sistemi digitali. Inoltre, i dati sintetici possono essere anche prodotti, talvolta, in modo più rapido ed economico rispetto ai dati storici. AlphaZero è diventato il miglior giocatore di scacchi del globo in nove ore (ci sono volute dodici ore per Shogi e tredici giorni per Go).

			Tra dati storici più o meno mascherati (impoveriti attraverso una risoluzione inferiore, per esempio tramite l’anonimizzazione) e dati puramente sintetici, esiste una varietà di dati più o meno ibridi, che possiamo raffigurare come un prodotto di dati storici e sintetici. L’idea di base è utilizzare i dati storici per ottenere alcuni nuovi dati sintetici che non sono semplicemente dati storici impoveriti. Un buon esempio, introdotto da Goodfellow e coautori (2014), è fornito dalle reti generative avverse:

			Due reti neurali – un Generatore e un Discriminatore [mie le maiuscole nel testo] – competono l’una contro l’altra per avere successo in un gioco. Lo scopo del gioco per il Generatore è di trarre in inganno il Discriminatore con esempi che paiono simili al set di addestramento. […] Quando il Discriminatore rigetta un esempio prodotto dal Generatore, il Generatore impara qualcosa di più su come si presenta un buon esempio. […] In altri termini, il Discriminatore fa trapelare informazioni su quanto il Generatore fosse vicino e su come dovrebbe procedere per avvicinarsi. […] Col passare del tempo, il Discriminatore impara dal set di addestramento e invia segnali sempre più significativi al Generatore. Quando ciò si verifica, il Generatore si avvicina sempre di più all’apprendimento dell’aspetto degli esempi dal set di addestramento. Ancora una volta, gli unici input che il Generatore ha sono una distribuzione iniziale di probabilità (spesso la distribuzione normale) e l’indicatore che riceve dal Discriminatore. Non vede mai alcun esempio reale [corsivo mio].5

			Il Generatore impara a creare dati sintetici che sono equivalenti a dati di input conosciuti. Per questo c’è qui un po’ di natura ibrida, perché il Discriminatore deve avere accesso ai dati storici per “addestrare” il Generatore. Ma i dati generati dal Generatore sono nuovi, e non semplicemente un’astrazione a partire dai dati di addestramento. Pertanto, non si tratta di un caso di partenogenesi, come AlphaZero che dà alla luce i propri dati, ma vi si avvicina abbastanza da produrre comunque alcune delle caratteristiche molto interessanti dei dati sintetici. Per esempio, i volti umani sintetici creati da un Generatore non sollevano problemi in termini di privacy, consenso o riservatezza nella fase di sviluppo.6

			Molti metodi per generare dati ibridi o sintetici sono già disponibili o in fase di sviluppo, spesso con caratteristiche specifiche per settore. Esistono anche tendenze altruistiche per rendere pubblicamente disponibili tali insiemi di dati (Howe, Stoyanovich, Ping et al., 2017). Chiaramente, il futuro dell’ia non risiede soltanto nei “piccoli dati” ma anche, o forse principalmente, nella sua crescente capacità di generare i propri dati. Si tratterebbe di uno sviluppo notevole e ci si può aspettare che vengano compiuti sforzi significativi in tale direzione. La domanda seguente è: quale fattore può far spostare l’indicatore, nella Figura 3.1, da sinistra a destra?



			La differenza è costituita dal processo genetico, cioè dalle regole usate per creare i dati. I dati storici sono ottenuti tramite regole di registrazione, in quanto sono il risultato di osservazioni del comportamento di un sistema. I dati sintetizzati sono ottenuti tramite regole di astrazione, che eliminano, mascherano o offuscano alcuni gradi di risoluzione a partire dai dati storici, per esempio mediante l’anonimizzazione. Dati ibridi e realmente sintetici possono essere generati tramite regole vincolanti o costitutive. Non esiste una mappatura in scala uno a uno, ma è utile considerare i dati ibridi come i dati su cui dobbiamo fare affidamento, utilizzando regole vincolanti, quando non disponiamo di regole costitutive in grado di generare dati sintetici da zero. Occorre chiarire questo punto.

			L’indicatore si sposta facilmente verso i dati sintetici ogniqualvolta l’ia si occupa di “giochi” – intesi come qualsiasi interazione formale in cui i giocatori competono secondo regole e in vista del raggiungimento di un obiettivo – le cui regole sono costitutive e non semplicemente vincolanti. La differenza che intendo tracciare7 diventa chiara se si mettono a confronto gli scacchi e il calcio. Entrambi sono giochi, ma negli scacchi le regole stabiliscono le mosse valide e non valide prima che sia possibile una qualsiasi attività di tipo scacchistico; perciò, generano tutte e solo le mosse accettabili. Mentre nel calcio un’attività precedente – come calciare un pallone – è “regolamentata” o strutturata da regole che arrivano dopo l’attività. Le regole non determinano né possono determinare le mosse dei giocatori, ma pongono semplicemente limiti a quali mosse siano “valide”. Negli scacchi, come in tutti i giochi da tavolo le cui regole sono costitutive (Dama, Go, Monopoli, Shogi ecc.), l’ia può utilizzare le regole per provare qualsiasi possibile mossa valida che vuole esplorare. In nove ore, AlphaZero ha giocato 44 milioni di partite di addestramento. Per avere un’idea dell’entità del risultato si consideri che la Opening Encyclopedia 2018 contiene circa 6,3 milioni di partite, selezionate dall’intera storia degli scacchi. Ma nel calcio questo non avrebbe senso perché le regole non costituiscono il gioco ma si limitano a modellarlo. Ciò non significa che l’ia non possa giocare a calcio virtuale; contribuire a identificare la migliore strategia per vincere contro una squadra di cui siano registrati i dati relativi a partite e strategie precedenti; aiutare a identificare potenziali giocatori o contribuire ad allenarli meglio. Naturalmente, tutte queste applicazioni sono ora banalmente fattibili e già presenti. Quello che voglio dire è che in tutti questi casi sono richiesti dati storici. Invece, quando

			1. 	un processo o un’interazione può essere trasformata in un gioco e

			2. 	il gioco può essere trasformato in un gioco formato da regole costitutive, allora

			3. 	l’ia sarà in grado di generare i propri dati, completamente sintetici, ed essere il miglior “giocatore” su questo pianeta, realizzando ciò che ha fatto AlphaZero con gli scacchi (questo processo è parte dell’avvolgimento descritto nel secondo capitolo).

			Per dirla con Wiener:

			Il miglior modello materiale di un gatto è un altro, o preferibilmente lo stesso, gatto. (Rosenblueth, Wiener, 1945, p. 316)

			Idealmente, i dati migliori su cui addestrare un’ia sono dati completamente storici o dati completamente sintetici generati dalle stesse regole che hanno generato i dati storici. In qualsiasi gioco da tavolo, questo accade per impostazione predefinita. Ma nella misura in cui uno qualsiasi dei due passaggi (1)-(2), di cui sopra, sia difficile da raggiungere, è probabile che l’assenza di regole o la presenza di regole meramente vincolanti rappresentino un limite. Non abbiamo il gatto vero e proprio, ma solo un suo modello più o meno affidabile. Le cose possono diventare più complicate quando ci rendiamo conto che, nei giochi reali, le regole vincolanti sono semplicemente imposte convenzionalmente su un’attività che si è verificata in precedenza, mentre nella vita reale, quando osserviamo alcuni fenomeni, per esempio il comportamento di una tipologia di tumore in uno specifico insieme di pazienti in determinate circostanze, le regole genetiche devono essere estratte dal “gioco” reale attraverso la ricerca scientifica (oggi possibilmente basata sull’ia). Per esempio, non conosciamo e forse non conosceremo mai quali siano le esatte “regole” che presiedono allo sviluppo dei tumori cerebrali. Disponiamo di alcuni principi e teorie generali in base ai quali comprendiamo il loro sviluppo. Perciò, in questa fase (che potrebbe essere una fase permanente), non c’è modo di “ludicizzare” (cioè trasformare in un gioco nel senso già specificato; evito l’espressione “gamificare” che ha una diversa e consolidata accezione) tumori cerebrali in un “gioco di regole costitutive” (si pensi agli scacchi), in modo tale che un sistema di ia, giocando secondo le regole identificate, possa generare propri dati sintetici sui tumori cerebrali che sarebbero equivalenti ai dati storici che potremmo raccogliere, facendo per tali tumori quanto AlphaZero ha fatto per le partite di scacchi. Ciò non è necessariamente un problema. Al contrario l’ia, basandosi su dati storici o ibridi (per esempio, scansioni cerebrali) e apprendendo da essi, può ottenere risultati ancora migliori di quelli degli esperti ed espandere le proprie capacità oltre i set finiti di dati storici esistenti (per esempio, scoprendo nuovi modelli di correlazioni) o fornire servizi accessibili dove difettano le competenze. È già un grande successo se si può estrarre un numero sufficiente di regole vincolanti per produrre dati affidabili in silico. Tuttavia, senza un sistema affidabile di regole costitutive, alcuni dei vantaggi sopra menzionati dei dati sintetici non sarebbero pienamente disponibili. La vaghezza di questa affermazione è dovuta al fatto che possiamo ancora utilizzare dati ibridi.

			La ludicizzazione e la presenza o assenza di regole vincolanti/costitutive non sono rigidi limiti mutualmente esclusivi. Non dimentichiamo che i dati ibridi possono contribuire a sviluppare dati sintetici. È probabile che, in futuro, diventerà sempre più chiaro quando database di dati storici di alta qualità risultino essere assolutamente necessari e inevitabili, cioè quando è indispensabile il gatto reale, per parafrasare Wiener, e pertanto quando dovremo gestire questioni relative a disponibilità, accessibilità, rispetto delle norme giuridiche e, nel caso di dati personali, privacy, consenso, sensibilità e altre questioni etiche. Tuttavia, la tendenza verso la generazione di dati quanto più possibile sintetici (sintetizzati, più o meno ibridi, fino a diventare completamente sintetici) è probabilmente un Sacro Graal dell’ia. Per questo mi aspetto che la comunità dell’ia spinga in modo convinto in quella direzione: il modello del gatto senza il gatto, per riferirsi ancora una volta all’immagine di Wiener. Generare sempre più dati non storici, spostando il più possibile l’indicatore verso destra, richiederà una “ludicizzazione” dei processi, e per tale motivo mi aspetto anche che la comunità dell’ia sia sempre più interessata all’industria dei giochi, perché è lì che probabilmente si trovano le migliori competenze in materia di “ludicizzazione”. Inoltre, in termini di risultati negativi, le prove matematiche dell’impossibilità di ludicizzare intere tipologie o aree di processi o interazioni dovrebbero essere le benvenute al fine di chiarire dove o fino a che punto un approccio del tipo di AlphaZero potrebbe non essere mai realizzabile dall’ia.





3.3 Problemi difficili, problemi complessi e il bisogno di avvolgimento


			Nel secondo capitolo, ho affermato che probabilmente è preferibile comprendere l’ia come una riserva di capacità di agire che può essere usata per risolvere problemi ed eseguire compiti con successo. L’ia consegue i propri obiettivi scindendo la capacità di eseguire un compito con successo da qualsiasi esigenza di essere intelligente nel farlo. L’app nel mio cellulare non deve essere intelligente per giocare a scacchi meglio di me. Ogni volta che questa scissione è realizzabile, in linea di principio diventa possibile qualche soluzione di ia. Questa è la ragione per cui comprendere il futuro dell’ia significa anche comprendere la natura dei problemi in relazione ai quali tale scissione può essere tecnicamente realizzabile, almeno in teoria, ed economicamente attuabile nella pratica. Ora, molti dei problemi che cerchiamo di risolvere con l’ia si verificano nel mondo fisico: dalla guida alla scansione di etichette in un supermercato, dalla pulizia di pavimenti o finestre al taglio dell’erba in giardino. Perciò, il lettore potrebbe pensare all’ia in termini di robotica nel resto di questo paragrafo. Tuttavia, non sto parlando solo di robotica: per esempio, anche applicazioni smart per allocare prestiti o interfacce smart che facilitano e migliorano le interazioni con l’Internet delle Cose sono oggetto dell’analisi. Con ciò intendo suggerire che, allo scopo di comprendere gli sviluppi dell’ia in relazione ad ambienti analogici e digitali, è utile mappare i problemi in base alle risorse che sono necessarie per risolverli e capire in che misura l’ia può disporre di tali risorse. Mi riferisco alle risorse computazionali e, pertanto, ai gradi di complessità; e alle risorse relative alle abilità e, pertanto, ai gradi di difficoltà.

			I gradi di complessità di un problema sono ben noti e ampiamente studiati nella teoria computazionale (Arora, Barak, 2009; Sipser, 2012). Non dirò molto su questa dimensione ma mi limito a rimarcare che è altamente quantitativa e che la trattabilità matematica che offre è dovuta alla disponibilità di criteri standard di confronto, forse anche idealizzati ma chiaramente definiti, come le risorse computazionali di una macchina di Turing. Se disponiamo di un “metro”, possiamo misurare le lunghezze. Parimenti, se adottiamo una macchina di Turing come punto di partenza, possiamo calcolare quanto tempo, in termini di passaggi, e quanto spazio, in termini di memoria o nastro, “consuma” un problema computazionale per essere risolto. Per motivi di semplicità e tenendo presente che, se necessario, è possibile ottenere gradi di precisione finemente granulari e sofisticati utilizzando strumenti della teoria della complessità, conveniamo di mappare la complessità di un problema (trattato dall’ia in termini di spazio-tempo = memoria e passaggi richiesti) da 0 (semplice) a 1 (complesso).

			I gradi di difficoltà di un problema, intesi in termini di abilità richieste per risolverlo, dall’accendere e spegnere una luce a stirare delle camicie, richiedono in questo caso qualcosa di più di una stipulazione per essere mappati, perché di solito la letteratura pertinente, per esempio, sullo sviluppo motorio umano, non si concentra su una tassonomia dei problemi basata sulle risorse necessarie, ma su una tassonomia basata sulla valutazione delle prestazioni degli agenti umani e delle loro capacità o abilità dimostrate nella risoluzione di un problema o nell’esecuzione di un compito. Si tratta anche di una letteratura più qualitativa. In particolare, ci sono molte maniere per valutare una prestazione e quindi svariati modi per catalogare i problemi relativi alle abilità, ma una distinzione standard è tra abilità motorie grossolane e fini. Le abilità grosso-motorie richiedono l’uso di grandi gruppi muscolari, per eseguire attività come camminare o saltare, prendere o calciare una palla. Le abilità motorie fini richiedono l’uso di gruppi muscolari più piccoli, nei polsi, nelle mani, nelle dita, nei piedi e nelle dita dei piedi, per svolgere compiti come lavare i piatti, scrivere, digitare, usare o suonare uno strumento. Nonostante le precedenti difficoltà, possiamo riconoscere immediatamente che abbiamo a che fare con diversi gradi di difficoltà. Ancora una volta, per motivi di semplicità e ricordando che anche in questo caso gradi di precisione finemente granulari e sofisticati possono essere ottenuti, se necessario, utilizzando strumenti della psicologia dello sviluppo, conveniamo di mappare la difficoltà di un problema (trattato dall’ia in termini di abilità richieste) da 0 = facile, a 1 = difficile. Siamo adesso pronti per mappare le due dimensioni, nella Figura 3.2, dove ho aggiunto quattro esempi.



			Accendere la luce è un problema la cui soluzione ha un grado molto basso di complessità (pochissimi passaggi e stati) e di difficoltà (anche un bambino può farlo). Tuttavia, allacciarsi le scarpe richiede capacità motorie avanzate, così come allacciare quelle altrui, per cui si tratta di un’attività con bassa complessità (facile), ma che richiede un’abilità elevata (difficile). Come ha osservato il ceo di Adidas Kasper Rørsted nel 2017:

			La sfida più grande per il settore calzaturiero è come creare un robot che inserisca il laccio nella scarpa. Non sto scherzando. Oggi è un processo interamente manuale. Non esiste una tecnologia per questo.8

			Lavare i piatti è l’opposto: può richiedere molti passaggi e spazio, anzi tanti più quanti più sono i piatti da lavare, ma non è difficile (persino un filosofo come me può farlo). E, naturalmente, in alto a destra troviamo lo stirare le camicie, che è sia un’attività che consuma risorse, come lavare i piatti, sia un’attività impegnativa in termini di abilità. Si tratta, dunque, di un’attività al contempo complessa e difficile, che è di regola la mia scusa per cercare di evitarla. Usando gli esempi precedenti del calcio e degli scacchi, è possibile dire che giocare a calcio è semplice ma difficile, mentre giocare a scacchi è facile (puoi imparare le regole in pochi minuti) ma molto complesso: ecco perché l’ia può vincere contro chiunque a scacchi, ma una squadra di androidi che vinca la coppa del mondo di calcio è fantascienza.

			Abbiamo visto che avvolgere significa adattare l’ambiente e le attività alle capacità dell’ia. Più sofisticate sono queste capacità, meno necessario è l’avvolgimento, ma stiamo cercando un compromesso, una sorta di equilibrio, tra robot in grado di cucinare9 e robot in grado di cucinare gli hamburger.10 Parimenti, in un aeroporto, che è un ambiente altamente controllato e perciò più facilmente “avvolgibile”, una navetta potrebbe essere un veicolo autonomo, ma sembra improbabile modificare lo scuolabus di una cittadina, dato che il suo autista deve essere in grado di operare in circostanze estreme e difficili (campagna, neve, nessun segnale, nessuna copertura satellitare ecc.), prendersi cura dei bambini, aprire porte, spostare biciclette ecc., tutte attività che sono più improbabili (attenzione, non logicamente impossibili; su questa distinzione si dirà di più nel decimo capitolo) da avvolgere.

			In una prospettiva simile, nel 2016, Nike ha lanciato HyperAdapt 1.0, le sue scarpe automatiche elettroniche che si allacciano da sole, non sviluppando un’ia che le allacci al posto nostro, ma reinventando il concetto di cosa significa adattare le scarpe ai piedi: ogni scarpa ha un sensore, una batteria, un motore e un sistema di cavi che, insieme, possono regolare l’adattamento tramite un’equazione algoritmica di pressione. Succedono cose strane quando il software non funziona correttamente.11

			Potrebbero esserci problemi, e quindi compiti relativi che li risolvono, che non si prestano facilmente a essere avvolti. Eppure qui non si tratta di prove, ma piuttosto di ingegno, costi economici, esperienze e preferenze dell’utente o del cliente. Per esempio, un robot che stira le camicie può essere progettato. Nel 2012, un team dell’Università Carlos iii di Madrid, in Spagna, ha costruito teo, un robot che pesa circa 80 chili ed è alto un metro e ottanta. teo può salire le scale, aprire le porte e, più recentemente, ha mostrato di essere in grado di stirare le camicie (Estevez, Victores, Fernandez-Fernandez et al., 2017), anche se occorre appoggiare gli indumenti sull’asse da stiro e poi ritirarli. L’opinione, abbastanza diffusa, è che:

			“teo è costruito per fare ciò che fanno gli esseri umani come lo fanno gli esseri umani”, afferma Juan Victores, membro del team dell’Università Carlos iii di Madrid. Lui e i suoi colleghi vogliono che teo sia in grado di eseguire altri compiti domestici, come dare una mano in cucina. Il loro obiettivo finale è di far sì che teo sia in grado di imparare a svolgere un compito semplicemente osservando persone senza specifiche competenze che l’eseguono. “Avremo robot come teo nelle nostre case. È soltanto questione di chi lo farà per primo”, afferma Victores. (Ibidem)

			Come è intuibile, penso che questo sia esattamente l’opposto di ciò che è destinato ad accadere. Dubito fortemente che questo sia il futuro. Si tratta di una visione che non riesce a cogliere la distinzione tra compiti difficili e complessi, e l’enorme vantaggio di avvolgere i compiti per renderli facili (con una difficoltà molto bassa), per quanto altamente complessi. Non dimentichiamoci del fatto che non stiamo costruendo veicoli autonomi mettendo i robot al posto di guida, ma al contrario stiamo ripensando l’intero ecosistema di veicoli e ambienti, rimuovendo del tutto il posto di guida. Perciò, se la mia analisi è corretta, il futuro dell’ia non è popolato da androidi simili a teo che imitano il comportamento umano, ma è rappresentato più probabilmente da Effie,12 Foldimate13 e altre simili macchine automatiche domestiche che asciugano e stirano i vestiti. Non sono androidi, come teo, ma sistemi simili a scatole che possono essere piuttosto sofisticati dal punto di vista computazionale. Assomigliano più a lavastoviglie e lavatrici, con la differenza che, nei loro ambienti avvolti, il loro input è costituito da vestiti stropicciati e il loro output da vestiti stirati. Forse macchine simili saranno costose, forse non funzioneranno sempre nel modo desiderato, forse potrebbero essere implementate in modi che non riusciamo a immaginare ora, ma è percepibile che si tratti della logica corretta. Stiamo trasformando che cosa sia e come appaia un tagliaerba, non stiamo costruendo androidi che spingono il mio vecchio tagliaerba in giro come farei io. La lezione? Non dobbiamo cercare di imitare gli esseri umani attraverso l’ia. Dobbiamo sfruttare, invece, ciò che le macchine, inclusa l’ia, fanno meglio. La difficoltà è nemica delle macchine, la complessità il loro alleato: per questo, occorre avvolgere il mondo che le circonda, disegnare nuove forme di implementazione per incorporarle con successo nel loro involucro. A quel punto diventerà ragionevole ottenere una serie di perfezionamenti progressivi, dimensioni di mercato, marketing e prezzi adeguati, seguiti da nuovi miglioramenti.





3.4 Il design come futuro dell’ia


			I due futuri che ho delineato in questa sede sono complementari e basati sulla nostra attuale e prevedibile comprensione dell’ia. Ci sono incognite sconosciute, ovviamente, ma tutto quello che si può dire al loro riguardo è esattamente questo: esistono, ma non ne sappiamo alcunché. È un po’ come dire che sappiamo che vi sono domande che non ci stiamo ponendo, ma non siamo in grado di dire quali siano. Il futuro dell’ia è pieno di incognite sconosciute. Ciò che ho cercato di fare in questo capitolo è “scrutare nei semi del tempo” che abbiamo già seminato. Mi sono concentrato sulla natura dei dati e dei problemi perché i primi consentono all’ia di funzionare e i secondi delineano i confini entro i quali l’ia può operare con successo. A questo livello di astrazione, due inferenze appaiono molto plausibili. Cercheremo di sviluppare l’ia utilizzando dati il più possibile ibridi e preferibilmente sintetici, attraverso un processo di ludicizzazione di interazioni e compiti. In altri termini, tenteremo di allontanarci dai dati puramente storici, per quanto possibile. In settori come la sanità e l’economia, è ben possibile che i dati storici o al massimo ibridi restino necessari, a causa della differenza tra regole vincolanti e costitutive. Faremo tutto questo traducendo il più possibile problemi difficili in problemi complessi, attraverso l’avvolgimento della realtà attorno alle capacità dei nostri artefatti. In sintesi, cercheremo di creare dati ibridi o sintetici per affrontare problemi complessi, ludicizzando compiti e interazioni in ambienti avvolti. Quanto più questo è possibile, tanto più l’ia avrà successo. Questo è il motivo per cui una tendenza come lo sviluppo delle città gemelle digitali è molto interessante. Il che mi porta ad altri due commenti.

			Ludicizzare e avvolgere sono questione di disegnare, e talvolta ridisegnare, le realtà con cui ci confrontiamo (Floridi, 2019c). Pertanto, il futuro prevedibile dell’ia dipenderà dalle nostre capacità di design e ingegno. Dipenderà anche dalla nostra capacità di negoziare le derivanti (e serie) questioni etiche, giuridiche e sociali, dalle nuove forme di privacy (tramite proxy [cioè dati vicarianti], predittiva o di gruppo) alle forme di nudging (spinta gentile) e autodeterminazione. Abbiamo visto nel secondo capitolo che l’idea stessa di plasmare sempre più i nostri ambienti (analogici o digitali) per renderli adatti all’ia dovrebbe far riflettere chiunque. Prevedere tali questioni, per facilitarne la soluzione ed evitarne o mitigarne gli aspetti negativi, è il vero valore di qualsiasi analisi rivolta al futuro. È interessante cercare di capire quali possano essere i percorsi più probabili nell’evoluzione dell’ia. Tuttavia, sarebbe piuttosto sterile tentare di prevedere “quale grano crescerà e quale no” e poi non fare nulla per garantire che i chicchi buoni crescano e quelli cattivi no (Floridi, 2014d). Il futuro non è del tutto aperto perché il passato lo plasma, ma non è neppure del tutto determinato, perché il passato può essere indirizzato in una direzione diversa. Ecco perché la sfida che ci attende non sarà tanto quella dell’innovazione digitale di per sé, quanto piuttosto quella della governance del digitale, compresa l’ia.





3.5 Conclusione: l’ia e le sue stagioni


			Il problema con le metafore stagionali è che sono cicliche. Se diciamo che l’ia ha trascorso un brutto inverno, dobbiamo anche ricordarci che l’inverno farà ritorno, ed è meglio farsi trovare pronti. L’inverno dell’ia è quella fase in cui la tecnologia, gli affari e i media escono dalla loro calda e confortevole bolla, si raffreddano, temperano le loro speculazioni fantascientifiche e le loro esagerazioni irragionevoli, e fanno i conti con ciò che l’ia può o non può davvero fare come tecnologia (Floridi, 2019d), in modo misurato. Gli investimenti diventano più attenti e i giornalisti smettono di scrivere di ia, per inseguire altri temi in voga e alimentare la moda seguente.

			L’ia ha conosciuto diversi inverni.14 Tra i più rilevanti, ce n’è stato uno alla fine degli anni Settanta e un altro a cavallo degli anni Ottanta e Novanta. Oggi parliamo di un altro prevedibile inverno (Nield, 2019; Walch, 2019; Schuchmann, 2019).15 L’ia è soggetta a questi cicli di esagerazioni perché è una speranza o una paura che abbiamo nutrito da quando siamo stati cacciati dal paradiso: qualcosa che fa tutto per noi, al nostro posto, meglio di noi, con tutti i vantaggi sognati (saremo in vacanza per sempre) e i rischi paventati (saremo ridotti in schiavitù) che ne derivano. Per alcune persone, speculare su tutto questo è irresistibile. È il selvaggio West delle “ipotesi” e degli scenari del “come se”. Ma spero che il lettore mi perdonerà, se mi permetterò di dire: “Te l’avevo detto”. Da tempo ho messo in guardia contro commentatori ed “esperti” che facevano a gara per vedere chi la raccontava più grossa (Floridi, 2016d). Ne è seguita una pletora di miti. Parlavano dell’ia come se fosse l’ultima panacea, che avrebbe risolto e superato tutto; o come la catastrofe finale, una superintelligenza che avrebbe distrutto milioni di posti di lavoro, sostituendo avvocati e medici, giornalisti e ricercatori, camionisti e tassisti, e finendo per dominare gli esseri umani come se fossero animali domestici.

			Molti hanno seguito Elon Musk nel dichiarare che lo sviluppo dell’ia rappresentava il più grande rischio esistenziale corso dall’umanità. Come se la maggior parte dell’umanità non vivesse nella miseria e nella sofferenza. Come se guerre, carestie, inquinamento, riscaldamento globale, ingiustizia sociale e fondamentalismo fossero fantascienza o solo seccature trascurabili, che non meritano considerazione. Oggi, la pandemia da Covid-19 ha posto fine a queste sciocche affermazioni. Alcuni hanno insistito sul fatto che leggi e regolamenti giungerebbero sempre troppo tardi senza tenere mai il passo dell’ia, quando in realtà le norme non riguardano il ritmo ma la direzione dell’innovazione, poiché dovrebbero guidare il corretto sviluppo di una società. Se ci piace dove stiamo andando, possiamo andarci alla velocità che vogliamo. È a causa della nostra mancanza di visione che abbiamo paura. Oggi sappiamo che una normativa è in corso di elaborazione, almeno nell’Unione Europea. Altri (non necessariamente diversi dai precedenti) hanno affermato che l’ia fosse una scatola nera magica che non potremmo mai spiegare, quando in realtà si tratta di individuare il corretto livello di astrazione al quale interpretare le complesse interazioni ingegnerizzate: anche il traffico automobilistico nel centro di una città diventa una scatola nera se pretendiamo di scoprire perché ogni singolo individuo si trovi lì in quel momento. Oggigiorno, c’è un crescente sviluppo di strumenti adeguati per monitorare e capire come i sistemi di apprendimento automatico (ml) raggiungono i loro risultati (Watson, Krutzinna, Bruce et al., 2019; Watson, Floridi, 2020; Watson, Gultchin, Taly et al., 2021). Inoltre, tali persone diffondono scetticismo sulla possibilità di delineare un quadro etico che sintetizzi ciò che intendiamo per ia socialmente buona, laddove in realtà la ue, l’ocse e la Cina convergono su principi molto simili, che offrono una piattaforma comune per ulteriori accordi, come vedremo nel quarto capitolo. Si tratta di irresponsabili in cerca di titoli. Dovrebbero vergognarsi e chiedere scusa. Non solo per i loro commenti insostenibili, ma anche per la grande trascuratezza e l’allarmismo, che hanno tratto in inganno l’opinione pubblica sia su una tecnologia potenzialmente utile – che può fornire e difatti fornisce soluzioni utili, dalla medicina ai sistemi di sicurezza e monitoraggio (Taddeo, Floridi, 2018a) – sia sui rischi reali, che sappiamo essere concreti ma molto meno fantasiosi, dalla manipolazione quotidiana delle scelte (Milano, Taddeo, Floridi, 2019, 2020) all’aumento della pressione sulla privacy individuale e di gruppo (Floridi, 2014c), dai conflitti informatici all’uso dell’ia da parte della criminalità organizzata per riciclaggio di denaro e furto di identità, come vedremo nei capitoli settimo e ottavo.

			Il rischio insito in ogni estate dell’ia è che le aspettative esagerate si trasformino in una distrazione di massa. Il rischio insito in ogni inverno dell’ia è che il contraccolpo sia eccessivo, la delusione troppo profonda, cosicché soluzioni potenzialmente preziose vengono buttate via con l’acqua delle illusioni. Gestire il mondo è un compito sempre più complesso: le megalopoli e la loro “trasformazione in città smart” ne sono un buon esempio. Inoltre, siamo posti a confronto con problemi planetari – pandemie, cambiamenti climatici, ingiustizie sociali, migrazioni – che richiedono livelli di coordinamento sempre più elevati per essere risolti. Abbiamo bisogno naturalmente di tutta la buona tecnologia che possiamo disegnare, sviluppare e implementare per affrontare queste sfide, e di tutta l’intelligenza umana che possiamo esercitare per mettere tale tecnologia al servizio di un futuro migliore. L’ia può svolgere un ruolo importante in tutto questo perché abbiamo bisogno di modalità sempre più intelligenti per elaborare immense quantità di dati, in maniera efficiente, efficace, sostenibile ed equa. Ma l’ia deve essere trattata come una normale tecnologia, non come un miracolo né come una piaga, bensì come una delle tante soluzioni che l’ingegno umano è riuscito a escogitare. Questa è anche la ragione per cui il dibattito etico resta sempre una questione interamente umana.

			Ora che il nuovo inverno potrebbe arrivare, possiamo provare a imparare alcune lezioni ed evitare di allignare in questo andirivieni di illusioni irragionevoli e disillusioni esagerate. Non dimentichiamo che l’inverno dell’ia non dovrebbe essere l’inverno delle sue opportunità. Certamente non sarà l’inverno dei suoi rischi o delle sue sfide. Dobbiamo chiederci se le soluzioni di ia rimpiazzeranno davvero le soluzioni precedenti, come ha fatto l’automobile con la carrozza, le diversificheranno, come ha fatto la moto con la bicicletta, o le integreranno ed espanderanno, come ha fatto lo smartwatch digitale con l’orologio analogico. Quale sarà il livello di sostenibilità, accettabilità sociale o preferibilità di ogni ia che emergerà in futuro, forse dopo un nuovo inverno? Indosseremo davvero degli strani occhiali per vivere in un mondo virtuale o aumentato creato e abitato da sistemi di ia? Oggi molte persone sono restie a indossare occhiali anche quando ne hanno davvero bisogno, solo per motivi estetici. E poi ci sono soluzioni ia fattibili nella vita di tutti i giorni? Sono disponibili le competenze, gli insiemi di dati, l’infrastruttura e i modelli di business necessari per garantire il successo dell’implementazione dell’ia? I futurologi trovano queste domande noiose. A loro piace un’idea unica, semplice, che interpreta e cambia tutto, che può essere dispiegata con leggerezza in un libro facile che fa sentire il lettore intelligente, un libro che deve essere letto da tutti oggi e ignorato da tutti domani. È la cattiva alimentazione del cibo spazzatura per i pensieri e la maledizione del bestseller da aeroporto. Dobbiamo resistere all’eccesso di semplificazione. Questa volta, dobbiamo pensare in modo più approfondito ed esteso a ciò che stiamo facendo e pianificando con l’ia. Questo esercizio si chiama filosofia, non futurologia. Ed è ciò a cui spero di contribuire nella seconda parte di questo libro.



* * *





			 				 					1. Per una rassicurante e convergente rassegna basata non sulla natura dei dati o sulla natura dei problemi, ma sulla natura delle soluzioni tecnologiche, fondata su un ampio esame della letteratura sull’intelligenza artificiale, vedi “We analyzed 16,625 papers to figure out where ai is headed next”, https://www.technologyreview.com/s/612768/we-analyzed-16625-papers-to-figure-out-where-ai-is-headed-next/.



				 					2. https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/.



				 					3. https://www.tcs.com/blogs/the-masking-vs-synthetic-data-debate.



				 					4. https://deepmind.com/blog/alphazero-shedding-new-light-grand-games-chess-shogi-and-go/.



				 					5. https://securityintelligence.com/generative-adversarial-networks-and-cybersecurity-part-1/.



				 					6. https://motherboard.vice.com/en_us/article/7xn4wy/this-website-uses-ai-to-generate-the-faces-of-people-who-dont-exist.



				 					7. La letteratura filosofica traccia distinzioni diverse, soprattutto tra regole regolative e costitutive, che sembrano compatibili ma non sono le stesse, vedi Rawls (1955), Searle (2018) e Hage (2018). Per Searle, sia gli scacchi sia il calcio hanno quelle che lui chiama “regole costitutive”. Per evitare confusione, preferisco usare il termine “vincolanti” invece che regolative.



				 					8. https://qz.com/966882/robots-cant-lace-shoes-so-sneaker-production-cant-be-fully-automated-just-yet/.



				 					9. http://www.moley.com/.



				 					10. https://misorobotics.com/.



				 					11. https://www.bbc.co.uk/news/business-47336684.



				 					12. https://helloeffie.com/.



				 					13. https://foldimate.com/.



				 					14. https://en.wikipedia.org/wiki/AI_winter.



				 					15. Anche la bbc, che ha contribuito al clamore (vedi per esempio: https://www.bbc.co.uk/programmes/p031wmt7), ora riconosce che potrebbe essere stato… un clamore: https://www.bbc.co.uk/news/technology-51064369.





Seconda parte


			valutare l’intelligenza artificiale





4


			Un quadro unificato di principi etici per l’ia

			Sommario In precedenza, nella prima parte, abbiamo visto che l’ia è una nuova forma dell’agire, che può affrontare con successo compiti e problemi, in vista di un obiettivo, senza alcun bisogno di essere intelligente. Ogni successo di qualsiasi applicazione di ia non sposta l’asticella di ciò che significa essere un agente intelligente, l’aggira. Il successo di tale agire artificiale è sempre più facilitato dal processo di avvolgere (vale a dire, rimodellare in termini di contesti adattati all’ia) gli ambienti in cui opera l’ia. La disgiunzione tra agire e intelligenza e l’avvolgimento del mondo generano importanti sfide etiche, in particolare riguardo ad autonomia, pregiudizi (bias), spiegabilità (definita sotto più precisamente come esplicabilità), equità, privacy, responsabilità, trasparenza e fiducia. Per questo, almeno dal 2017, quando sono stati pubblicati i Principi di Asilomar per l’ia e la Dichiarazione di Montréal per uno sviluppo responsabile dell’intelligenza artificiale, molte organizzazioni hanno lanciato un’ampia gamma di iniziative per stabilire principi etici per l’adozione di un’ia socialmente vantaggiosa. È diventata presto un’attività artigianale. Purtroppo, l’enorme volume di principi proposti rischia di risultare soverchiante e fuorviante. In questo capitolo, presento i risultati di un’analisi comparativa di alcuni degli insiemi di principi etici di più alto profilo per l’ia e valuto se tali principi convergano su un nucleo di principi condivisi, o divergano, con un significativo disaccordo su ciò che costituisce un’“ia etica”. Nel capitolo, sostengo che c’è un elevato grado di sovrapposizione tra gli insiemi di principi analizzati. Esiste un quadro generale costituito da cinque principi fondamentali per l’ia etica. Quattro di questi sono principi fondamentali comunemente usati in bioetica: beneficenza, non maleficenza, autonomia e giustizia. Ciò non sorprende dal momento che l’ia è interpretata come forma dell’agire. Sulla base dell’analisi comparativa occorre aggiungere un nuovo principio: l’esplicabilità, intesa come principio che include sia il senso epistemologico di intelligibilità (come risposta alla domanda: “Come funziona?”) sia quello etico di responsabilità (accountability) (come risposta alla domanda: “Chi è responsabile del modo in cui funziona?”). Questo ulteriore principio è reso necessario dal fatto che l’ia è una nuova forma dell’agire. Il capitolo si conclude con una disamina delle implicazioni di questo quadro etico per gli sforzi futuri volti a creare leggi, regole, standard tecnici e le migliori pratiche (best practices) per l’ia etica in un’ampia varietà di contesti.





4.1 Introduzione: troppi principi?


			Come abbiamo osservato nei capitoli precedenti, l’ia sta già avendo un forte impatto sulla società. Le questioni chiave sono come, dove, quando e da chi sarà avvertito l’impatto dell’ia. Tornerò su questo punto nell’undicesimo capitolo. Qui, vorrei concentrarmi su una conseguenza di questa tendenza: molte organizzazioni hanno lanciato un’ampia gamma di iniziative per stabilire principi etici per l’adozione di un’ia socialmente vantaggiosa. Purtroppo, l’enorme volume di principi proposti – più di 160 nel 2020, secondo l’ai Ethics Guidelines Global Inventory (la rassegna globale delle linee guida etiche per l’ia) di Algorithm Watch (2019)1 – rischia di diventare soverchiante e fuorviante, sollevando due potenziali problemi. O i vari insiemi di principi etici per l’ia sono simili, portando a inutili ripetizioni e ridondanze, oppure, se differiscono in modo significativo, sono suscettibili di ingenerare confusione e ambiguità. Il peggior risultato sarebbe quello di creare un “mercato dei principi” in cui le parti interessate potrebbero essere tentate di “acquistare” quelli più allettanti, come vedremo nel quinto capitolo.

			Come si potrebbe risolvere il problema della “proliferazione dei principi”? In questo capitolo presento e discuto i risultati di un’analisi comparativa di alcuni degli insiemi di principi etici di più alto profilo per l’ia. Valuto se questi principi sono convergenti su un nucleo di principi condivisi, o divergenti, con un significativo disaccordo su ciò che costituisce un’“ia etica”. L’analisi rivela un elevato numero di punti in comune tra gli insiemi di principi esaminati. Ciò porta a identificare un quadro generale costituito da cinque principi fondamentali per l’ia etica. Nella discussione che segue, prendo atto dei limiti e valuto le implicazioni di questo quadro etico per gli sforzi futuri volti a creare leggi, regole, standard e le migliori pratiche (best practices) per l’ia etica in un’ampia varietà di contesti.





4.2 Un quadro unificato di cinque principi per l’ia etica


			Abbiamo visto nel primo capitolo che l’affermazione dell’ia come campo di ricerca accademica risale agli anni Cinquanta (McCarthy, Minsky, Rochester, Shannon, 2006 [1955]). Il dibattito etico è quasi altrettanto vecchio (Wiener, 1960; Samuel, 1960). Tuttavia, è solo negli ultimi anni che i notevoli progressi nelle capacità e nelle applicazioni dei sistemi di ia hanno portato maggiormente all’attenzione le opportunità e i rischi dell’ia per la società (Yang, Bellingham, Dupont et al., 2018). L’esigenza crescente di riflessione e politiche chiare sull’impatto dell’ia sulla società ha prodotto un eccesso di iniziative. Ogni nuova iniziativa produce un’ulteriore dichiarazione di principi, valori o ideali per guidare lo sviluppo e l’adozione dell’ia. Si ha l’impressione che a un certo punto si sia prodotta una corsa a partecipare con un’escalation del tipo “anch’io”. Nessuna organizzazione poteva risultare priva di principi etici per l’ia, e accettare quelli già disponibili sembrava inelegante, poco originale e privo di leadership. L’“anch’io” è stato seguito dal “mio e solo mio”. A distanza di anni, il rischio è ancora quello di inutili ripetizioni e sovrapposizioni, se i vari insiemi di principi sono simili, o di confusione e ambiguità, se differiscono. In entrambi i casi, l’elaborazione di leggi, regole, standard e migliori pratiche per garantire che l’ia sia socialmente vantaggiosa può essere ritardata dalla necessità di navigare attraverso la panoplia di principi e dichiarazioni stabiliti da una serie di iniziative in continua espansione. È giunto il momento di procedere a un’analisi comparativa di questi documenti, che consenta anche di valutare se siano convergenti o divergenti e, nel primo caso, se sia quindi possibile produrre un quadro unificato di sintesi.

			La Tabella 4.1 mostra le sei iniziative di alto profilo stabilite nell’interesse dell’ia socialmente vantaggiosa che sono state selezionate come quelle che forniscono maggiori informazioni per un’analisi comparativa:

			Tabella 4.1 Sei delle serie di principi etici più influenti per l’ia.

			 				 					 					 				 				 					 						 							1

						 						 							I Principi di Asilomar per l’ia (Future of Life Institute, 2017), sviluppati sotto gli auspici del Future of Life Institute, in collaborazione con i partecipanti alla conferenza Asilomar di alto livello di gennaio 2017 (di seguito “Asilomar”);



					 						 							2

						 						 							La Dichiarazione di Montréal per l’ia responsabile (Università di Montréal, 2017), sviluppata sotto gli auspici dell’Università di Montréal, a seguito del Forum sullo sviluppo socialmente responsabile dell’ia del novembre 2017 (di seguito “Montréal”);*2



					 						 							3

						 						 							I principi generali offerti nella seconda versione di Ethically Aligned Design: A Vision for Prioritizing Human Wellbeing with Autonomous and Intelligent Systems (ieee, 2017). Questo documento crowd-sourced ha ricevuto contributi da 250 leader di pensiero globali per sviluppare principi e raccomandazioni per lo sviluppo etico e la progettazione di sistemi autonomi e intelligenti, ed è stato pubblicato nel dicembre 2017 (di seguito “ieee”; p. 6);



					 						 							4

						 						 							I principi etici offerti nella Dichiarazione su intelligenza artificiale, robotica e sistemi autonomi (ege, 2018), pubblicata dal Gruppo europeo sull’etica della scienza e delle nuove tecnologie della Commissione europea nel marzo 2018 (di seguito “ege”);



					 						 							5

						 						 							I “cinque principi generali per un codice di intelligenza artificiale” offerti nel rapporto del Comitato per l’intelligenza artificiale della Camera dei Lord del Regno Unito, “ai in the uk: ready, willing and able?”, pubblicato nell’aprile 2018 (di seguito “aiuk”; House of Lords – Artificial Intelligence Committee, 2017, §417);



					 						 							6

						 						 							I Principi di partenariato sull’ia (Partnership on ai, 2018), un’organizzazione multi-stakeholder composta da docenti universitari, ricercatori, organizzazioni della società civile, imprese di costruzione e utilizzazione di tecnologie di ia e da altri gruppi (di seguito “il Partenariato”).



				 			 			Ciascun insieme di principi nella Tabella 4.1 soddisfa quattro criteri di base, per cui è:

			a) 	recente, pubblicato a partire dal 2017;

			b) 	direttamente rilevante per l’ia e il suo impatto sulla società nel suo insieme (escludendo quindi documenti specifici per un particolare ambito, industria o settore);

			c) 	di elevata reputazione, pubblicato da autorevoli organizzazioni multi-stakeholder di portata almeno nazionale;3

			d)	influente (a causa di a-c).

			Considerati unitariamente, producono 47 principi.4 Nonostante ciò, le differenze sono principalmente linguistiche e c’è un grado di coerenza e sovrapposizione tra i sei insiemi di principi che è impressionante e rassicurante. Questa convergenza può essere mostrata più chiaramente confrontando gli insiemi di principi con i quattro principi fondamentali comunemente usati in bioetica: beneficenza, non maleficenza, autonomia e giustizia (Beauchamp, Childress, 2013). Il confronto non dovrebbe sorprendere. Come il lettore ricorderà, in questo libro sostengo l’idea che l’ia non è una nuova forma di intelligenza, ma una forma di agire senza precedenti. Per questo, di tutte le aree dell’etica applicata, la bioetica è quella che più assomiglia all’etica digitale nel trattare ecologicamente nuove forme di agenti, pazienti e ambienti (Floridi, 2013). Tuttavia, mentre i quattro principi bioetici si adattano sorprendentemente bene alle nuove sfide etiche poste dall’ia, non offrono una traduzione perfetta. Come vedremo, il significato di fondo di ciascuno dei principi è contestato, in quanto termini simili sono spesso usati per significare cose diverse. Inoltre, i quattro principi non sono esaustivi. Sulla base dell’analisi comparativa, emerge distintamente l’esigenza di aggiungere un nuovo principio: l’esplicabilità, intesa come principio che include sia il senso epistemologico di intelligibilità (come risposta alla domanda: “Come funziona?”) sia quello etico di responsabilità (accountability) (come risposta alla domanda: “Chi è responsabile del modo in cui funziona?”); e ciò vale sia per gli esperti, come per esempio progettisti o ingegneri di prodotto, sia per i non esperti, come per esempio pazienti o clienti.5 Il nuovo principio è reso necessario dal fatto che l’ia non è simile ad alcuna forma biologica di agire. Tuttavia, anche la convergenza rilevata tra questi diversi insiemi di principi richiede cautela. Spiegherò le ragioni di questa cautela più avanti nel capitolo, ma prima vorrei introdurre i cinque principi.





4.3 Beneficenza: promuovere il benessere, preservare la dignità e sostenere il pianeta


			Il principio per cui le tecnologie di ia sono create a beneficio dell’umanità è espresso in modi diversi nei sei documenti, ma è forse il più facile da osservare dei quattro principi della bioetica tradizionale. I principi di Montréal e ieee utilizzano entrambi il termine “benessere”; per Montréal, “lo sviluppo dell’intelligenza artificiale dovrebbe in definitiva promuovere il benessere di tutte le creature senzienti”, mentre ieee afferma la necessità di “dare priorità al benessere umano come risultato in ogni design di sistema”. aiuk e Asilomar caratterizzano entrambi questo principio come “bene comune”: l’ia dovrebbe “essere sviluppata per il bene comune e il beneficio dell’umanità”, secondo aiuk. Il Partenariato esprime l’intento di “assicurare che le tecnologie dell’ia beneficino e dotino di maggiori poteri quante più persone possibili”, mentre ege sottolinea il principio sia della “dignità umana” sia della “sostenibilità”. Il suo principio di “sostenibilità” articola forse la più ampia di tutte le interpretazioni di beneficenza, sostenendo che “la tecnologia dell’ia deve essere in linea con […] l’assicurare le precondizioni di base per la vita sul nostro pianeta, la continua prosperità per l’umanità e la conservazione di un buon ambiente per le generazioni future”. Nel suo insieme, la rilevanza della beneficenza sottolinea fermamente l’importanza centrale di promuovere il benessere delle persone e del pianeta con l’ia. Questi sono aspetti sui quali tornerò nei capitoli 9, 11, 12 e 13.





4.4 Non maleficenza: privacy, sicurezza e “cautela della capacità”


			Benché “Fa’ soltanto del bene” (beneficenza) e “Non fare del male” (non maleficenza) possano sembrare logicamente equivalenti, non lo sono e rappresentano principi distinti. I sei documenti incoraggiano tutti la creazione di un’ia benefica e ciascuno mette anche in guardia contro le varie conseguenze negative derivanti dall’uso eccessivo o improprio delle tecnologie di ia (vedi capitoli 5, 7 e 8). Di particolare interesse è la prevenzione delle violazioni della privacy personale, inclusa come principio in cinque delle sei serie. Molti dei documenti sottolineano come evitare l’uso improprio delle tecnologie di ia in altri modi. I Principi di Asilomar mettono in guardia contro le minacce di una corsa agli armamenti di ia e dell’automiglioramento ricorsivo dell’ia, mentre il Partenariato afferma analogamente l’importanza che l’ia operi “all’interno di limiti sicuri”. Il documento ieee cita la necessità di “evitare usi impropri” e la Dichiarazione di Montréal sostiene che coloro che sviluppano ia “dovrebbero assumersi le proprie responsabilità operando contro i rischi derivanti dalle loro innovazioni tecnologiche”. Tuttavia, da questi vari avvertimenti non è del tutto chiaro se siano le persone che sviluppano ia, o la tecnologia stessa, che dovrebbero essere incoraggiate a non fare del male; in altre parole, se dovremmo guardarci dalla maleficenza del dottor Frankenstein (come suggerisco) o del suo mostro (vedi capitolo 10). Al centro di questo dilemma c’è la questione dell’autonomia.





4.5 Autonomia: il potere di “decidere di decidere”


			Quando adottiamo l’ia e il suo agire smart, cediamo volontariamente parte del nostro potere decisionale ad artefatti tecnologici. Per questo, affermare il principio di autonomia nel contesto dell’ia significa trovare un equilibrio tra il potere decisionale che ci riserviamo e quello che deleghiamo agli agenti artificiali. Il rischio è che la crescita dell’autonomia artificiale possa minare il fiorire dell’autonomia umana. Non sorprende quindi che il principio di autonomia sia esplicitamente previsto in quattro dei sei documenti. La Dichiarazione di Montréal esprime la necessità di un equilibrio tra il processo decisionale guidato dagli esseri umani e quello guidato dalle macchine, affermando che “lo sviluppo dell’ia dovrebbe promuovere l’autonomia [corsivo mio] di tutti gli esseri umani”. ege sostiene che i sistemi autonomi “non devono compromettere [la] libertà degli esseri umani di stabilire i propri standard e norme”, mentre aiuk adotta la posizione più restrittiva secondo la quale “il potere autonomo di danneggiare, distruggere o ingannare gli esseri umani non dovrebbe mai essere conferito all’ia”. Allo stesso modo, il documento di Asilomar sostiene il principio di autonomia, nella misura in cui “gli esseri umani dovrebbero scegliere come e se delegare le decisioni ai sistemi di ia, per raggiungere gli obiettivi scelti da esseri umani”. È chiaro dunque sia che l’autonomia umana debba essere promossa, sia che l’autonomia delle macchine debba essere limitata e resa intrinsecamente reversibile, qualora l’autonomia umana debba essere protetta o ristabilita (si pensi al caso di un pilota in grado di disattivare il pilota automatico e riprendere il pieno controllo dell’aereo). Ciò introduce una nozione che può essere definita come meta-autonomia, o modello di decisione di delega. Gli esseri umani dovrebbero mantenere il potere di decidere quali decisioni prendere, esercitando la libertà di scelta dove necessario e cedendola nei casi in cui ragioni di primaria importanza, come l’efficacia, possano prevalere sulla perdita di controllo sul processo decisionale. Ma qualsiasi delega dovrebbe anche rimanere in linea di principio rivedibile, adottando come ultima garanzia il potere di decidere di decidere di nuovo.





4.6 Giustizia: promuovere la prosperità, preservare la solidarietà, evitare l’iniquità


			La decisione di prendere o delegare decisioni non avviene nel vuoto. Né questa capacità è distribuita equamente nella società. Gli effetti di tale disparità di autonomia sono affrontati con il principio di giustizia. L’importanza della giustizia è citata esplicitamente nella Dichiarazione di Montréal, che sostiene che “lo sviluppo dell’ia dovrebbe promuovere la giustizia e cercare di eliminare tutti i tipi di discriminazione”, mentre i Principi di Asilomar includono la necessità sia di “benefici condivisi” sia di “prosperità condivisa” per mezzo dell’ia. In base al suo principio denominato “giustizia, equità e solidarietà”, ege sostiene che l’ia dovrebbe “contribuire alla giustizia globale e alla parità nell’accesso ai benefici” delle tecnologie di ia. Mette inoltre in guardia contro il rischio di distorsioni (bias) negli insiemi di dati utilizzati per addestrare i sistemi di ia e, unico fra tutti i documenti, sostiene l’esigenza di difendersi dalle minacce alla “solidarietà”, inclusi i “sistemi di assistenza reciproca come nell’assicurazione sociale e nell’assistenza sanitaria”. Altrove “giustizia” ha ancora altri significati (soprattutto nel senso di equità), variamente collegati all’uso dell’ia per correggere errori del passato come eliminare discriminazioni ingiuste, promuovere la diversità e prevenire l’insorgenza di nuove minacce alla giustizia. I diversi modi in cui è caratterizzata la giustizia rinviano a una più ampia assenza di chiarezza sull’ia come riserva di agire intelligente creata dagli esseri umani. In parole povere, siamo noi (umani, vedi sotto) il paziente che riceve le “cure” dell’ia, il medico che le prescrive, o entrambi? Tale questione può essere risolta solo con l’introduzione di un quinto principio che emerge dall’analisi precedente.





4.7 Esplicabilità: rendere possibili gli altri principi tramite l’intelligibilità e la responsabilità


			La risposta breve alla domanda precedente (se siamo il paziente o il medico) è che in realtà potremmo essere entrambi, a seconda delle circostanze e di chi sia il “noi” a cui ci riferiamo nella vita di tutti i giorni. La situazione è intrinsecamente diseguale: una piccola frazione dell’umanità è attualmente impegnata a sviluppare una serie di tecnologie che stanno già trasformando la vita quotidiana di quasi tutti gli altri. Questa cruda realtà non è sfuggita agli autori dei documenti in esame. Tutti fanno riferimento alla necessità di comprendere e di rendere conto dei processi decisionali dell’ia. Diversi termini esprimono questo principio: “trasparenza” in Asilomar e ege; sia “trasparenza” sia “responsabilità” in ieee; “intelligibilità” in aiuk; e “comprensibile e interpretabile” per il Partenariato. Ciascuno di questi principi coglie qualcosa di apparentemente nuovo riguardo all’ia come forma dell’agire: il suo funzionamento è spesso invisibile o incomprensibile a tutti tranne (nella migliore delle ipotesi) agli osservatori più esperti.

			L’aggiunta del principio di “esplicabilità”, che include sia il senso epistemologico di “intelligibilità” sia il senso etico di “responsabilità”, è il cruciale pezzo mancante del puzzle etico dell’ia. Questo principio completa gli altri quattro: affinché l’ia sia benefica e non malefica, dobbiamo essere in grado di comprendere il bene o il danno che sta effettivamente facendo alla società e in quali modi; affinché l’ia promuova e non limiti l’autonomia umana, la nostra “decisione su chi dovrebbe decidere” deve essere informata dalla conoscenza di come l’ia agirebbe al nostro posto e, in tal caso, di come migliorare le sue prestazioni; e, affinché l’ia sia giusta, dobbiamo sapere chi ritenere eticamente o legalmente responsabile in caso di un esito grave e negativo, il che richiederebbe a sua volta un’adeguata comprensione del perché tale esito si sia prodotto.





4.8 Una visione sinottica


			Considerati unitariamente, i cinque principi precedenti colgono ciascuno dei 47 principi contenuti nei sei documenti di alto profilo formati dagli esperti di cui alla Tabella 4.1. Come test, vale la pena di sottolineare che ogni principio è incluso in quasi tutte le dichiarazioni di principi analizzate nella Tabella 4.2 (vedi sotto). I cinque principi costituiscono, dunque, un quadro etico entro il quale possono essere formulate politiche, migliori pratiche e altre raccomandazioni. Questo quadro di principi è illustrato nella Figura 4.1.





4.9 L’etica dell’ia: da dove e per chi?


			È importante osservare che ciascuno dei sei insiemi di principi etici per l’ia di cui alla Tabella 4.1 è emerso da iniziative di portata globale o all’interno delle democrazie liberali occidentali. Per avere una più ampia applicazione, tale quadro trarrebbe senza dubbio beneficio dalle prospettive di regioni e culture attualmente non rappresentate o sottorappresentate nel nostro campione. Di particolare interesse in questo senso è il ruolo della Cina, che è già sede della start-up di ia di maggiore valore al mondo (Jezard, 2018), gode di diversi vantaggi strutturali nello sviluppo dell’ia (Lee, Triolo, 2017) e il cui governo ha dichiarato la sua ambizione di essere alla guida mondiale nelle tecnologie all’avanguardia di ia entro il 2030 (China State Concil, 2017). Non è questo il contesto in cui analizzare le politiche di ia della Cina (sul punto vedi Roberts, Cowls, Morley et al., 2021; Roberts, Cowls, Hine et al., 2021), per cui permettetemi di aggiungere solo alcune osservazioni conclusive.

			Nella comunicazione del Consiglio di Stato sull’ia e in altri documenti, il governo cinese ha espresso interesse per un’ulteriore considerazione dell’impatto sociale ed etico dell’ia (Ding, 2018). L’entusiasmo per l’uso delle tecnologie non è prerogativa esclusiva dei governi, ma è condiviso anche dal grande pubblico, più in Cina e in India che in Europa o negli Stati Uniti, come mostra una recente ricerca rappresentativa di un sondaggio (Vodafone Institute for Society and Communications, 2018). In passato, un dirigente della principale azienda tecnologica cinese, Tencent, ha suggerito che l’Unione Europea dovrebbe concentrarsi sullo sviluppo dell’ia che comporta

			il massimo beneficio per la vita umana, anche se tale tecnologia non è competitiva per affrontare [il] mercato americano o cinese. (Boland, 2018)

			A ciò hanno fatto eco le affermazioni secondo cui l’etica potrebbe essere “l’arma vincente dell’Europa” nella “battaglia globale dell’ia” (Delcker, 2018). Non sono d’accordo. L’etica non è appannaggio di un singolo continente o di una data cultura. Ogni azienda, apparato governativo o istituzione accademica che disegna, sviluppa o implementa l’ia ha l’obbligo di farlo in linea con un quadro etico – anche se non necessariamente sulla falsariga di quello presentato in queste pagine – sufficientemente ampio da incorporare un insieme di prospettive diverse dal punto di vista geografico, culturale e sociale. Allo stesso modo, anche le leggi, le regole, gli standard e le migliori pratiche per limitare o controllare l’ia – comprese tutte quelle attualmente all’esame di enti di regolamentazione, organismi legislativi o gruppi industriali – potrebbero trarre vantaggio dalla considerazione ravvicinata di un quadro unificato di principi etici.





4.10 Conclusione: dai principi alle pratiche


			Se il quadro presentato in questo capitolo fornisce una panoramica coerente e sufficientemente completa dei principi etici centrali per l’ia (Floridi, Cowls, Beltrametti et al., 2018), allora può fungere da architettura all’interno della quale vengono sviluppate leggi, regole, standard tecnici e migliori pratiche per specifici settori, industrie e giurisdizioni. In questi contesti, il quadro può svolgere un ruolo sia abilitante (vedi Cowls, Tsamados, Taddeo et al., 2021b), sia vincolante, come nella necessità di regolamentare le tecnologie di ia nel contesto della criminalità online (vedi capitolo 7) e cyberwar (che esamino in Floridi, Taddeo, 2014; Taddeo, Floridi, 2018b). In effetti, il quadro mostrato nella Figura 4.1 ha svolto un ruolo prezioso in altri cinque documenti:

			7.	il lavoro di ai4People (Floridi, Cowls, Beltrametti et al., 2018), il primo forum globale europeo sull’impatto sociale dell’ia, che lo ha adottato per proporre alla Commissione europea venti concrete raccomandazioni per una “società della buona ia” (avviso: ho presieduto il progetto; vedi “ai4People” nella Tabella 4.2).

			Il lavoro di ai4People è stato ampiamente adottato da:

			8.	le Linee guida etiche per l’ia affidabile pubblicate dal gruppo di esperti di alto livello sull’ia della Commissione europea (hlegai, 2018, 2019; avviso: ero un membro; vedi “hleg” nella Tabella 4.2);

			che a sua volta ha influenzato:

			9.	la Raccomandazione del Consiglio sull’ia dell’ocse (ocse, 2019; vedi “ocse” nella Tabella 4.2), rivolta a 42 paesi.

			Tutto ciò è stato seguito dalla pubblicazione dei cosiddetti:

			10.	Principi dell’ia di Pechino (Beijing Academy of Artificial Intelligence, 2019; vedi “Pechino” nella Tabella 4.2);

			Tabella 4.2 I cinque principi nei sei documenti analizzati e in altri documenti.

			 				 					 					 					 					 					 					 				 				 					 						 						 							Beneficenza

						 						 							Non maleficenza

						 						 							Autonomia

						 						 							Giustizia

						 						 							Esplicabilità



					 						 							AIUK

						 						 							•

						 						 							•

						 						 							•

						 						 							•

						 						 							•



					 						 							Asilomar

						 						 							•

						 						 							•

						 						 							•

						 						 							•

						 						 							•



					 						 							EGE

						 						 							•

						 						 							•

						 						 							•

						 						 							•

						 						 							•



					 						 							IEEE

						 						 							•

						 						 							•

						 						 						 						 							•



					 						 							Montréal

						 						 							•

						 						 							•

						 						 							•

						 						 							•

						 						 							•



					 						 							Partenariato

						 						 							•

						 						 							•

						 						 						 							•

						 						 							•



					 						 							AI4People

						 						 							•

						 						 							•

						 						 							•

						 						 							•

						 						 							•



					 						 							hLEG

						 						 							•

						 						 							•

						 						 							•

						 						 							•

						 						 							•



					 						 							OCSE

						 						 							•

						 						 							•

						 						 							•

						 						 							•

						 						 							•



					 						 							Pechino

						 						 							•

						 						 							•

						 						 						 							•

						 						 							•



					 						 							Rome Call

						 						 							•

						 						 							•

						 						 							•

						 						 							•

						 						 							•



				 			 			11.	Rome Call for an ai Ethics (Pontificia Accademia per la Vita, 2020), un documento elaborato dalla Pontificia Accademia per la Vita e sottoscritto anche da Microsoft, ibm, fao e il ministero italiano per l’Innovazione (avviso: sono stato tra gli autori della bozza; vedi “Rome Call” nella Tabella 4.2).

			Lo sviluppo e l’uso dell’ia hanno un potenziale impatto sia positivo sia negativo sulla società, al fine di alleviare o amplificare le disuguaglianze esistenti, di risolvere problemi vecchi e nuovi o di causarne senza precedenti. Tracciare il percorso socialmente preferibile (equo) e sostenibile dal punto di vista ambientale dipenderà non solo da una regolamentazione ben articolata e da standard comuni, ma anche dall’utilizzo di un quadro di principi etici, all’interno del quale si possono collocare azioni concrete. Il quadro presentato in questo capitolo, in quanto emerge dal dibattito in corso, può fungere da preziosa architettura per garantire risultati sociali positivi frutto della tecnologia di ia e per procedere dai buoni principi alle buone pratiche (Floridi, Cowls, King et al., 2020; Morley, Floridi Kinsey et al., 2020). Inoltre, può fornire il quadro richiesto da un auditing dell’ia basato sull’etica. Nel corso di questa transizione, è necessario mappare alcuni rischi, per garantire che l’etica dell’ia non cada in nuove o vecchie trappole. Questo è il compito del prossimo capitolo.



* * *





			 				 					1. Vedi anche Jobin, Ienca, Vayena (2019).



				 					2. I principi qui menzionati sono quelli che sono stati pubblicamente annunciati a partire dal 1° maggio 2018 e disponibili al momento della stesura di questo capitolo qui: https://www.montrealdeclaration-responsibleai.com/the-declaration.



				 					3. Un’analoga valutazione delle linee guida etiche dell’ia è stata recentemente intrapresa da Hagendorff (2020), che adotta diversi criteri di inclusione e valutazione. Si noti che la valutazione include nel suo campione l’insieme dei principi che descriviamo qui.



				 					4. Dei sei documenti, i Principi di Asilomar offrono il maggior numero di principi con la portata probabilmente più ampia. I 23 principi sono organizzati in tre sezioni, “problemi di ricerca”, “etica e valori” e “problemi a lungo termine”. La considerazione dei cinque “problemi di ricerca” viene qui omessa in quanto sono legati specificamente agli aspetti pratici dello sviluppo dell’ia nel contesto più ristretto del mondo accademico e industriale. Allo stesso modo, gli 8 principi del Partenariato consistono sia in obiettivi intraorganizzativi sia in principi più ampi per lo sviluppo e l’uso dell’ia. Qui sono inclusi solo i principi più ampi (il primo, il sesto e il settimo principio).



				 					5. C’è una terza domanda, che riguarda quali nuovi spunti possono essere forniti dall’esplicabilità, che non è rilevante in questo contesto ma altrettanto importante; si veda Watson, Floridi (2020).





5


			Dai principi alle pratiche: i rischi di comportamenti contrari all’etica

			Sommario In precedenza, nel quarto capitolo, ho offerto un quadro unificato di principi etici per l’ia. In questo capitolo, proseguo l’analisi identificando cinque rischi principali che possono minare anche i migliori sforzi per tradurre i principi etici in effettive buone pratiche. I rischi in questione sono: (1) lo shopping etico, (2) il bluewashing etico, (3) il lobbismo etico, (4) il dumping etico e (5) l’elusione dell’etica. Nessuno di questi rischi è senza precedenti, poiché si verificano anche in altri contesti di etica applicata, come l’etica ambientale, la bioetica, l’etica medica e l’etica degli affari. Tuttavia, in questo capitolo sostengo che ciascuno di essi acquisisce caratteristiche specifiche in quanto correlato in modo precipuo all’etica dell’ia. Si tratta di rischi che sono tutti evitabili, se si sa come individuarli, e in questo capitolo suggerisco come fare. La conclusione è che l’approccio etico all’ia consiste, almeno in parte, nel creare consapevolezza della natura e dell’insorgenza di tali rischi e nel rafforzare un approccio preventivo.





5.1 Introduzione: traduzioni rischiose


			C’è voluto molto tempo (Floridi, 2013), ma abbiamo visto che oggi il dibattito sull’impatto etico e le implicazioni delle tecnologie digitali è giunto sulle prime pagine dei giornali. Ciò è comprensibile: le tecnologie digitali, dai servizi basati sul web alle soluzioni di ia, influenzano sempre di più la vita quotidiana di miliardi di persone, soprattutto dopo la pandemia, per cui si nutrono molte speranze ma anche preoccupazioni riguardo al loro design, sviluppo e implementazione (Cath, Wachter, Mittelstadt et al., 2018). Dopo più di mezzo secolo di ricerca accademica (Wiener, 1960; Samuel, 1960), come abbiamo visto nel quarto capitolo, la recente reazione pubblica è stata una fioritura di iniziative per stabilire quali principi, linee guida, codici o quadri di riferimento possono guidare eticamente l’innovazione digitale, in particolare per quanto concerne l’ia, a beneficio dell’umanità e dell’intero ambiente. Si tratta di uno sviluppo positivo, che mostra consapevolezza dell’importanza del tema e interesse ad affrontarlo in modo sistematico. Eppure, è tempo che il dibattito evolva dal cosa al come: non si tratta soltanto di individuare quale etica sia necessaria, ma anche come l’etica possa essere applicata e implementata in modo efficace e con successo per cambiare le cose in meglio. Per esempio, gli orientamenti etici dell’Unione Europea per un’ia affidabile (hlegai, 2018)1 stabiliscono un punto di riferimento per ciò che può o non può qualificarsi nella ue come ia buona dal punto di vista etico. La loro pubblicazione sarà sempre più seguita da sforzi pratici nei processi di verifica, applicazione e implementazione.

			Il passaggio da una prima fase, più teorica, relativa al cosa, a una seconda fase, più pratica, relativa al come, per così dire, è ragionevole, lodevole e fattibile (Morley, Floridi, Kinsey et al., 2020). Tuttavia, nel tradurre i principi etici in buone pratiche, anche i migliori sforzi possono essere minati da alcuni rischi di comportamenti contrari all’etica. Nel capitolo, metterò in evidenza i rischi principali che sembrano i più idonei a ostacolare anche i migliori sforzi. Vedremo che si tratta di insiemi di rischi piuttosto che di rischi singoli, e potrebbero esserci anche altri insiemi, ma questi cinque sono quelli già incontrati o prevedibili nel dibattito internazionale sull’etica digitale e l’etica dell’ia.2 Ecco l’elenco: (1) lo shopping etico, (2) il bluewashing etico, (3) le azioni di lobbismo etico, (4) il dumping etico e (5) l’elusione dell’etica. Come ha suggerito Josh Cowls, possiamo considerare i primi tre come problemi più “fuorvianti” e gli ultimi due come problemi più “distruttivi”. Consideriamo ciascuno di essi in dettaglio.





5.2 Lo shopping etico


			Nel quarto capitolo, abbiamo visto che negli ultimi anni è stato proposto un grande numero di principi etici, codici, linee guida o quadri di riferimento. In quel contesto ho ricordato al lettore che, alla fine del 2020, esistevano più di 160 linee guida, che comprendevano tutte numerosi principi, spesso formulati in modo diverso, proprio sull’etica dell’ia (Algorithm Watch, 2019; Winfield, 2019). Questa proliferazione di principi sta generando incoerenza e confusione tra le parti interessate riguardo ai principi che potrebbero essere preferibili. Inoltre, mette pressione agli attori pubblici e privati che disegnano, sviluppano o implementano soluzioni digitali, affinché producano le loro proprie dichiarazioni per timore di apparire retrogradi, contribuendo così ulteriormente alla ridondanza delle informazioni. In questo caso, il principale rischio contrario all’etica è che tutta questa iperattività crei un “mercato di principi e valori” in cui attori pubblici e privati possano acquistare il tipo di etica che meglio si adatta a giustificare i loro comportamenti attuali, piuttosto che rivedere questi comportamenti per renderli coerenti con un quadro etico socialmente condiviso (Floridi, Lord Clement-Jones, 2019). Ecco una definizione di questo rischio:

			Shopping etico digitale = def. il malcostume di scegliere, adattare o rivedere (“mescolare e abbinare”) principi etici, linee guida, codici, quadri di riferimento o altri standard simili (specialmente ma non solo nell’etica dell’ia), estraendoli da una varietà di offerte disponibili, per conferire una patina nuova ad alcuni comportamenti preesistenti (scelte, processi, strategie ecc.) e in tal modo giustificarli a posteriori, invece di implementare o affinare nuovi comportamenti confrontandoli con standard etici pubblici.

			Certo, nel quarto capitolo ho sostenuto che gran parte della diversità “nel mercato dell’etica” è apparente ed è più dovuta alla formulazione e al vocabolario che al contenuto effettivo. Tuttavia, il rischio potenziale di “mescolare e abbinare” l’elenco dei principi etici che si preferiscono, come i gusti del gelato, resta reale, perché la duttilità semantica e la ridondanza consentono il relativismo interpretativo. Lo shopping etico provoca, dunque, l’incompatibilità degli standard. Per esempio, è difficile capire se due aziende seguano gli stessi principi etici nello sviluppo di soluzioni di ia. Inoltre, l’incompatibilità porta a una minore possibilità di concorrenza, valutazione e responsabilità. Tutto ciò può facilmente tradursi in un approccio all’etica come mera sorta di relazioni pubbliche, che non richiede alcun cambiamento o sforzo reali.

			La strategia per affrontare lo shopping etico digitale è quella di stabilire standard etici chiari, condivisi e pubblicamente accettati. Questo è il motivo per cui ho sostenuto (Floridi, 2019a) che la pubblicazione delle Linee guida etiche per l’ia affidabile è stato un miglioramento significativo, dato che rappresenta ciò che più si avvicina nell’Unione Europea a uno standard completo, autorevole e pubblico di ciò che può considerarsi come ia socialmente buona.3 A maggior ragione alla luce del fatto che nel 2021 tali Linee guida hanno esplicitamente influenzato la proposta, adottata dalla Commissione europea, di un regolamento sui sistemi di ia, che è stato descritto come “il primo quadro giuridico in assoluto sull’ia”. Data la disponibilità di questo quadro concettuale, il malcostume dello shopping etico digitale dovrebbe essere almeno più evidente se non più difficile da scusare, perché chiunque nella ue può semplicemente sottoscrivere le Linee guida, piuttosto che acquistare (o addirittura cucinarsi) la propria “etica”. Ciò vale anche per il documento dell’ocse (2019), anch’esso influenzato dalle Linee guida. I membri dell’ocse dovrebbero considerare almeno tale quadro come eticamente vincolante. Nel contesto di questo libro, lo scopo del quarto capitolo è sostenere tale convergenza di opinioni, per garantire che le parti interessate possano raggiungere lo stesso accordo sui principi fondamentali dell’etica dell’ia che possono essere trovati nella bioetica o nell’etica medica, per citare due casi importanti in cui sono inaccettabili lo “shopping etico” e l’adeguamento dei principi etici a pratiche immutate.





5.3 Il “bluewashing” etico


			Nell’etica ambientale, il greenwashing (Delmas, Burbano, 2011) è il malcostume di un attore pubblico o privato che cerca di apparire più verde, più sostenibile o più rispettoso dell’ambiente di quanto non sia in realtà. Con bluewashing etico intendo fare riferimento alla versione digitale del greenwashing.4 Poiché non esiste un colore specifico associato alle buone pratiche etiche nelle tecnologie digitali, il “blu” può servire a ricordare che non stiamo parlando di sostenibilità ecologica ma di mera cosmetica dell’etica digitale.5 Ecco una definizione di questo rischio:

			Bluewashing etico = def. il malcostume di fare affermazioni infondate o fuorvianti al riguardo (o di attuare misure superficiali a favore) dei valori etici e dei benefici di processi, prodotti, servizi o altre soluzioni digitali al fine di apparire più etici dal punto di vista digitale di quanto non si sia effettivamente.

			Il greenwashing e il bluewashing etici sono entrambi forme di disinformazione, spesso ottenute impiegando una frazione delle risorse che sarebbero necessarie per affrontare i problemi etici che pretendono di affrontare. Entrambe queste pratiche scorrette (greenwashing e bluewashing) si concentrano su marketing, pubblicità o altre attività di pubbliche relazioni (per esempio sponsorizzazioni), compresa la creazione di gruppi consultivi che possono essere disarmati, impotenti o non sufficientemente critici. Inoltre, entrambe sono allettanti perché in ogni caso gli obiettivi sono molteplici e tutti compatibili:

			a)	distrarre il destinatario del messaggio – di regola il pubblico, ma anche eventuali azionisti o parti interessate – da tutto ciò che sta andando storto, potrebbe andare meglio o non sta accadendo ma dovrebbe accadere;

			b)	mascherare e lasciare inalterati i comportamenti che dovrebbero essere migliorati;

			c)	conseguire risparmi economici;

			d)	ottenere qualche vantaggio, competitivo o sociale, per esempio in termini di “buona volontà” dei clienti.

			Tuttavia, contrariamente a quanto accade con il greenwashing, il bluewashing può essere più facilmente combinato con lo shopping etico digitale: un attore privato o pubblico acquista i principi che meglio si adattano alle sue pratiche attuali, li pubblicizza il più ampiamente possibile e poi procede a enfatizzare l’impegno etico (bluewash) della sua innovazione tecnologica senza produrre alcun miglioramento reale, costi molto inferiori o qualche potenziale beneficio sociale. Oggigiorno, il bluewashing etico è particolarmente allettante nel contesto dell’ia, dove le questioni etiche sono molte, i costi per fare la cosa giusta possono essere elevati e la confusione normativa è diffusa.

			La migliore strategia contro il bluewashing è la stessa già adottata contro il greenwashing: trasparenza e formazione. La trasparenza pubblica, responsabile e basata su dati in riferimento alle buone pratiche e alle affermazioni etiche, dovrebbe essere una priorità da parte degli attori che desiderano evitare l’apparenza di essere impegnati in qualche pratica scorretta di bluewashing. La formazione pubblica e fattuale di ogni destinatario di bluewashing (non solo il pubblico in generale, ma anche i membri di comitati esecutivi e consigli consultivi, nonché politici e legislatori) sul se e su quali pratiche etiche efficaci siano effettivamente implementate comporta che gli attori potrebbero avere meno probabilità di (essere tentati di) distogliere l’attenzione del pubblico dalle sfide etiche con cui si confrontano e decidere invece di affrontarle.

			Come vedremo nel decimo capitolo, lo sviluppo di metriche per l’affidabilità dei prodotti e servizi di ia (e delle soluzioni digitali in generale) consentirebbe una valutazione comparativa condotta dagli utenti di tutte le offerte commercializzate e faciliterebbe l’individuazione del mero bluewashing, incrementando la comprensione pubblica e la competitività per lo sviluppo di prodotti e servizi più sicuri e vantaggiosi dal punto di vista sociale e ambientale. Sul lungo termine, un sistema di certificazione per prodotti e servizi digitali potrebbe conseguire ciò che altre soluzioni simili hanno ottenuto nell’etica ambientale: rendere il bluewashing visibile e vergognoso quanto il greenwashing.





5.4 Il lobbismo etico


			Gli attori privati cercano, talvolta, di utilizzare l’autoregolazione (o almeno si sospetta che lo facciano) nell’ambito dell’etica dell’ia per esercitare pressioni (azioni di lobbying) contro l’introduzione di norme giuridiche, oppure a favore del loro annacquamento, di una loro applicazione più attenuata, o infine per fornire una giustificazione a una loro limitata osservanza. Questo specifico malcostume investe molti settori, ma pare più probabile in quello digitale (Benkler, 2019), dove l’etica può essere sfruttata come se fosse un’alternativa al diritto, in nome dell’innovazione tecnologica e del suo impatto positivo sulla crescita economica: un ragionamento che non può essere facilmente sostenuto in contesti ambientali o biomedici. Ecco una definizione:

			Lobbismo etico digitale = def. il malcostume di sfruttare l’etica digitale per ritardare, rivedere, sostituire o evitare un’idonea e necessaria regolazione giuridica (o la sua applicazione) relativa al design, lo sviluppo e l’implementazione di processi, prodotti, servizi o altre soluzioni digitali.

			Si potrebbe obiettare che il lobbismo etico digitale è una strategia mediocre, che rischia di fallire nel lungo periodo perché nella migliore delle ipotesi è miope: prima o poi la regolazione giuridica tende a recuperare il ritardo. Tuttavia, indipendentemente dal fatto che questo argomento sia convincente, il lobbismo etico digitale come tattica a breve termine può comunque creare molti danni, ritardando l’introduzione delle norme necessarie, contribuendo a eludere in tutto o in parte interpretazioni più rigorose delle norme esistenti, rendendone così più agevole il rispetto ma disancorandolo dallo spirito della legge, ovvero inducendo i legislatori ad approvare norme ingiustamente più favorevoli al lobbista di quanto ci si aspetterebbe altrimenti. Inoltre, e cosa molto importante, questo malcostume, o il suo sospetto, rischia di minare alla base il valore di qualsiasi autoregolazione etica digitale. Questo danno collaterale è profondamente deplorevole perché l’autoregolazione è uno dei principali e più preziosi strumenti disponibili per la formulazione di policy. Come sosterrò nel sesto capitolo, l’autoregolazione non può di per sé sostituire la legge, ma se correttamente attuata può essere utilmente complementare ogni volta che

			–	la normativa non è disponibile (per esempio, nelle sperimentazioni sui prodotti di realtà aumentata);

			–	la normativa è disponibile, ma necessita anche di un’interpretazione etica (per esempio, in termini di comprensione di un diritto alla spiegazione nel Regolamento generale sulla protezione dei dati personali della ue, gdpr);

			–	la normativa è disponibile, ma necessita anche di un contrappeso etico, se

			–	è meglio non fare qualcosa, anche se non è illecito farlo (per esempio, automatizzare interamente e completamente alcune procedure mediche senza alcuna supervisione umana);

			–	è meglio fare qualcosa, sebbene non sia giuridicamente richiesto (per esempio, implementare migliori condizioni del mercato del lavoro nella gig economy: Tan, Aggarwal, Cowls et al., 2021).

			La strategia contro il lobbismo etico digitale è duplice. Da un lato, deve essere contrastato da una buona normativa e da una sua applicazione efficace. Ciò è più facile se chi esercita pressioni lobbistiche (private o pubbliche) ha minore influenza sui legislatori, ovvero ogniqualvolta l’opinione pubblica può esercitare il giusto livello di pressione etica. D’altra parte, il lobbismo etico digitale deve essere reso manifesto ogni volta che si verifica ed essere chiaramente distinto dalle vere forme di autoregolazione. Ciò può avvenire in modo più credibile se il processo è esso stesso parte di un codice di autoregolamentazione di un intero settore industriale, nel nostro caso quello dell’ia, che ha un interesse più generale a mantenere un contesto integro in cui un’autentica autoregolazione è socialmente benvenuta ed efficace, e il lobbismo etico è presentato come inaccettabile.





5.5 Il dumping etico


			“Dumping etico” è un’espressione coniata nel 2013 dalla Commissione europea per descrivere l’esportazione di pratiche di ricerca contrarie all’etica di paesi dove vi siano cornici legali ed etiche e meccanismi di applicazione delle norme più deboli (o lassisti, o anche solo differenti, nel caso dell’etica digitale). Tale pratica si applica a qualsiasi tipo di ricerca, compresa l’informatica, la scienza dei dati, l’apprendimento automatico, la robotica e altri generi di ia, ma è più grave nei contesti sanitari e biologici. Fortunatamente, l’etica biomedica e ambientale può essere considerata universale e globale: ci sono quadri di riferimento e accordi internazionali ed esistono istituzioni a livello internazionale che ne monitorano l’applicazione, per questo il dumping etico può essere combattuto in modo più efficace e coerente quando la ricerca coinvolge ambiti biomedici ed ecologici. Tuttavia, in ambiti digitali lo spettro di regimi giuridici e cornici etiche facilita l’esportazione di (quelle che sono considerate nel contesto originale in cui opera chi effettua il dumping) pratiche contrarie all’etica (o addirittura illegali) e l’importazione dei risultati di tali pratiche. In altre parole, il problema è duplice, di etica della ricerca e di etica del consumo. Ecco una definizione di questo rischio:

			Dumping etico digitale = def. il malcostume di (a) esportare attività di ricerca riguardo a processi, prodotti, servizi o altre soluzioni digitali, in altri contesti o luoghi (per esempio, da organizzazioni europee al di fuori della ue) in modi che sarebbero eticamente inaccettabili nel contesto o luogo di origine; e di (b) importare i risultati di tali attività di ricerca contrarie all’etica.

			Sia (a) sia (b) sono importanti. Per offrire un esempio concreto, anche se lontano, non è raro che i paesi vietino la coltivazione di organismi geneticamente modificati, ma ne consentano l’importazione. Questa asimmetria di trattamento etico (e giuridico) tra una pratica (ricerca non etica e/o illecita) e il suo risultato (consumo eticamente e legalmente accettabile del prodotto della ricerca non etica) implica che il dumping etico può influenzare l’etica digitale non solo in termini di esportazione non etica di attività di ricerca ma anche in termini di importazione non etica dei risultati di tali attività. Per esempio, un’azienda può esportare la propria ricerca e quindi progettare, sviluppare e addestrare algoritmi, per esempio per il riconoscimento facciale, su dati personali locali in un paese extra ue con un quadro etico e giuridico diverso, più debole o inapplicato, di protezione dei dati personali, che non sarebbe etico e lecito nella ue in forza del gdpr. Una volta addestrati, gli algoritmi possono essere importati nella ue e implementati senza incorrere in alcuna sanzione o disapprovazione. Mentre la prima fase (a) può essere bloccata, almeno in termini di etica della ricerca (Nordling, 2018), la seconda fase (b), che comporta il consumo di risultati della ricerca non etici, è più sfocata, meno visibilmente problematica e quindi più difficile da monitorare e limitare.

			Purtroppo, è probabile che, nel prossimo futuro, il problema del dumping etico digitale diventi sempre più serio, a causa dei seguenti fattori: il profondo impatto delle tecnologie digitali sull’assistenza sanitaria e sociale, nonché su difesa, polizia e sicurezza; la facilità della loro portabilità; la complessità dei processi produttivi (alcuni dei quali possono comportare il dumping etico) e gli immensi interessi economici in gioco. Per esempio, in particolare nell’ambito dell’ia, in cui la ue è un importatore netto di soluzioni dagli Stati Uniti e dalla Cina, gli attori pubblici e privati rischiano non soltanto di esportare pratiche non etiche, ma anche (e indipendentemente) di importare soluzioni che potrebbero essere state sviluppate in modi che non sarebbero stati considerati eticamente accettabili all’interno della ue.

			Anche in questo caso la strategia è duplice. Bisogna concentrarsi sull’etica della ricerca e sull’etica del consumo. Se si vuole essere coerenti, entrambe devono ricevere pari attenzione.

			Per ciò che concerne l’etica della ricerca, è leggermente più facile esercitare un controllo alla fonte, attraverso la gestione etica dei finanziamenti pubblici alla ricerca. Al riguardo, la ue è in una posizione di primo piano. Tuttavia, resta il problema significativo che gran parte dell’attività di ricerca e sviluppo delle soluzioni digitali è nelle mani del settore privato, dove i finanziamenti possono essere meno vincolati dai confini geografici (un attore privato può più facilmente trasferire ricerca e sviluppo in un luogo eticamente meno impegnativo: una variante geografica dello shopping etico visto in precedenza) e non sono verificati eticamente allo stesso modo della ricerca finanziata con fondi pubblici.

			Per ciò che concerne l’etica dei consumi, soprattutto di prodotti e servizi digitali, si può fare molto attraverso l’istituzione di un sistema di certificazione di prodotti e servizi che ne possa regolare l’appalto, così come la fruizione pubblica e privata. Come nel caso del bluewashing, la provenienza affidabile ed eticamente accettabile dei sistemi e delle soluzioni digitali svolgerà un ruolo crescente negli anni a venire se si vuole evitare l’ipocrisia di essere attenti all’etica della ricerca nei contesti digitali e al contempo lassisti rispetto alla fruizione non etica dei suoi risultati.





5.6 L’elusione dell’etica


			Gli esperti di etica conoscono bene l’antico malcostume di applicare due pesi e due misure nelle valutazioni morali. Applicando un approccio più o meno indulgente o rigoroso, si possono valutare e trattare gli agenti (le loro azioni o le conseguenze delle loro azioni) in modo diverso da agenti (azioni o conseguenze) simili, laddove in realtà dovrebbero essere trattati tutti allo stesso modo. Di regola, il rischio di adottare standard diversi si basa, talora persino inavvertitamente, su pregiudizi, parzialità o interessi egoistici. Il rischio che vorrei mettere in evidenza in questo contesto appartiene alla stessa famiglia, ma ha una genesi diversa. Per farne risaltare la specificità, prenderò in prestito l’espressione “elusione dell’etica” dal settore finanziario.6 Ecco una definizione di questo rischio:

			Elusione dell’etica = def. il malcostume di svolgere sempre meno “lavoro etico” (come adempiere ai doveri, rispettare i diritti, onorare gli impegni ecc.) in un dato contesto quanto più basso è percepito (erroneamente) il ritorno di tale lavoro etico in quel contesto.

			L’elusione dell’etica, come il dumping etico, ha radici storiche e segue spesso contorni geopolitici. È più probabile che gli attori si impegnino nel dumping e nell’elusione dell’etica in contesti in cui prevalgono popolazioni svantaggiate, istituzioni più deboli, incertezza giuridica, regimi corrotti, distribuzione iniqua del potere e altri mali economici, giuridici, politici o sociali. Non è insolito mappare con precisione entrambe queste pratiche scorrette lungo il divario tra il Nord e il Sud del mondo, o percepire che entrambe colpiscono principalmente i paesi con basso e medio reddito. Il passato coloniale esercita ancora un ruolo vergognoso. È anche importante ricordare che, nei contesti digitali, queste pratiche scorrette possono investire segmenti di popolazione all’interno del Nord del mondo. La gig economy può essere vista come un caso di elusione dell’etica all’interno dei paesi sviluppati. Lo sviluppo delle automobili a guida autonoma può essere interpretato come un esempio di dumping della ricerca in alcuni Stati statunitensi. In questo caso, la Convenzione di Vienna del 1968 sulla circolazione stradale, che stabilisce i principi internazionali per disciplinare il diritto della circolazione, richiede che un conducente abbia sempre il pieno controllo e la responsabilità del comportamento di un veicolo nel traffico. Tuttavia, gli Stati Uniti non sono un paese firmatario, per cui tale obbligo non si applica lì, il che significa che i codici statali della circolazione non vietano i veicoli automatizzati e diversi Stati hanno emanato disposizioni per i veicoli automatizzati. Questo è anche il motivo per cui la ricerca sulle auto a guida autonoma ha luogo principalmente negli Stati Uniti, così come i relativi incidenti e sofferenze umane.

			La strategia contro l’elusione dell’etica consiste nell’affrontare la sua origine, che è la mancanza di una chiara allocazione di responsabilità. Gli agenti possono essere tanto più tentati di sottrarsi al loro impegno etico in un dato contesto, quanto più reputano di poter trasferire le responsabilità altrove. Ciò accade più frequentemente e facilmente nei “contesti D”, dove la propria responsabilità viene percepita (erroneamente) come meno elevata perché distante, diminuita, delegata o distribuita (Floridi, 2012b). Pertanto, l’elusione dell’etica è il costo non etico della deresponsabilizzazione dell’agire. È tale genesi che ne fa un caso particolare del problema etico dei doppi standard. Per questa ragione, più equità e meno pregiudizi sono necessari, nella misura in cui l’elusione dell’etica è un caso particolare del problema dei doppi standard, ma al contempo insufficienti per rimuovere l’incentivo a impegnarsi nell’elusione dell’etica. Per sradicare un tale malcostume è necessaria anche un’etica della responsabilità distribuita (Floridi, 2016a) che ricollochi le responsabilità – e quindi lode e biasimo, ricompensa e punizione, e in definitiva responsabilità causale e giuridica – nel luogo che è loro proprio.





5.7 Conclusione: l’importanza di conoscere meglio


			In questo capitolo ho fornito una mappa concettuale a coloro che desiderano evitare o ridurre al minimo alcuni dei rischi etici più evidenti e importanti in cui si incorre nel tradurre i principi in pratiche di etica digitale. In una prospettiva socratica, secondo la quale il male umano è un problema epistemico, un malcostume è spesso il risultato di una soluzione mal giudicata o di un’opportunità fraintesa. Comprendere quanto prima possibile che scorciatoie, dilazioni o soluzioni rapide non portano a soluzioni etiche migliori ma a problemi più seri, che diventano sempre più difficili e costosi da risolvere man mano che li si affronta, non garantisce che i cinque malcostumi esaminati in questo capitolo spariranno, ma significa che saranno ridotti nella misura in cui sono realmente basati su incomprensioni e giudizi errati. Non conoscere meglio è la fonte di molti mali.7 Perciò, la soluzione risiede spesso in maggiori e migliori informazioni per tutti. Nelle pagine precedenti ho accennato al fatto che l’autoregolazione può integrare in modo efficace l’approccio legislativo. Questo è tanto più vero quando si tratta di pratiche etiche che presuppongono ma vanno oltre il rispetto della legge: ciò costituisce l’argomento del prossimo capitolo.



* * *





			 				 					1. Vedi Commissione europea, 8 aprile 2019 (hlegai, 2019), pubblicato dal gruppo di esperti di alto livello (hleg) sull’intelligenza artificiale (ai) nominato dalla Commissione europea.



				 					2. Vedi per esempio i dibattiti su (a) la “Valutazione dell’impatto sui diritti umani di Facebook in Myanmar” pubblicata da Business for Social Responsibility, https://www.bsr.org/en/our-insights/blog-view/facebook-in-myanmar-human-rights-impact-assessment; (b) la chiusura dell’Advanced Technology External Advisory Council di Google, https://blog.google/technology/ai/external-advisory-council-help-advance-responsible-development-ai/; e (c) le linee guida etiche per un’ia affidabile, pubblicate dal gruppo di esperti di alto livello della Commissione europea (hlegai, 2018, 2019).



				 					3. Vedi anche Mazzini (2019).



				 					4. Conserviamo l’originaria formulazione di bluewashing, ricalcata su quella di greenwashing, che non ha un diretto corrispettivo nella lingua italiana ma viene di regola tradotta come ecologismo o ambientalismo di facciata. [NdT]



				 					5. Questo non deve essere confuso con il termine bluewashing “usato per criticare le partnership aziendali formate nell’ambito dell’iniziativa Global Compact delle Nazioni Unite (alcuni dicono che questa associazione con le Nazioni Unite aiuta a migliorare la reputazione delle aziende) e per denigrare progetti dubbi sull’uso sostenibile dell’acqua” (Schott, 2010).



				 					6. https://www.nasdaq.com/investing/glossary/s/shirking. Devo il suggerimento di includere l’“elusione dell’etica” come un rischio significativo nell’etica digitale e di utilizzare l’espressione stessa per catturarlo a Cowls, Png, Au (non pubblicato).



				 					7. Un sentito ringraziamento va a Josh Cowls, Jessica Morley e David Watson per i loro feedback di grande valore su una precedente versione di questo capitolo.





6


			Etica soft e governance dell’ia

			Sommario In precedenza, nei capitoli quarto e quinto, ho suggerito un quadro unificato per i principi etici dell’ia e ho identificato alcuni dei principali rischi etici derivanti dalla traduzione dei principi in pratiche. In questo capitolo discuto il tema della governance dell’ia, e più in generale delle tecnologie digitali, nei termini della nuova sfida posta dall’innovazione tecnologica. Introduco una nuova distinzione tra etica soft,1 che si applica dopo il rispetto delle norme giuridiche (cioè l’etica post-compliance), come il Regolamento generale sulla protezione dei dati personali nell’Unione Europea, ed etica hard, che precede e poi contribuisce ulteriormente a plasmare la regolamentazione giuridica. Il capitolo si conclude elaborando un’analisi del ruolo dell’etica digitale in rapporto alla regolamentazione e alla governance del digitale, preparando così il terreno per il capitolo successivo sui principi etici fondamentali per un’etica dell’ia.





6.1 Introduzione: dall’innovazione digitale alla governance del digitale


			Oggi, in qualsiasi matura società dell’informazione (Floridi, 2016b), non viviamo più online o offline, ma onlife (Floridi, 2014b), cioè viviamo sempre più in quello spazio speciale, o infosfera, che è ininterrottamente analogico e digitale, offline e online. Se questo sembra confuso, forse un’analogia può aiutare a chiarire il punto. Immaginiamo che qualcuno ci chieda se l’acqua è dolce o salata nell’estuario dove il fiume incontra il mare. Chiaramente, quel qualcuno non ha compreso la natura particolare del luogo. Le nostre mature società dell’informazione stanno crescendo in questo nuovo e liminale tipo di luogo, come le mangrovie che crescono nell’acqua salmastra. In queste “società delle mangrovie”, i dati leggibili dalle macchine, le nuove forme di agire smart e le interazioni onlife sono in continua evoluzione, perché le nostre tecnologie sono perfettamente adatte a sfruttare questo nuovo ambiente, spesso in quanto unici veri nativi. Di conseguenza, il ritmo della loro evoluzione può essere strabiliante. E ciò a sua volta giustifica una certa apprensione. Tuttavia, non dovremmo essere distratti dalla portata, dalla profondità e dal ritmo dell’innovazione digitale. È vero, stravolge alcuni presupposti profondamente radicati della vecchia società, che era esclusivamente analogica, per esempio, riguardo a concorrenza, intrattenimento, istruzione, lavoro, logistica, personalizzazione, politica, produzione, sanità o sicurezza, solo per citare alcune questioni cruciali, in mero ordine alfabetico. Eppure non è questa la sfida più importante che abbiamo di fronte. Non è l’innovazione digitale ciò che conta di più, ma la governance del digitale, e quello che ne facciamo. Più precisamente, è semmai il modo in cui disegniamo l’infosfera e le mature società dell’informazione che si sviluppano al suo interno a essere più rilevante. E questo perché la rivoluzione digitale trasforma le nostre opinioni sui valori e sulle loro priorità, sui comportamenti buoni e su quale sia il tipo di innovazione non soltanto sostenibile ma anche socialmente preferibile (equa): governare su tutto questo è diventato ormai il problema fondamentale. Lasciatemi spiegare.

			Per molti, ciò che l’innovazione digitale sfornerà in futuro potrebbe sembrare la vera sfida. La domanda stessa è ricorrente e banale: quale sarà la prossima innovazione dirompente? Quale la nuova app sbalorditiva? Sarà questo l’anno della contesa finale tra realtà virtuale e aumentata? O sarà l’Internet delle Cose a rappresentare la nuova frontiera, magari in combinazione con le città smart? È vicina la fine della televisione tradizionale? La sanità sarà stravolta dai sistemi di apprendimento automatico o dovremo concentrare piuttosto la nostra attenzione sull’automazione della logistica e dei trasporti? Che cosa faranno i nuovi assistenti smart nelle nostre case, oltre a dirci che tempo fa e a permetterci di scegliere la prossima canzone? Come si adatterà la strategia militare ai conflitti informatici? Quale industria sarà “uberizzata” prossimamente? Dietro domande simili si cela il tacito assunto per cui l’innovazione digitale è alla guida, e tutto il resto rimane indietro o nel migliore dei casi segue: modelli di business, condizioni di lavoro, standard di vita, legislazione, norme sociali, abitudini, aspettative, persino speranze. Eppure questa è proprio la narrativa fuorviante a cui dovremmo resistere. Non perché sia sbagliata, ma perché è solo superficialmente corretta. La verità più profonda è che la rivoluzione digitale è già avvenuta. Il passaggio da un mondo interamente analogico e offline a uno sempre più digitale e online non si ripeterà mai più nella storia dell’umanità. Forse, un giorno, un dispositivo di calcolo quantistico, che esegue app di ia, potrebbe trovarsi nella tasca di un qualsiasi adolescente, ma la nostra generazione è l’ultima ad aver conosciuto un mondo non digitale. Ed è questa la svolta davvero straordinaria. Perché l’atterraggio nell’infosfera e l’inizio dell’onlife accadono solo una volta. Quale sarà il volto di questo nuovo mondo, via via che lo creiamo, è affascinante, in termini di opportunità, e preoccupante, in termini di rischi. In realtà, l’“esplorazione” dell’infosfera, per indugiare ancora un po’ nella metafora geografica, per quanto impegnativa, fa sorgere una domanda molto più basilare, di carattere sociopolitico e davvero cruciale: che tipo di mature società dell’informazione vogliamo costruire? Qual è il nostro progetto umano per l’era digitale? Guardando indietro al nostro presente, vale a dire da una prospettiva futura, questo è il momento storico in cui si vedrà che abbiamo gettato le basi per le nostre mature società dell’informazione. Saremo giudicati dalla qualità del nostro lavoro. Per questo, chiaramente, la vera sfida non è la buona innovazione digitale, ma la buona governance del digitale, come anticipavo sopra.

			La prova che le cose stanno così possiamo facilmente rinvenirla nella proliferazione di iniziative che affrontano l’impatto del digitale sulla vita quotidiana e il modo di regolarlo. È anche implicito nell’attuale narrativa sulla natura inarrestabile e irraggiungibile dell’innovazione digitale, se la si osserva un po’ più da vicino. Perché nello stesso contesto in cui le persone si lamentano della rapidità dell’innovazione digitale, e dell’impossibilità di starle dietro per racchiuderla entro un quadro normativo, riscontriamo anche la pari certezza che una regolamentazione sbagliata rischia seriamente di compromettere del tutto l’innovazione digitale o di pregiudicare interi settori e sviluppi tecnologici. Non occorre essere Nietzsche – “Was mich nicht umbringt macht mich stärker”, “Ciò che non mi uccide mi rende più forte” (Nietzsche, 1889) – per capire che l’inferenza da trarre è che aggiornare le regole del gioco sia perfettamente possibile – in fin dei conti, tutti riconoscono che può avere enormi conseguenze – ma che reagire all’innovazione tecnologica non sia l’approccio migliore. Dobbiamo passare dall’inseguire al condurre. Se ci piace la direzione in cui ci muoviamo, o dove stiamo andando, allora la velocità con cui ci muoviamo o vi arriviamo può effettivamente rappresentare qualcosa di molto positivo. Quanto più ci piace la nostra destinazione, tanto più rapidamente vorremo arrivarci. È perché ci manca un chiaro senso di direzione sociopolitica che siamo preoccupati dalla velocità del nostro viaggio tecnologico. È giusto esserlo. Eppure la soluzione non è rallentare, ma decidere insieme dove andare. Perché ciò accada, la società deve smettere di giocare in difesa e iniziare a giocare in attacco. La domanda non è se, ma come. E, per iniziare ad affrontare il come, sono utili alcune precisazioni. Questo è il contributo del presente capitolo.





6.2 Etica, regolazione e governance


			Sulla governance delle tecnologie digitali in generale e dell’ia in particolare (non farò questa distinzione nel resto del capitolo, dove parlerò di “digitale” per riferirmi a entrambe) c’è molto da dire, e ancor più da capire e teorizzare, ma un punto è chiaro:

			i)	la governance del digitale, d’ora in poi la governance digitale,

			ii)	l’etica del digitale, d’ora in poi l’etica digitale, nota anche come etica del computer, dell’informazione o dei dati (Floridi, Taddeo, 2016)

			iii)	e la regolazione del digitale, d’ora in poi la regolazione digitale,

			sono approcci normativi diversi, complementari, da non confondere tra loro, ma da tenere chiaramente distinti, nel senso seguente (si veda la Figura 6.1 per una rappresentazione visiva).



			La governance digitale è la pratica di stabilire e attuare politiche, procedure e standard per i corretti sviluppo, utilizzo e gestione dell’infosfera. È anche una questione di convenzioni e buon coordinamento, talora né morale né immorale, né legale né illegale. Per esempio, attraverso la governance digitale un’agenzia governativa o una società può (a) determinare e controllare i processi e i metodi utilizzati dai gestori e custodi di dati al fine di migliorare la qualità, l’affidabilità, l’accesso, la sicurezza e la disponibilità dei loro servizi; e (b) individuare procedure efficaci per i processi decisionali e per l’identificazione delle responsabilità in relazione ai processi relativi ai dati. Un’applicazione tipica della governance digitale è stato il lavoro che ho copresieduto per il British Cabinet Office nel 2016 per un “Quadro etico della scienza dei dati” (Cabinet Office, Government Digital Service, 2016), che era “volto a fornire ai dipendenti pubblici indicazioni per svolgere progetti relativi alla scienza dei dati, e la fiducia per innovare con i dati”.2 Nonostante il titolo, molte raccomandazioni non avevano nulla a che fare con l’etica e riguardavano solo una gestione ragionevole.

			La governance digitale può comprendere linee guida e raccomandazioni che si sovrappongono alla regolazione digitale ma non sono identiche a essa. Questo è solo un altro modo di riferirsi alla legislazione pertinente, un sistema di leggi elaborato e applicato attraverso istituzioni sociali o governative per regolare il comportamento degli agenti rilevanti nell’infosfera. Non tutti gli aspetti della regolazione digitale sono una questione di governance digitale e non tutti gli aspetti della governance digitale sono una questione di regolazione digitale. In questo caso, un buon esempio è fornito dal Regolamento generale sulla protezione dei dati personali (di più sul gdpr di seguito).3 La compliance (vale a dire la conformità alle norme) è la relazione cruciale attraverso la quale la regolazione digitale modella la governance digitale.

			Tutto ciò vale per l’etica digitale, intesa come quel settore dell’etica che studia e valuta i problemi morali relativi a dati e informazioni (inclusi generazione, registrazione, cura, trattamento, diffusione, condivisione e utilizzo), algoritmi (tra cui ia, agenti artificiali, ml e robot) e le relative pratiche e infrastrutture (inclusi innovazione responsabile, programmazione, hackeraggio, codici professionali e standard), al fine di formulare e supportare soluzioni moralmente buone, per esempio buone condotte o buoni valori (Floridi, Taddeo, 2016). L’etica digitale modella la regolazione digitale e la governance digitale attraverso la relazione di valutazione morale di ciò che è socialmente accettabile o preferibile (ibidem).

			La governance digitale nella Figura 6.1 è solo una delle tre forze normative che possono modellare e guidare lo sviluppo del digitale. Ma non è insolito usare quella parte per il tutto e parlare di governance digitale in riferimento all’intero insieme. Si tratta di utilizzare il termine “governance” come sineddoche, un po’ come usare “coca” per qualsiasi varietà di cola. È ciò che ho fatto all’inizio di questo capitolo, quando ho affermato che la vera sfida oggi è la governance del digitale. Con ciò intendevo riferirmi non solo alla governance digitale ma anche all’etica digitale e alla regolazione digitale, ovvero all’intera mappa normativa: E+R+G. Ed è così che interpreto anche il rapporto “Gestione e uso dei dati: la governance nel ventunesimo secolo” che abbiamo pubblicato nel 2017 come gruppo di lavoro congiunto della British Academy e della Royal Society (British Academy, The Royal Society, 2017). Finché la sineddoche è chiara, non c’è problema.

			Una volta compresa la mappa, due importanti conseguenze diventano chiare. Permettetemi di discutere ciascuna di esse in distinti paragrafi.





6.3 La compliance nei confronti delle norme: necessaria ma insufficiente


			Quando i decisori politici, in contesti sia politici sia aziendali, si chiedono perché dovremmo impegnarci in valutazioni etiche quando la compliance, intesa come conformità legale e rispetto delle norme, è già presupposta (questo è, per esempio, un argomento ricorrente nella discussione sul gdpr), la risposta dovrebbe essere chiara: la compliance è necessaria ma insufficiente per guidare la società nella giusta direzione. Perché la regolazione digitale indica quali sono le mosse valide e non valide nel gioco, per così dire, ma non dice nulla su quali potrebbero essere le mosse buone o migliori, tra quelle valide, per vincere la partita, cioè per avere una società migliore. Questo è il compito sia dell’etica digitale, sul lato dei valori e delle preferenze morali, sia della buona governance digitale, sul lato della gestione. Ed è per questo che, per esempio, il Garante europeo della protezione dei dati personali (l’autorità indipendente per la protezione dei dati nella ue) ha istituito nel 2015 il Comitato consultivo etico, per analizzare le nuove sfide etiche sollevate dagli sviluppi digitali e dalla normativa vigente, soprattutto in relazione al gdpr. Il rapporto che abbiamo pubblicato (edps Ethics Advisory Board, 2018) dovrebbe essere letto come un contributo a una governance normativa dell’infosfera nella ue e un trampolino di lancio verso la sua attuazione. Dunque, che tipo di etica digitale dovremmo adottare, per integrare la compliance normativa?





6.4 Etica hard e soft


			Se guardiamo la Figura 6.2, l’etica digitale può dunque essere intesa in due modi, come etica hard o soft. Tale distinzione è vagamente basata su quella tra hard e soft law (Shaffer, Pollack, 2009), cioè fra diritto tradizionale e altri strumenti normativi privi di forza giuridicamente vincolante, come le risoluzioni e le raccomandazioni del Consiglio d’Europa. Tuttavia, è soprattutto una questione teorica – è logicamente possibile e spesso utile distinguere l’etica soft da quella hard e discuterne separatamente – e non tanto una questione pratica, perché in realtà l’etica soft e quella hard spesso si intrecciano indissolubilmente. Esaminiamo la distinzione in dettaglio.



			L’etica hard (vedi A+B+C nella Figura 6.1) è ciò che di solito abbiamo in mente quando discutiamo di valori, diritti, doveri e responsabilità – o, più in generale, di ciò che è moralmente giusto o sbagliato, di ciò che dovrebbe o non dovrebbe essere fatto – quando formuliamo nuove normative o sottoponiamo a critica quelle esistenti. In breve, nella misura in cui (e tale misura può essere non particolarmente estesa) l’etica contribuisce a creare, plasmare o modificare il diritto, possiamo chiamarla etica hard. Per esempio, fare pressione a favore di una buona legislazione o allo scopo di migliorare quella esistente può costituire un caso di etica hard. L’etica hard ha contribuito a smantellare la legislazione sull’apartheid in Sudafrica e ha sostenuto l’approvazione della legislazione in Islanda che richiede alle imprese pubbliche e private di dimostrare di offrire parità di retribuzione ai dipendenti, indipendentemente dal loro genere (a proposito, il divario retributivo tra i generi continua a essere uno scandalo nella maggior parte dei paesi). Ne consegue che, nell’etica hard, non è vero che “si deve fare x” in termini giuridici (dove x contempla l’universo delle azioni possibili) implica “si può fare x” in termini etici. È perfettamente ragionevole aspettarsi che “si deve fare x” possa essere seguito da “anche se non si può fare x”. Definiamolo il Principio di Rosa Parks, per il suo celebre rifiuto di obbedire alla legge e di cedere il suo posto sull’autobus nella “sezione riservata alle persone di colore” a un passeggero bianco, dopo che la sezione per soli bianchi era stata riempita.

			L’etica soft comprende lo stesso ambito normativo dell’etica hard (vedi ancora A+B+C nella Figura 6.1), ma lo fa considerando ciò che dovrebbe o non dovrebbe essere fatto al di là della normativa vigente, non contro di essa, o nonostante il suo ambito di applicazione, o per cambiarla. Pertanto, l’etica soft può includere l’autoregolazione (vedi capitolo 5). In altre parole, l’etica soft è un’etica post-compliance perché, in questo caso, “il dover fare qualcosa implica il poter fare quel qualcosa”. Questo è il motivo per cui, nella Figura 6.1, ho scritto che le regolazioni vincolano l’etica del software attraverso la compliance. Definiamolo il Principio di Matteo, da Matteo 22,15-22: “Rendete a Cesare quel che è di Cesare”.

			Come già indicato in precedenza, sia l’etica hard sia quella soft presuppongono la possibilità o, in termini più kantiani, presuppongono il principio fondamentale per cui “il dovere implica il potere”, dato che un agente ha l’obbligo morale di compiere un’azione x solo se x è in primo luogo possibile. L’etica non dovrebbe essere supererogatoria nel senso specifico di chiedere qualcosa di impossibile. Ne consegue che l’etica soft presuppone anche un approccio post-possibilità. Aggiungiamo che qualsiasi approccio etico, almeno nell’Unione Europea, accetta, come punto di partenza minimo, l’attuazione della Dichiarazione universale dei diritti dell’uomo, della Convenzione europea dei diritti dell’uomo e della Carta dei diritti fondamentali dell’Unione Europea. Il risultato è che lo spazio dell’etica soft è sia parzialmente limitato, sia illimitato. È agevole comprenderne il perché raffigurandolo nella forma di un trapezio (vedi la Figura 6.2), con il lato inferiore che rappresenta una base di possibilità in continua espansione nel tempo (possiamo fare sempre più cose grazie all’innovazione tecnologica), i due lati vincolanti, sinistro e destro, che rappresentano la conformità normativa e i diritti umani, e il lato superiore aperto che rappresenta lo spazio in cui ciò che è moralmente buono può accadere in generale e, nel contesto di questo capitolo, può accadere in quanto modella e guida lo sviluppo etico delle nostre mature società dell’informazione.

			Ho già osservato che l’etica hard e quella soft procedono spesso di pari passo. La loro distinzione è utile ma spesso logica piuttosto che fattuale. Nel prossimo paragrafo, analizzerò la loro relazione reciproca e la loro interazione con il diritto facendo riferimento al caso specifico offerto dal Regolamento generale sulla protezione dei dati personali. In questo paragrafo è d’obbligo un’ultima precisazione.

			Quando distinguibile, l’etica digitale soft può essere esercitata tanto più facilmente quanto più la regolazione digitale si trova sul lato positivo della divisione tra morale e immorale. Pertanto, sarebbe un errore sostenere un approccio etico soft per stabilire un quadro normativo quando gli agenti (soprattutto governi e aziende) operano in contesti in cui i diritti umani sono disattesi, per esempio in Cina, Corea del Nord o Russia. Al contempo, in altri contesti, in cui i diritti umani sono rispettati, può essere nondimeno necessaria un’etica hard per modificare alcune normative vigenti che sono percepite come eticamente inaccettabili. Il referendum irlandese sull’aborto nel 2018 è un buon esempio. In ambito digitale, argomenti di etica hard sono stati utilizzati per contrastare la decisione della Federal Communications Commission (fcc) degli Stati Uniti (dicembre 2017) di abrogare la norma sulla neutralità della rete (il principio per cui tutto il traffico di Internet dovrebbe essere trattato nello stesso modo, senza bloccare, degradare o privilegiare alcun particolare contenuto lecito). Ne è conseguito che, nel marzo 2018, quello di Washington è diventato il primo Stato statunitense ad approvare una normativa che impone la neutralità della rete. All’interno dell’Unione Europea, l’etica soft può essere correttamente esercitata per aiutare gli agenti (inclusi individui, gruppi, aziende, governi, organizzazioni) a sfruttare maggiormente e meglio, dal punto di vista morale, le opportunità offerte dall’innovazione digitale. Perché anche nella ue la normativa è necessaria ma insufficiente. Non disciplina tutto (né dovrebbe farlo), e gli agenti dovrebbero sfruttare l’etica digitale per valutare e decidere quale ruolo desiderano svolgere nell’infosfera, quando le norme non forniscono una risposta semplice o diretta, quando è necessario bilanciare valori e interessi in competizione (o addirittura quando le norme non forniscono chiare indicazioni), e quando c’è di più che può essere fatto al di là di quanto strettamente richiesto dalla legge. In particolare, un buon uso dell’etica soft potrebbe portare le aziende a esercitare una “buona cittadinanza d’impresa” all’interno di una matura società dell’informazione.

			È giunto il momento di fornire un’analisi più specifica, e per questo farò affidamento sul gdpr. La scelta sembra ragionevole: dato che la regolazione digitale nella ue è ora determinata dal gdpr, e che la normativa della ue è solitamente rispettosa dei diritti umani, può essere utile comprendere il valore della distinzione tra etica soft e hard e le loro relazioni con il diritto utilizzando il gdpr come caso concreto di applicazione. L’ipotesi di fondo è che, se l’analisi etica soft/hard non funziona nel caso del gdpr, probabilmente non funzionerà in nessun altro contesto.





6.5 L’etica soft come quadro etico


			Per comprendere il ruolo dell’etica hard e soft rispetto al diritto in generale e al gdpr in particolare, è necessario introdurre cinque elementi (vedi Figura 6.3).4



			In primo luogo, ci sono le implicazioni etiche, giuridiche e sociali (iegs) del gdpr, per esempio, nei confronti delle organizzazioni. Tale è, per esempio, l’impatto del gdpr sulle imprese. Poi c’è il gdpr stesso. Questa è la normativa che ha sostituito la direttiva sulla protezione dei dati 95/46/ce. È progettata per armonizzare le norme sulla protezione dei dati personali in tutta Europa, per proteggere e far rispettare la privacy dei dati di tutti i cittadini della ue, indipendentemente dalla posizione geografica, e per migliorare il modo in cui le organizzazioni in tutta la ue affrontano la privacy dei dati. Il gdpr comprende 99 articoli, questo è il secondo elemento. Come spesso accade con legislazioni complesse, gli articoli non comprendono tutto, lasciano zone grigie di incertezza normativa anche al riguardo di aspetti che trattano, sono soggetti a interpretazioni e possono richiedere un aggiornamento quando sono applicati a circostanze nuove, soprattutto in un contesto tecnologico dove l’innovazione si sviluppa in modo così rapido e radicale, si pensi per esempio ai software di riconoscimento facciale, o ai cosiddetti software di deep fake. Pertanto, per agevolare la comprensione del loro significato, della loro portata e applicabilità, gli articoli sono accompagnati da 173 considerando. Questo è il terzo elemento. I considerando, nel diritto della ue, sono testi che spiegano le ragioni delle disposizioni di un atto, ma non sono giuridicamente vincolanti e non dovrebbero contenere un linguaggio normativo. Normalmente, i considerando sono utilizzati dalla Corte di Giustizia dell’Unione Europea (cgue) per interpretare una direttiva o un regolamento e adottare una decisione nel contesto di un caso concreto.5 Tuttavia, nel caso del gdpr, è importante notare che i considerando possono essere utilizzati anche dal Comitato europeo per la protezione dei dati (edpb, che sostituisce il Gruppo di lavoro dell’articolo 29), per garantire che il gdpr sia applicato coerentemente in Europa. Anche i considerando richiedono un’interpretazione, e questo è il quarto elemento. Parte di questa interpretazione è fornita da un quadro etico, che contribuisce, insieme ad altri fattori, alla comprensione dei considerando. Infine, articoli e considerando sono stati formulati grazie a un lungo processo di negoziazione tra il Parlamento europeo, il Consiglio d’Europa e la Commissione europea (il cosiddetto trilogo formale), sfociato in una proposta comune. Questo è il quinto elemento, ovvero la prospettiva che ha informato l’elaborazione del gdpr. È qui che l’etica hard gioca un ruolo, insieme ad altri fattori (per esempio politici, economici ecc.), che può essere concretamente percepito, svolgendo un’analisi comparativa dei lavori preparatori del Parlamento europeo e della Commissione europea e degli emendamenti al testo della Commissione proposti dal Consiglio europeo.6 Di seguito, una sintesi degli elementi che dobbiamo considerare:

			1.	le implicazioni e opportunità etiche, giuridiche e sociali (ioegs) generate dagli articoli in (2). La distinzione tra implicazioni e opportunità ha lo scopo di comprendere sia ciò che deriva dal gdpr (implicazioni) sia ciò che è lasciato (parzialmente o completamente) non disciplinato dal gdpr. Il lettore che trova ridondante la distinzione (si potrebbe obiettare che le opportunità sono solamente un sottoinsieme delle implicazioni) dovrebbe sentirsi libero di non considerare la o contenuta nell’acronimo “ioegs”. Il lettore che trova la distinzione confusa potrebbe aggiungere al diagramma un altro riquadro, denominato “opportunità” e un’altra freccia, dal gdpr a esso, denominata “genera”. Nella Figura 6.3 ho adottato un compromesso: una denominazione duplice. Si noti che le opportunità non devono essere necessariamente positive, possono essere negative, anche nel senso etico di possibili comportamenti scorretti: per esempio, il gdpr può consentire di sfruttare un’opportunità eticamente sbagliata;

			2.	gli articoli del gdpr che generano (1);

			3.	i considerando del gdpr che contribuiscono a interpretare gli articoli in (2);

			4.	il quado etico soft che contribuisce a interpretare i considerando in (3) e gli articoli in (2), che è coerente con il quadro etico hard in (5), e contribuisce ad affrontare ioegs in (1);

			5.	 il quadro etico hard che contribuisce a generare gli articoli in (2) e i considerando in (3).

			L’etica hard in (5) è l’elemento etico (insieme ad altri) che ha motivato e guidato il processo che ha portato all’elaborazione della legge, in questo caso il gdpr. L’etica soft in (4) fa parte del quadro che consente le migliori interpretazioni dei considerando in (3). Affinché l’etica soft in (4) funzioni in mo-do adeguato nell’interpretazione dei considerando in (3) deve essere coerente con, e informata da, l’etica hard in (5) che ha condotto alla loro formulazione in primo luogo.

			Un altro ottimo esempio è offerto dal rapporto della Camera dei Lord sull’ia (House of Lords – Artificial Intelligence Committee, 2017). Il ragionamento sviluppato nel rapporto è che gli Stati Uniti hanno abbandonato del tutto la leadership morale, mentre Germania e Giappone sono troppo avanti dal punto di vista tecnologico per rendere possibile la concorrenza, ma questo crea un vuoto in cui il Regno Unito dovrebbe posizionarsi quale leader dell’ia etica intesa sia come obiettivo socialmente desiderabile sia come opportunità di business. Ciò spiega in parte la recente creazione del Centro per l’etica dei dati e l’innovazione (il Centro in realtà si concentra molto anche sull’ia). La lezione fondamentale è che, invece di promuovere un insieme di nuove norme, può essere preferibile, all’interno della legislazione attuale, favorire un approccio etico allo sviluppo dell’ia che promuova il bene sociale.

			Chiaramente, il ruolo dell’etica è sia precedente sia successivo alla legge, in quanto contribuisce prima a renderla possibile e in seguito a integrarla (costringendola talora anche a cambiare). In tale contesto, la posizione che sto difendendo sulla relazione tra etica e diritto è vicina alla (e può essere intesa come la controparte etica della) posizione di Dworkin laddove sosteneva che il diritto contiene non solo regole ma anche principi (Dworkin, 1967). Soprattutto nei casi difficili, poco chiari o non disciplinati (i cosiddetti “hard cases” di Dworkin), in cui le regole non riescono a essere applicabili in modo completo o inequivocabile a una situazione concreta o non offrono un approccio adeguato, la decisione del caso è e dovrebbe essere guidata da principi di etica soft. Questi non sono necessariamente esterni al sistema giuridico e usati solo come guida (una posizione difesa da Hart) ma possono e spesso sono incorporati (almeno implicitamente) nel diritto come suoi elementi costitutivi, e agevolano l’esercizio della discrezionalità e del giudizio.7





6.6 Analisi dell’impatto etico


			Dato il futuro aperto dell’etica digitale, è ovvio che l’analisi di previsione dell’impatto etico dell’innovazione digitale, o semplicemente analisi dell’impatto etico (aie), debba diventare una priorità (Floridi, 2014d). Oggi, l’aie può essere basata sull’analisi dei dati applicata strategicamente alla valutazione dell’impatto etico di tecnologie, beni, servizi e pratiche digitali (vedi Figura 6.4). È cruciale perché il compito dell’etica digitale non è soltanto quello di “scrutare nei semi [digitali] del tempo / e dire quali chicchi germoglieranno, e quali no” (Macbeth, i, iii, 59-60), ma anche quello di cercare di determinare quali dovrebbero crescere e quali no.



			Oppure, per usare una metafora introdotta poco sopra, il modo migliore per prendere il treno tecnologico non è inseguirlo, ma trovarsi già alla prossima stazione. Dobbiamo anticipare e guidare lo sviluppo etico dell’innovazione tecnologica. Lo possiamo fare valutando ciò che è effettivamente fattibile, privilegiando, al suo interno, ciò che è sostenibile dal punto di vista ambientale, quindi ciò che è socialmente accettabile e infine, idealmente, scegliendo ciò che è socialmente preferibile (vedi Figura 6.5).





6.7 Preferibilità digitale e cascata normativa


			Non disponiamo ancora, per l’infosfera, di un concetto equivalente alla sostenibilità per la biosfera, perciò la nostra attuale equazione è incompleta (vedi Figura 6.6).



			Nella Figura 6.5, ho suggerito di interpretare la x nella Figura 6.6 come “preferibilità sociale”, ma sono consapevole che questo potrebbe essere solo un surrogato per un’idea migliore da individuare (si osservi che, ovviamente, le tecnologie digitali hanno anche un impatto ambientale, quindi la sostenibilità è rilevante, ma può essere anche fuorviante: vedi Cowls, Tsamados, Taddeo et al., 2021a). Un potenziale candidato potrebbe essere “equo”. Tuttavia, trovare la giusta cornice concettuale può richiedere del tempo, dato che “la tragedia dei beni comuni” è stata pubblicata nel 1968 ma l’espressione “sviluppo sostenibile” è stata coniata solo dal rapporto Brundtland (1987) quasi vent’anni dopo. Eppure la mancanza di terminologia concettuale non rende la buona governance del digitale meno urgente o un mero sforzo utopico. In particolare, l’etica digitale, con i suoi valori, principi, scelte, raccomandazioni e vincoli, influenza già in modo significativo, e talvolta molto più di ogni altra forza, il mondo della tecnologia. Questo accade perché la valutazione di ciò che è moralmente buono, giusto o necessario conforma l’opinione pubblica – e in tal modo quanto socialmente accettabile o preferibile – e ciò che è politicamente possibile, e pertanto, in definitiva, quello che è giuridicamente applicabile, e che gli agenti possono o non possono fare. Sul lungo termine, le persone (come utenti, consumatori, cittadini, pazienti ecc.) sono vincolate in ciò che possono o non possono fare (possibilità) dai beni e servizi forniti dalle organizzazioni, per esempio le imprese, che sono vincolate dal diritto (compliance), ma quest’ultimo è modellato e vincolato (anche, quantunque non solo) dall’etica, che concerne l’ambito in cui le persone decidono in che tipo di società vogliono vivere (vedi Figura 6.7). Purtroppo, una tale cascata normativa diventa palese soprattutto quando si verifica un contraccolpo, cioè specialmente in contesti negativi, quando il pubblico rifiuta alcune soluzioni, anche laddove possono essere buone soluzioni. Una cascata normativa dovrebbe invece essere utilizzata in modo costruttivo, per perseguire la costruzione di una società dell’informazione matura di cui essere orgogliosi.





6.8 Il duplice vantaggio dell’etica digitale


			È ovvio che le tecnologie digitali, e in particolare l’ia, offrono molte opportunità, alle quali sono associati, però, anche sfide e potenziali rischi. Perciò, è altrettanto ovvio che assicurare risultati socialmente preferibili significa risolvere la tensione tra assimilare i benefici e mitigare i potenziali danni, in breve, promuovere queste tecnologie ed evitare al contempo il loro uso improprio, sottoutilizzo o impiego dannoso. È in questo contesto che diventa manifesto anche il valore di un approccio etico. Ho sostenuto poco sopra che il rispetto delle norme è senz’altro necessario, ma largamente insufficiente. L’adozione di un approccio etico all’innovazione digitale conferisce quello che può essere definito un “duplice vantaggio”, riprendendo la terminologia del “duplice uso” diffusa in filosofia della tecnologia almeno a partire dal dibattito sugli usi civili e militari del nucleare. Da un lato, l’etica soft può fornire una strategia di opportunità, consentendo agli attori di sfruttare il valore sociale delle tecnologie digitali. In ciò consiste il vantaggio di poter individuare e sfruttare nuove opportunità socialmente accettabili o preferibili, bilanciando ogni principio di precauzione con il dovere di non omettere ciò che si potrebbe e si dovrebbe fare, per esempio, per sfruttare il patrimonio di dati accumulato o, nel contesto di questo libro, le forme di agire smart esistenti. D’altra parte, l’etica fornisce anche una soluzione per la gestione del rischio, in quanto consente alle organizzazioni di anticipare ed evitare errori costosi (lo scandalo Cambridge Analytica che coinvolge i dati di Facebook è ormai un esempio classico). In ciò consiste il vantaggio di prevenire e mitigare i corsi di azione che risultano socialmente inaccettabili e sono quindi respinti. In tal modo, l’etica può anche abbassare i costi derivanti dall’opportunità delle scelte non compiute o delle opzioni non colte per paura di sbagliare.

			Il duplice vantaggio dell’etica soft può funzionare solo in un contesto di legislazione adeguata, fiducia pubblica e responsabilità chiare in senso più ampio. L’accettazione pubblica e l’adozione di tecnologie digitali, compresa l’ia, avranno luogo soltanto se i benefici saranno percepiti come significativi ed equamente distribuiti, e i rischi come potenziali, ma prevenibili o minimizzabili, o quantomeno come qualcosa contro cui essere protetti – anche tramite gestione del rischio (per esempio assicurazioni) e riparazione dei danni – senza investire ingiustamente gruppi di persone discriminate. Rassicura il fatto che questo sia l’approccio seguito dalla ue nella sua legislazione (European Commission, 2021). Questi atteggiamenti dipenderanno a loro volta dall’impegno pubblico nello sviluppo delle tecnologie di ia (e più in generale di tutte le tecnologie digitali), dall’apertura relativa al modo in cui operano e da meccanismi comprensibili e largamente accessibili di regolamentazione e riparazione. In questo modo, un approccio etico all’ia può essere percepito anche come un precoce sistema di allerta contro i rischi che potrebbero mettere in pericolo intere organizzazioni. Il valore manifesto per qualsiasi organizzazione del duplice vantaggio di un approccio etico all’ia giustifica ampiamente le spese di coinvolgimento, apertura e contestabilità che tale approccio richiede.





6.9 Conclusione: l’etica come strategia


			L’etica in generale, e l’etica digitale in particolare, non può essere una mera suppellettile, un ripensamento, un ultimo arrivato, una civetta di Minerva, per usare la metafora di Hegel sulla filosofia, che si alza in volo solo quando le ombre della notte si stanno addensando, una volta che l’innovazione digitale è avvenuta e sono state implementate soluzioni forse sbagliate, sono state scelte alternative meno buone o sono stati commessi errori. Anche perché alcuni errori sono irreversibili, alcune opportunità mancate sono irrecuperabili e qualsiasi male prevenibile che accade è un disastro morale. Né l’etica può consistere solamente nel mettere in questione qualcosa. La costruzione della consapevolezza critica è importante, ma è anche solo uno dei quattro compiti di un corretto approccio etico al design e alla governance del digitale. Gli altri tre consistono nel segnalare quali sono i problemi etici rilevanti, nell’impegnarsi con le parti interessate da tali problemi etici e, soprattutto, nel disegnare e implementare soluzioni condivisibili. Qualsiasi esercizio etico che non riesca alla fine a fornire e attuare alcune raccomandazioni accettabili è soltanto un timido preambolo. Perciò, l’etica deve informare le strategie per lo sviluppo e l’uso delle tecnologie digitali fin dal principio, quando cambiare rotta è più facile e meno costoso, in termini di risorse, impatto e opportunità mancate. L’etica deve sedersi al tavolo delle scelte politiche e delle procedure decisionali fin dal primo giorno. Perché non dobbiamo solo pensarci due volte ma, soprattutto, dobbiamo pensare prima di fare passi importanti. Ciò è particolarmente rilevante nella ue, dove ho sostenuto che l’etica soft può essere esercitata in modo appropriato e dove è riconosciuto che sia cruciale un approccio etico morbido agli sviluppi seti (scienza, ingegneria, tecnologia e innovazione) (Floridi, Cowls, Beltrametti et al., 2018). Se l’etica digitale soft può essere una priorità ovunque, questo è certamente vero in Europa. Dovremmo adottarla quanto prima possibile. Per farlo è fondamentale avere un punto di partenza condiviso. L’etica sembra avere una cattiva fama quando si tratta di raggiungere un accordo su questioni fondamentali. Questo è giustificato, ma è anche esagerato. Spesso, soprattutto nei dibattiti sulla tecnologia, il disaccordo etico non verte su ciò che è giusto o sbagliato, ma su quanto sia distante da raggiungere, sul perché lo sia, e su come assicurare che ciò che è giusto prevalga su ciò che è sbagliato. Abbiamo visto nel quarto capitolo che ciò è particolarmente vero quando si tratta delle analisi dei principi di base, dove l’accordo è molto più ampio e comune di quanto talvolta si percepisca. È giunto il momento di mappare alcuni dei problemi etici specifici sollevati dagli algoritmi. Questo è il tema del prossimo capitolo.



* * *





			 				 					1. Manteniamo il termine originale inglese, soft, che è invalso nell’uso corrente nell’espressione soft law, per cui sono state offerte nel tempo, senza grande successo, varie ipotesi di traduzione (debole, mite, morbida, soffice ecc.). [NdT]



				 					2. Disponibile su https://www.gov.uk/government/publications/data-science-ethical-framework.



				 					3. Regolamento (ue) 2016/679 del Parlamento europeo e del Consiglio, del 27 aprile 2016, relativo alla protezione delle persone fisiche con riguardo al trattamento dei dati personali e alla libera circolazione di tali dati, che abroga la direttiva 95/46/ce (Regolamento generale sulla protezione dei dati), guue L119, 04/05/2016.



				 					4. In una versione precedente di questo capitolo il testo poteva interpretarsi come se sostenessi che l’etica modella e interpreta il diritto. Questo è semplicemente insostenibile e sono grato a uno dei revisori anonimi per aver evidenziato quella lettura potenzialmente errata.



				 					5. Vedi per esempio “C-131/12 Google Spain SL, Google Inc. v Agencia Española de Protección de Datos, Mario Costeja González”, http://curia.europa.eu/juris/document/document.jsf?text=&docid=152065&pageIndex=0&doclang=IT&mode=req&, o Domestic cctv e Direttiva 95/46/ec (European Court of Justice [ecj] Judgment in Case C-212/13 Ryneš), http://amberhawk.typepad.com/amberhawk/2014/12/what-does-the-ecj-ryne%C5%A1-ruling-mean-for-the-domestic-purpose-exemption.html.



				 					6. European Digital Rights, Comparison of the Parliament and Council Text on the General Data Protection Regulation, https://edri.org/files/EP_Council_Comparison.pdf.



				 					7. Sono molto grato a uno dei revisori anonimi per aver richiamato la mia attenzione su questo legame con la teoria del diritto di Dworkin.





7


			La mappatura dell’etica degli algoritmi

			Sommario In precedenza, nei capitoli 4-6, ho analizzato i principi etici, i rischi e la governance dell’ia. In questo capitolo esaminerò le attuali sfide etiche poste dall’ia, concentrandomi sul dibattito sull’etica degli algoritmi (dirò di più sulla robotica nell’ottavo capitolo). La ricerca sull’etica degli algoritmi si è notevolmente estesa da quando ne ho parlato in Floridi, Sanders (2004) e Floridi (2013). In particolare, negli ultimi dieci anni, dato lo sviluppo esponenziale e l’applicazione diffusa di algoritmi di apprendimento automatico (ml), si sono identificati nuovi problemi etici e soluzioni relative al loro pervasivo utilizzo nella società. In questo capitolo esamino lo stato attuale del dibattito, con l’obiettivo di contribuire all’identificazione e all’analisi delle implicazioni etiche degli algoritmi, di fornire una disamina aggiornata delle questioni epistemiche e normative e di offrire indicazioni utili per una governance di design, sviluppo e implementazione degli algoritmi.





7.1 Introduzione: una definizione operativa di algoritmo


			Vorrei iniziare con una precisazione concettuale. C’è un limitato accordo nella letteratura rilevante sulla definizione di algoritmo. Il termine è spesso usato per indicare sia la definizione formale di un algoritmo in quanto costrutto matematico, con “una struttura di controllo finita, astratta, efficace, composta, data in modo imperativo, che realizza un dato scopo sotto determinate condizioni” (Hill, 2016, p. 47), sia accezioni relative ad ambiti specifici, che si concentrano sull’implementazione di tali costrutti matematici in tecnologie configurate per un compito specifico. In questo capitolo adotto lo stesso approccio che abbiamo seguito al Digital Ethics Lab di Oxford (Mittelstadt, Allo, Taddeo et al., 2016; Tsamados, Aggarwal, Cowls et al., 2021) e mi concentro sulle questioni etiche poste dagli algoritmi come costrutti matematici, dalle loro implementazioni come programmi e configurazioni (applicazioni) e dai modi in cui tali questioni possono essere affrontate.

			Compresi in questi termini, gli algoritmi sono diventati un elemento chiave alla base di servizi e infrastrutture cruciali delle società dell’informazione. Per esempio, gli individui interagiscono con i sistemi di raccomandazione – sistemi algoritmici che danno suggerimenti su ciò che potrebbe piacere a un utente – ogni giorno, che si tratti di scegliere una canzone, un film, un prodotto o persino un amico (Paraschakis, 2017; Perrault, Yoav, Brynjolfsson et al., 2019; Milano, Taddeo, Floridi, 2019, 2020, 2021). Al contempo, scuole e ospedali (Obermeyer, Powers, Vogeli et al., 2019; Zhou, Zhang, Lv et al., 2019; Morley, Machado, Burr et al., 2020), istituzioni finanziarie (Lee, Floridi, 2020; Lee, Floridi, Denev, 2020; Aggarwal, 2020), tribunali (Green, Chen, 2019; Yu, Du, 2019), enti governativi locali (Eubanks, 2017; Lewis, 2019) e governi nazionali (Labati, Genovese, Muñoz et al., 2016; Hauer, 2019; Taddeo, Floridi, 2018b; Taddeo, McCutcheon, Floridi, 2019) fanno sempre più affidamento su algoritmi per prendere decisioni rilevanti.

			La capacità potenziale degli algoritmi di migliorare il benessere individuale e sociale si accompagna a notevoli rischi etici (Floridi, Taddeo, 2016). È risaputo che gli algoritmi non sono eticamente neutri. Consideriamo, per esempio, come i risultati degli algoritmi di traduzione e dei motori di ricerca siano largamente percepiti quali oggettivi, anche se spesso codificano il linguaggio con modalità condizionate dal genere (Larson, 2017; Prates, Avelar, Lamb, 2019). Anche la presenza di pregiudizi è stata ampiamente segnalata, per esempio nella pubblicità algoritmica, con opportunità di lavori più remunerativi e di impieghi nel campo della scienza e della tecnologia pubblicizzati più spesso per gli uomini che per le donne (Datta, Tschantz, Datta, 2015; Lambrecht, Tucker, 2019). Allo stesso modo, gli algoritmi di previsione utilizzati per gestire i dati sanitari di milioni di pazienti negli Stati Uniti aggravano i problemi esistenti, con pazienti bianchi che ricevono cure significativamente migliori rispetto a pazienti di colore che si trovano in situazioni analoghe (Obermeyer, Powers, Vogeli et al., 2019). Mentre vengono discusse e progettate soluzioni a questi e ad altri problemi simili, il numero di sistemi algoritmici che presentano problemi etici continua a crescere.

			Abbiamo visto nella prima parte del libro che, almeno dal 2012, l’ia sta attraversando una nuova “estate”, sia per i progressi tecnici in atto sia per l’attenzione che il settore ha ricevuto da accademici, politici, tecnologi, investitori (Perrault, Yoav, Brynjolfsson et al., 2019) e quindi dal largo pubblico. A seguito di questa nuova “stagione” di successi, vi è stato un incremento considerevole delle ricerche sulle implicazioni etiche degli algoritmi, in particolare in relazione agli aspetti di equità, responsabilità e trasparenza (Lee, 2018; Hoffmann, Roberts, Wolf et al., 2018; Shin, Park, 2019). Nel 2016, il nostro gruppo di ricerca presso il Digital Ethics Lab ha pubblicato uno studio completo che ha cercato di mappare queste preoccupazioni etiche (Mittelstadt, Allo, Taddeo et al., 2016). Tuttavia, si tratta di un settore in rapida evoluzione e sono emersi sia nuovi problemi etici sia nuovi modi per affrontarli, rendendo necessario migliorare e aggiornare tale studio. In particolare, i lavori sull’etica degli algoritmi sono aumentati in modo significativo dal 2016, quando i governi nazionali, le organizzazioni non governative e le aziende private hanno iniziato ad assumere un ruolo di primo piano nel dibattito sull’ia e sugli algoritmi “equi” ed “etici” (Sandvig, Hamilton, Karahalios et al., 2016; Binns, 2018; Selbst, Boyd, Friedler et al., 2019; Wong, 2019; Ochigame, 2019). Sia la quantità sia la qualità delle ricerche disponibili sul tema sono enormemente cresciute. Inoltre, la pandemia da Covid-19 ha esacerbato tanto i problemi quanto la diffusa consapevolezza globale su di essi (Morley, Cowls, Taddeo et al., 2020). Alla luce di questi cambiamenti, abbiamo pubblicato un nuovo studio (Tsamados, Aggarwal, Cowls et al., 2021; vedi anche Morley, Morton, Karpathakis et al., 2021), migliorando il precedente (Mittelstadt, Allo, Taddeo et al., 2016), aggiungendo nuove prospettive sull’etica degli algoritmi, aggiornando l’analisi iniziale, includendo riferimenti alla letteratura che erano assenti nella rassegna iniziale, e ampliando i temi analizzati, includendo per esempio il lavoro sull’ia per il bene sociale. Allo stesso tempo, la mappa concettuale proposta nel 2016 (vedi Figura 7.1) rimane un quadro fecondo per esaminare l’attuale dibattito sull’etica degli algoritmi, identificare i problemi etici che gli algoritmi generano e le soluzioni che sono state proposte nella letteratura rilevante recente; per cui nel prossimo paragrafo prenderò le mosse da tale quadro. Questo capitolo si basa su questi due lavori. Nei paragrafi 3-8, fornirò una meta-analisi dell’attuale dibattito sull’etica degli algoritmi e traccerò dei collegamenti con i tipi di preoccupazioni etiche precedentemente identificati. Il paragrafo 9 conclude il capitolo con una panoramica e un’introduzione al capitolo successivo.





7.2 La mappa dell’etica degli algoritmi


			Si possono usare algoritmi

			1.	per trasformare i dati in prove (informazioni) per un dato risultato

			che si usa

			2.	per innescare e motivare un’azione che può avere conseguenze etiche.

			Le azioni (1) e (2) possono essere eseguite da algoritmi (semi) autonomi, come gli algoritmi di apprendimento automatico (ml), e questo complica una terza azione, vale a dire:

			3.	attribuire la responsabilità degli effetti delle azioni che un algoritmo può innescare.

			Nel contesto di (1)-(3), il ml è di particolare interesse, in quanto campo che include architetture di deep learning (apprendimento profondo). I sistemi informatici che implementano algoritmi di ml possono essere descritti come “autonomi” o “semi-autonomi”, nella misura in cui i loro risultati sono indotti dai dati e quindi non sono deterministici.

			In base a questo approccio, la mappa concettuale mostrata nella Figura 7.1 identifica sei questioni etiche, che definiscono lo spazio concettuale dell’etica degli algoritmi in quanto ambito di ricerca. Tre delle questioni etiche si riferiscono a fattori epistemici, in particolare: prove inconcludenti, imperscrutabili e fuorvianti. Due sono esplicitamente normative: esiti ingiusti ed effetti trasformativi; mentre una, la tracciabilità, è rilevante a fini sia epistemici sia normativi. I fattori epistemici nella mappa mostrano la rilevanza della qualità e dell’accuratezza dei dati (Floridi, Illari, 2014) per la giustificabilità delle conclusioni che gli algoritmi raggiungono e che, a loro volta, possono modellare decisioni moralmente rilevanti che riguardano individui, società e ambiente. Le questioni normative identificate nella mappa si riferiscono esplicitamente all’impatto etico di azioni e decisioni guidate da algoritmi, che includono la mancanza di trasparenza (opacità) dei processi algoritmici, i risultati ingiusti e le conseguenze non volute. Le questioni epistemiche e normative, insieme alla distribuzione del design, dello sviluppo e dell’implementazione degli algoritmi, rendono difficile tracciare la catena di eventi e fattori che conducono a un determinato risultato, ostacolando così la possibilità di identificarne la causa e attribuirne la responsabilità (Floridi, 2012b). A ciò si riferisce la sesta questione etica, la tracciabilità.



			È importante sottolineare che questa mappa concettuale può essere interpretata a un livello di astrazione micro e macro-etico (Floridi, 2008b). A livello micro-etico, fa luce sui problemi etici che particolari algoritmi possono porre. Evidenziando come questi temi siano inseparabili da quelli relativi ai dati e alle responsabilità, mostra la necessità di adottare un approccio macro-etico per affrontare l’etica degli algoritmi come parte di uno spazio concettuale più ampio, ovvero l’etica digitale (Floridi, Taddeo, 2016). Come abbiamo sostenuto in passato:

			Pur trattandosi di linee di ricerca distinte, l’etica dei dati, degli algoritmi e delle pratiche sono naturalmente intrecciate… L’etica [digitale] deve affrontare l’intero spazio concettuale e pertanto tutti e tre gli assi di ricerca, anche se con priorità e focus differenti. (Ibidem, p. 4)

			Nel resto di questo capitolo affronterò a turno ciascuna di queste sei questioni etiche, offrendo un’analisi aggiornata della letteratura sull’etica degli algoritmi (a livello micro), con l’obiettivo di contribuire al dibattito sull’etica digitale (a livello macro).





7.3 Prove inconcludenti che portano ad azioni ingiustificate


			La ricerca incentrata su prove inconcludenti si riferisce al modo in cui gli algoritmi non deterministici di ml producono output espressi in termini probabilistici (James, Witten, Hastie et al., 2013; Valiant, 1984). Questi tipi di algoritmi generalmente identificano l’associazione e la correlazione tra le variabili nei dati sottostanti, ma non le connessioni causali. In quanto tali, possono incoraggiare la pratica dell’apofenia:

			vedere schemi ricorrenti (patterns) dove in realtà non ne esistono, semplicemente perché enormi quantità di dati possono offrire connessioni che si irradiano in tutte le direzioni. (Boyd, Crawford, 2012, p. 668)

			Ciò è problematico, poiché gli schemi ricorrenti (patterns) identificati dagli algoritmi possono essere il risultato di proprietà intrinseche del sistema modellato dai dati, degli insiemi di dati (cioè del modello stesso, piuttosto che del sistema sottostante) o di un’abile manipolazione degli insiemi di dati (proprietà né del modello né del sistema). Questo è il caso, per esempio, del paradosso di Simpson, in cui le tendenze osservate in diversi gruppi di dati si invertono quando i dati vengono aggregati (Blyth, 1972). Negli ultimi due casi, la scarsa qualità dei dati porta a prove inconcludenti a sostegno delle decisioni umane.

			Ricerche recenti hanno rimarcato la preoccupazione che prove inconcludenti possano dar luogo a gravi rischi etici. Per esempio, concentrarsi su indicatori non causali può distogliere l’attenzione dalle cause alla base di un determinato problema (Floridi, Cowls, King et al., 2020). Anche con l’uso di metodi causali, i dati disponibili potrebbero non contenere sempre informazioni sufficienti per giustificare un’azione o rendere equa una decisione (Olhede, Wolfe, 2018, p. 7). La qualità dei dati, come la tempestività, la completezza e la correttezza di un insieme di dati, delimita le domande a cui è possibile rispondere utilizzando un determinato insieme di dati (Olteanu, Castillo, Diaz et al., 2016). Inoltre, le informazioni che possono essere estratte dagli insiemi di dati dipendono fondamentalmente dai presupposti che hanno guidato il processo stesso di raccolta dei dati (Diakopoulos, Koliska, 2017). Per esempio, gli algoritmi progettati per prevedere gli esiti dei pazienti in ambito clinico si basano interamente su input di dati che possono essere quantificati (per esempio, segni vitali e precedenti tassi di successo di analoghe cure), mentre ignorano altri fattori emozionali (per esempio, la volontà di vivere), che possono avere un impatto significativo sugli esiti dei pazienti e quindi minare l’accuratezza della previsione algoritmica (Buhmann, Paßmann, Fieseler, 2019). Questo esempio mette in luce come le informazioni derivanti dall’elaborazione algoritmica dei dati possono essere incerte, incomplete e sensibili al tempo (Diakopoulos, Koliska, 2017).

			Si può abbracciare un approccio ingenuo e induttivista e presumere che le prove inconcludenti possano essere evitate se gli algoritmi sono alimentati con dati sufficienti, sebbene non sia possibile fornire una spiegazione causale per questi risultati. Tuttavia, la ricerca recente rifiuta questa visione. In particolare, la letteratura incentrata sui rischi etici della profilazione razziale mediante sistemi algoritmici ha dimostrato i limiti di tale approccio, evidenziando tra l’altro che le disuguaglianze strutturali di vecchia data sono spesso profondamente radicate negli insiemi di dati degli algoritmi e raramente, se non mai, emendate (Hu, 2017; Turner Lee, 2018; Noble, 2018; Benjamin, 2019; Richardson, Schultz, Crawford, 2019; Abebe, Barocas, Kleinberg et al., 2020). Più dati non conducono da soli a una maggiore precisione o a una maggiore rappresentazione. Al contrario, possono esacerbare il problema dei dati inconcludenti, consentendo di rinvenire correlazioni dove in realtà non ci sono. Come ha affermato Ruha Benjamin, “la profondità computazionale senza la profondità storica o sociologica è soltanto un apprendimento superficiale [non un apprendimento profondo]”.1

			Le suddette limitazioni pongono gravi vincoli alla giustificabilità dei risultati algoritmici, che potrebbero avere un impatto negativo su individui o su un’intera popolazione a causa di inferenze subottimali o, nel caso delle scienze fisiche, addirittura ribaltare le prove a favore o contro “una specifica teoria” (Ras, van Gerven, Haselager, 2018, p. 10). Questo è il motivo per cui è fondamentale garantire che i dati forniti agli algoritmi siano convalidati in modo indipendente e che siano messe in atto misure di conservazione e riproducibilità dei dati per mitigare le prove inconcludenti che portano ad azioni ingiustificate, insieme a processi di auditing per identificare risultati ingiusti e conseguenze non volute (Henderson, Sinha, Angelard-Gontier et al., 2018; Rahwan, 2018; Davis, Marcus, 2019; Brundage, Avin, Wang et al., 2020).

			Il pericolo derivante da prove inconcludenti ed errate informazioni attuabili deriva anche dall’oggettività meccanicistica percepita associata all’analisi generata dal computer (Karppi, 2018; Lee, 2018; Buhmann, Paßmann, Fieseler, 2019). Ciò può portare i decisori umani a ignorare le proprie valutazioni basate sull’esperienza – il cosiddetto “pregiudizio dell’automazione” (Cummings, 2012) – o persino a sottrarsi a parte della loro responsabilità per le decisioni (vedi il paragrafo 8) (Grote, Berens, 2020). Come vedremo nei paragrafi 4 e 8, l’incapacità di comprendere come gli algoritmi generano i risultati aggrava questo problema.





7.4 Prove imperscrutabili che portano all’opacità


			Le prove imperscrutabili riguardano i problemi legati alla mancanza di trasparenza che spesso caratterizzano gli algoritmi, in particolare algoritmi e modelli di ml, l’infrastruttura sociotecnica in cui essi esistono e le decisioni che supportano. L’assenza di trasparenza – intrinsecamente dovuta ai limiti della tecnologia, derivanti da decisioni di design e dall’offuscamento dei dati sottostanti (Lepri, Oliver, Letouzé et al., 2018; Dahl, 2018; Ananny, Crawford, 2018; Weller, 2019) oppure dovuta a vincoli giuridici in termini di proprietà intellettuale – si traduce spesso in una mancanza di controllo e/o di responsabilità (Oswald, 2018; Webb, Patel, Rovatsos et al., 2019) e conduce a una mancanza di “affidabilità” (vedi hlegai, 2019).

			Secondo studi recenti (Diakopoulos, Koliska, 2017; Stilgoe, 2018; Zerilli, Knott, Maclaurin et al., 2019; Buhmann, Paßmann, Fieseler, 2019), i fattori che contribuiscono alla mancanza generale di trasparenza algoritmica includono:

			–	l’impossibilità cognitiva per gli esseri umani di interpretare giganteschi modelli algoritmici e insiemi di dati;

			–	una mancanza di strumenti appropriati per visualizzare e tenere traccia di grandi volumi di codice e dati;

			–	codice e dati così mal strutturati da essere impossibili da leggere;

			–	aggiornamenti continui e influenza umana sul modello.

			L’assenza di trasparenza è anche una caratteristica intrinseca degli algoritmi di autoapprendimento, che alterano la loro logica decisionale (producono nuovi insiemi di regole) durante il processo di apprendimento, rendendo difficile per gli sviluppatori mantenere una comprensione dettagliata del motivo per cui sono state apportate alcune modifiche specifiche (Burrell, 2016; Buhmann, Paßmann, Fieseler, 2019). Tuttavia, questo non si traduce necessariamente in risultati opachi, poiché, anche senza comprendere ogni passaggio logico, gli sviluppatori possono regolare gli iperparametri, i parametri che regolano il processo di addestramento, per verificare vari output. A questo proposito, Martin (2019) sottolinea che, se certamente è reale la difficoltà di spiegare l’output degli algoritmi di ml, è al contempo importante non lasciare che questa difficoltà incentivi le organizzazioni a sviluppare sistemi complessi per sottrarsi alle responsabilità.

			La mancanza di trasparenza può anche derivare dalla malleabilità degli algoritmi, per la quale gli algoritmi possono essere riprogrammati in modo continuo, distribuito e dinamico (Sandvig, Hamilton, Karahalios et al., 2016). La malleabilità algoritmica consente agli sviluppatori di monitorare e migliorare un algoritmo già implementato, ma può essere anche abusata per offuscare la storia della sua evoluzione e lasciare gli utenti finali incerti sulle possibilità di un determinato algoritmo (Ananny, Crawford, 2018). Consideriamo, per esempio, l’algoritmo di ricerca principale di Google. La sua malleabilità consente all’azienda di effettuare continue revisioni, suggerendo uno stato permanente di destabilizzazione (Sandvig, Hamilton, Karahalios et al., 2016). Ciò richiede a coloro che sono interessati dall’algoritmo di monitorarlo costantemente e aggiornare di conseguenza la propria comprensione, un compito impossibile per i più (Ananny, Crawford, 2018).

			Come abbiamo osservato in passato, la trasparenza non è un

			principio etico in sé, ma una condizione pro-etica per consentire o frenare altre pratiche o principi etici. (Turilli, Floridi, 2009, p. 105)

			In altri termini, la trasparenza non è un valore intrinsecamente etico, ma uno strumento prezioso per fini etici. Fa parte delle condizioni di possibilità del comportamento etico, ciò che ho definito altrove come “infraetica” (etica infrastrutturale: Floridi, 2017a). Talora, l’opacità può essere più utile, per esempio, per assicurare la segretezza delle preferenze e dei voti politici dei cittadini, o per garantire la concorrenza nelle aste per i servizi pubblici. Infatti, anche in contesti algoritmici, la completa trasparenza può causare essa stessa specifici problemi etici (Ananny, Crawford, 2018). Può fornire agli utenti informazioni rilevanti sulle caratteristiche e sui limiti di un algoritmo, ma può anche sovraccaricare gli utenti di informazioni e in tal modo rendere l’algoritmo più opaco (Kizilcec, 2016; Ananny, Crawford, 2018). Altre ricerche sottolineano che un’eccessiva attenzione alla trasparenza può risultare dannosa per l’innovazione e distrarre inutilmente risorse che potrebbero invece essere impiegate per migliorare la sicurezza, le prestazioni e la precisione (Danks, London, 2017; Oswald, 2018; Ananny, Crawford, 2018; Weller, 2019). Per esempio, il dibattito sulla priorità della trasparenza (e della spiegabilità) è particolarmente controverso nel contesto degli algoritmi medici (Robbins, 2019). La trasparenza può anche consentire alle persone di ingannare il sistema (Martin, 2019; Magalhães, 2018; Cowls, King, Taddeo et al., 2019). La conoscenza della fonte di un set di dati, degli assunti in base ai quali è stato eseguito il campionamento o delle metriche adoperate da un algoritmo per classificare nuovi input, può essere usata per capire come sfruttare un algoritmo (Szegedy, Zaremba, Sutskever et al., 2014; Yampolsky, 2018). Tuttavia, la capacità di ingannare gli algoritmi è alla portata solo di alcuni gruppi di persone, in particolare quelli con competenze digitali più elevate e le risorse necessarie per utilizzarle, creando così un’ulteriore forma di disuguaglianza sociale (Martin, 2019; Bambauer, Zarsky, 2018). Perciò, confondere la trasparenza per un fine in sé, invece di considerarla un fattore pro-etico (Floridi, 2017a) che deve essere adeguatamente disegnato per consentire pratiche etiche cruciali, può non risolvere i problemi etici esistenti legati all’uso di algoritmi e, anzi, porne di nuovi. Ecco perché è importante distinguere tra i diversi elementi che possono ostacolare la trasparenza degli algoritmi, identificarne la causa e attenuare la richiesta di trasparenza, specificando quali fattori sono richiesti e a quali livelli dei sistemi algoritmici dovrebbero essere affrontati (Diakopoulos, Koliska, 2017).

			Esistono diversi modi per affrontare i problemi legati alla mancanza di trasparenza. Per esempio, Gebru e coautori propongono di affrontare i vincoli alla trasparenza posti dalla malleabilità degli algoritmi, in parte, utilizzando procedure documentali standard simili a quelle utilizzate nell’industria elettronica, dove

			ogni componente, non importa quanto semplice o complesso, è accompagnato da una scheda tecnica che ne descrive le caratteristiche operative, i risultati dei test, l’utilizzo consigliato e altre informazioni. (Gebru, Morgenstern, Vecchione et al., 2020, p. 2)

			Sfortunatamente, la documentazione disponibile per il pubblico è attualmente rara nello sviluppo di sistemi algoritmici e non esiste un formato condiviso per ciò che dovrebbe essere incluso quando si documenta l’origine di un insieme di dati (Arnold, Bellamy, Hind et al., 2019; Gebru, Morgenstern, Vecchione et al., 2020).

			Sebbene relativamente agli inizi, un altro approccio potenzialmente promettente per far rispettare la trasparenza algoritmica è l’utilizzo di strumenti tecnici per testare e controllare i sistemi algoritmici e il processo decisionale (Mökander, Floridi, 2021). In tale contesto, è sufficiente notare che verificare se gli algoritmi mostrano tendenze negative, come discriminazioni inique, e controllare in dettaglio una previsione o un percorso decisionale, può contribuire a mantenere un elevato livello di trasparenza (Weller, 2019; Malhotra, Kotwal, Dalal, 2018; Brundage, Avin, Wang et al., 2020). A tal fine, sono state sviluppate cornici discorsive per aiutare le imprese e le organizzazioni del settore pubblico a comprendere i potenziali impatti di algoritmi opachi, incoraggiando così buone pratiche (ico, 2020). Per esempio, l’ai Now Institute della New York University ha prodotto una guida alla valutazione dell’impatto algoritmico, che cerca di incrementare la consapevolezza e di migliorare il dialogo sui potenziali danni degli algoritmi di ml (Reisman, Schultz, Crawford et al., 2018). Ciò include due obiettivi: quello di consentire agli sviluppatori di disegnare algoritmi di ml più trasparenti e quindi più affidabili e quello di migliorare la comprensione e il controllo degli algoritmi da parte del pubblico. Allo stesso modo, Diakopoulos e Koliska hanno fornito un elenco completo di “fattori di trasparenza” attraverso quattro livelli di sistemi algoritmici: dati, modello, inferenza e interfaccia. Tali fattori includono, tra l’altro,

			incertezza (es. margini di errore), tempestività (es. quando sono stati raccolti i dati), completezza o elementi mancanti, metodo di campionamento, provenienza (es. fonti) e volume (es. dei dati di addestramento utilizzati in ml). (Diakopoulos, Koliska, 2017, p. 818)

			Procedure di trasparenza efficaci implicano di solito, anzi dovrebbero farlo sempre, una spiegazione interpretabile dei processi interni di questi sistemi. Buhmann, Paßmann e Fieseler (2019) sostengono che, benché l’assenza di trasparenza sia una caratteristica intrinseca di molti algoritmi di ml, ciò non significa che non possano essere apportati miglioramenti. Per esempio, aziende come Google e ibm hanno incrementato i loro sforzi per rendere gli algoritmi di ml più interpretabili e inclusivi, rendendo disponibili per il pubblico strumenti come Explainable ai, ai Explainability 360 e What-If Tool. Sicuramente saranno sviluppate altre strategie (Watson, Floridi, 2020; Watson, Gultchin, Taly et al., 2021) e strumenti, che offrano agli sviluppatori e al pubblico in generale interfacce visive interattive che migliorino la leggibilità umana, esplorino vari risultati del modello, forniscano ragionamenti basati su casi, regole direttamente interpretabili e identifichino e mitighino anche i pregiudizi non voluti negli insiemi di dati e nei modelli algoritmici (Mojsilovic, 2018; Wexler, 2018).

			Tuttavia, le spiegazioni per gli algoritmi di ml sono limitate dal tipo di spiegazione ricercata, dal fatto che le decisioni sono spesso multidimensionali per loro natura e che utenti diversi possono richiedere spiegazioni diverse (Edwards, Veale, 2017; Watson, Kurtzinna, Bruce et al., 2019). Identificare metodi appropriati per fornire spiegazioni è stato un problema dalla fine degli anni Novanta (Tickle, Andrews, Golea et al., 1998), ma gli sforzi attuali possono essere classificati in due approcci principali: le spiegazioni incentrate sul soggetto e le spiegazioni incentrate sul modello (Doshi-Velez, Kim, 2017; Lee, Kim, Lizarondo, 2017; Baumer, 2017; Buhmann, Paßmann, Fieseler, 2019). Nel primo, l’accuratezza e la lunghezza della spiegazione sono ritagliate sugli utenti e sulle loro interazioni specifiche con un determinato algoritmo (vedi per esempio Green, Viljoen, 2020, e il modello simile a un gioco che ho proposto con David Watson in Watson, Floridi, 2020). Nel secondo, le spiegazioni riguardano il modello nel suo insieme e non dipendono dal loro pubblico.

			La spiegabilità è particolarmente importante se si considera il numero in rapida crescita di modelli e insiemi di dati open source e di facile utilizzo. Sempre più spesso, non esperti sperimentano modelli algoritmici all’avanguardia ampiamente disponibili tramite librerie o piattaforme online, come GitHub, senza coglierne sempre appieno i limiti e le proprietà (Hutson, 2019). Ciò ha spinto gli studiosi a suggerire che, per affrontare il problema della complessità tecnica, è necessario investire maggiormente nell’istruzione pubblica per migliorare l’alfabetizzazione computazionale e relativa ai dati (Lepri, Oliver, Letouzé et al., 2018). Questa strategia rappresenterebbe un contributo a lungo termine adeguato a risolvere i problemi introdotti a più livelli dalla diffusione degli algoritmi, e in questa prospettiva il software open source è spesso citato come fondamentale per la loro soluzione (ibidem).





7.5 Prove fuorvianti che portano a pregiudizi (BIAS) non voluti


			Gli sviluppatori si preoccupano principalmente di assicurare che i loro algoritmi svolgano i compiti per cui sono stati disegnati. Perciò, il modo di pensare che guida gli sviluppatori è essenziale per comprendere come i pregiudizi possano emergere negli algoritmi e nel processo decisionale algoritmico. Alcuni studiosi si riferiscono al pensiero dominante nel campo dello sviluppo di algoritmi nei termini di “formalismo algoritmico”, caratterizzato dall’adesione a regole e forme prescritte (Green, Viljoen, 2020, p. 21). Sebbene questo approccio sia utile per astrarre e definire i processi analitici, tende a ignorare la complessità sociale del mondo reale (Katell, Young, Dailey et al., 2020). In effetti, questo approccio porta a interventi algoritmici che si sforzano di essere “neutri” ma, in tal modo, rischia di consolidare le condizioni sociali già esistenti (Green, Viljoen, 2020, p. 20), mentre crea l’illusione della precisione (Karppi, 2018; Selbst, Boyd, Friedler et al., 2019). Per tali motivi, in alcuni contesti l’uso di algoritmi viene messo del tutto in discussione (Selbst, Boyd, Friedler et al., 2019; Mayson, 2019; Katell, Young, Dailey et al., 2020; Abebe, Barocas, Kleinberg et al., 2020). Per esempio, un numero crescente di studiosi critica l’impiego di strumenti di valutazione del rischio basati su algoritmi in ambito giudiziario (Berk, Heidari, Jabbari et al., 2018; Abebe, Barocas, Kleinberg et al., 2020). Alcuni studiosi sottolineano i limiti delle astrazioni per quanto riguarda i pregiudizi non voluti negli algoritmi e sostengono la necessità di sviluppare una cornice sociotecnica per affrontare e migliorare l’equità degli algoritmi (Edwards, Veale, 2017; Selbst, Boyd, Friedler et al., 2019; Wong, 2019; Katell, Young, Dailey et al., 2020; Abebe, Barocas, Kleinberg et al., 2020). A questo proposito, Selbst e coautori (2019, pp. 60-63) indicano cinque “trappole” dell’astrazione, o incapacità di rendere conto del contesto sociale in cui operano gli algoritmi, che permangono nel design algoritmico a causa dell’assenza di una cornice sociotecnica, vale a dire:

			1.	l’incapacità di modellare l’intero sistema a cui sarà applicato un criterio sociale, come l’equità;

			2.	l’incapacità di comprendere come la riproposizione di soluzioni algoritmiche disegnate per un contesto sociale possa risultare fuorviante, imprecisa o comunque arrecare danno se applicata a un contesto diverso;

			3.	l’incapacità di rendere pienamente conto del significato di concetti sociali come equità, che possono essere procedurali, contestuali e discutibili, e non possono essere risolti tramite formalismi matematici;

			4.	l’incapacità di comprendere come l’introduzione di una tecnologia in un sistema sociale esistente modifichi i comportamenti e i valori incorporati nel sistema preesistente;

			5.	l’incapacità di riconoscere che la migliore soluzione a un problema possa non coinvolgere la tecnologia.

			Il termine “pregiudizio” (bias) ha spesso un’accezione negativa, ma qui è usato per indicare una “deviazione da uno standard” (Danks, London, 2017, p. 4692), che può verificarsi in qualsiasi fase del processo di design, sviluppo e implementazione. I dati utilizzati per addestrare un algoritmo sono una delle principali fonti da cui emerge il pregiudizio (Shah, 2018), attraverso dati campionati in modo preferenziale o da dati che riflettono pregiudizi sociali già esistenti (Diakopoulos, Koliska, 2017; Danks, London, 2017; Binns, 2018; Malhotra, Kotwal, Dalal, 2018). Per esempio, disuguaglianze strutturali moralmente problematiche, che svantaggiano alcune etnie, potrebbero non essere evidenti nei dati e quindi non corrette (Noble, 2018; Benjamin, 2019). Inoltre, i dati utilizzati per addestrare gli algoritmi sono raramente ottenuti “secondo uno specifico design sperimentale” (Olhede, Wolfe, 2018, p. 3) e sono impiegati anche se possono essere imprecisi, falsati o sistematicamente distorti, offrendo una rappresentazione inesatta della popolazione sotto esame (Richardson, Schultz, Crawford, 2019).

			Un possibile approccio per mitigare questo problema consiste nell’escludere intenzionalmente alcune specifiche variabili di dati dalla formazione del processo decisionale algoritmico. In effetti, il trattamento di variabili sensibili statisticamente rilevanti o di “variabili protette”, come il genere o la razza, è tipicamente limitato o vietato dal diritto antidiscriminatorio e dalla protezione dei dati, al fine di limitare i rischi di sleale discriminazione. Purtroppo, anche se la protezione di specifiche classi può essere codificata in un algoritmo, potrebbero sempre esserci dei pregiudizi (bias) che non sono stati considerati ex ante, come nel caso, per esempio, di modelli linguistici che riproducono testi fortemente maschilisti (Fuster, Goldsmith-Pinkham, Ramadorai et al., 2017; Doshi-Velez, Kim, 2017). Anche se i pregiudizi possono essere previsti e le variabili protette escluse dai dati, i proxy non previsti per queste variabili potrebbero essere comunque usati per ricostruire i pregiudizi, portando a “pregiudizi basati su proxy” che sono difficili da rilevare ed evitare (Fuster, Goldsmith-Pinkham, Ramadorai et al., 2017; Gillis, Spiess, 2019). I pregiudizi relativi al codice postale sono un tipico esempio.

			Allo stesso tempo, potrebbero esserci buone ragioni per fare affidamento su estimatori statisticamente distorti nell’elaborazione algoritmica, poiché possono essere utilizzati per mitigare la distorsione dei dati di addestramento. In questo modo, un tipo di pregiudizio algoritmico problematico è controbilanciato da un altro tipo di pregiudizio algoritmico, ovvero dall’introduzione di un pregiudizio compensatorio nell’interpretazione degli output algoritmici (Danks, London, 2017). Approcci più semplici per mitigare la distorsione nei dati comportano la gestione di algoritmi in diversi contesti e con vari insiemi di dati (Shah, 2018). Anche rendere pubblico un modello, i suoi insiemi di dati e i metadati (sulla provenienza), al fine di consentire un controllo esterno, può contribuire a correggere pregiudizi invisibili o indesiderati (ibidem). Vale anche la pena di osservare che i cosiddetti “dati sintetici”, ossia i dati generati dall’ia, prodotti tramite l’apprendimento per rinforzo o le reti generative avversarie (rga) offrono un’opportunità per affrontare i problemi di distorsione dei dati (Floridi, 2019c; Xu, Yuan, Zhang et al., 2018). La generazione di dati equi con le rga può contribuire a diversificare gli insiemi di dati utilizzati negli algoritmi di visione artificiale (Xu, Yuan, Zhang et al., 2018). Per esempio, la rete generativa avversaria Stylegan2 (Karras, Laine, Aila, 2019) è in grado di produrre immagini di alta qualità di volti umani inesistenti e si è dimostrata particolarmente utile nella creazione di diversi insiemi di dati di volti umani: cosa di cui difettano attualmente molti sistemi algoritmici per il riconoscimento facciale (Obermeyer, Powers, Vogeli et al., 2019; Kortylewski, Egger, Schneider et al., 2019; Harwell, 2020).

			La distorsione indesiderata si verifica anche a causa di un’implementazione impropria di un algoritmo. Consideriamo il pregiudizio del contesto di trasferimento: il pregiudizio problematico che emerge quando un algoritmo che funziona è utilizzato in un nuovo ambiente. Per esempio, se l’algoritmo relativo a un ospedale di ricerca è utilizzato con riferimento a una clinica rurale e assume che per la clinica rurale sia disponibile lo stesso livello di risorse dell’ospedale di ricerca, le decisioni di allocazione delle risorse sanitarie generate dall’algoritmo saranno imprecise e viziate (Danks, London, 2017). Allo stesso modo, Grgić-Hlača e coautori (2018) mettono in guardia nei confronti dei circoli viziosi che si verificano quando gli algoritmi effettuano valutazioni sbagliate della concatenazione. Per esempio, nel contesto dell’algoritmo di valutazione del rischio compas, uno dei criteri di valutazione per prevedere la recidiva è la storia criminale degli amici di un imputato. Ne deriva che avere amici con precedenti penali creerebbe un circolo vizioso in cui un imputato con amici condannati sarà ritenuto più incline a commettere crimini, e quindi sarà condannato alla pena, aumentando così il numero di persone con precedenti penali in un dato gruppo in base a una mera correlazione (Grgić-Hlača, Redmiles, Gummadi et al., 2018; Richardson, Schultz, Crawford, 2019).

			Esempi di rilievo di pregiudizi algoritmici avvenuti di recente, almeno a partire dai resoconti sul sistema compas (Angwin, Larson, Mattu et al., 2016), hanno prodotto una crescente attenzione nei confronti dei problemi di equità algoritmica. La definizione e realizzazione dell’equità algoritmica sono diventate “compiti urgenti nel mondo accademico e industriale” (Shin, Park, 2019), come evidenzia il significativo incremento del numero di articoli, workshop e conferenze dedicati a “equità, responsabilità e trasparenza” (Hoffmann, Roberts, Wolf et al., 2018; Ekstrand, Levy, 2018; Shin, Park, 2019). Analizzerò i temi e i contributi centrali in questo ambito nel paragrafo seguente.





7.6 Risultati ingiusti che portano alla discriminazione


			Esiste un ampio consenso sulla necessità di equità algoritmica, in particolare per mitigare i rischi di discriminazione diretta e indiretta (nel diritto statunitense, si parla rispettivamente di “trattamento discriminatorio” e “impatto discriminatorio”) dovuti a decisioni algoritmiche (Barocas, Selbst, 2016; Grgić- Hlača, Redmiles, Gummadi et al., 2018; Green, Chen, 2019). Tuttavia, non c’è pieno accordo tra gli studiosi sulla definizione, sui criteri di misura e standard dell’equità algoritmica (Gajane, Pechenizkiy, 2018; Saxena, Huang, DeFilippis et al., 2019; Lee, 2018; Milano, Taddeo, Floridi, 2019, 2020). Wong (2019) arriva a individuare circa 21 definizioni di equità in tutta la letteratura: tali definizioni sono spesso reciprocamente incoerenti (Doshi-Velez, Kim, 2017) e non sembra delinearsi un punto di sintesi finale. Il fatto è che ci sono numerose sfumature nella definizione, stima e applicazione di diversi standard di equità algoritmica. Per esempio, l’equità algoritmica può essere definita in relazione sia a gruppi sia a individui (ibidem). Per queste e altre ragioni correlate, di recente hanno acquisito importanza quattro definizioni principali di equità algoritmica (vedi, per esempio, Kleinberg, Mullainathan, Raghavan, 2016; Corbett-Davies, Goel, 2018; Lee, Floridi, 2020):

			1.	anti-classificazione, che fa riferimento a categorie protette, come razza e genere, e i loro proxy utilizzati in modo implicito nel processo decisionale;

			2.	parità di classificazione, che considera un modello equo se le misurazioni comuni delle prestazioni predittive, inclusi i tassi di falsi positivi e negativi, sono uguali tra i gruppi protetti;

			3.	calibrazione, che considera l’equità come una misura di quanto sia ben calibrato un algoritmo tra gruppi protetti;

			4.	parità statistica, che definisce l’equità come una stima uguale di probabilità media relativa a tutti i membri dei gruppi protetti.

			Tuttavia, ciascuna di queste definizioni comunemente utilizzate di equità presenta degli svantaggi; inoltre, sono in genere reciprocamente incompatibili (Kleinberg, Mullainathan, Raghavan, 2016). Prendendo per esempio l’anti-classificazione, le caratteristiche protette, come razza, genere e religione, non possono essere semplicemente rimosse dai dati di addestramento per prevenire la discriminazione, come osservato sopra (Gillis, Spiess, 2019). Le disuguaglianze strutturali significano che punti dati formalmente non discriminatori, come i codici postali, possono fungere da proxy ed essere utilizzati, intenzionalmente o no, per inferire caratteristiche protette, come la razza (Edwards, Veale, 2017). Inoltre, ci sono casi rilevanti in cui è opportuno considerare le caratteristiche protette per prendere decisioni eque. Per esempio, tassi di recidiva femminile più bassi significano che l’esclusione del genere come input negli algoritmi di recidiva comporterebbe per le donne valutazioni di rischio sproporzionatamente elevate (Corbett-Davies, Goel, 2018). Per questo motivo, Binns (2018) sottolinea l’importanza di considerare il contesto storico e sociologico, che non può essere colto nei dati forniti agli algoritmi, ma che può modellare approcci appropriati dal punto di vista contestuale all’equità negli algoritmi. È anche fondamentale osservare che i modelli algoritmici possono spesso produrre risultati inaspettati, contrari alle intuizioni umane, e turbare la loro comprensione. Per esempio, come illustrano Grgić-Hlača e coautori (2018), l’uso di caratteristiche che le persone ritengono eque può in taluni casi accrescere il razzismo mostrato dagli algoritmi e diminuire la precisione.

			Per quanto riguarda i metodi per migliorare l’equità algoritmica, Veale, Binns (2017) e Katell e coautori (2020) offrono due approcci. Il primo prevede l’intervento di una terza parte, vale a dire di un soggetto diverso da colui che fornisce gli algoritmi che disponga di dati su caratteristiche sensibili o protette e tenti di identificare e ridurre le discriminazioni causate dai dati e dai modelli. Il secondo approccio propone un metodo collaborativo basato sulla conoscenza che si concentri su risorse di dati generate dalla comunità che comprendano esperienze pratiche di ml e modellazione. I due approcci non si escludono a vicenda, ma possono portare benefici diversi a seconda dei contesti di applicazione, e anche la loro combinazione può risultare vantaggiosa.

			Dato l’impatto notevole che le decisioni algoritmiche hanno sulla vita delle persone e l’importanza del contesto per la scelta di misure appropriate di equità, sorprende che ci sia stato poco impegno nel cogliere le opinioni pubbliche sull’equità algoritmica (Lee, Kim, Lizarondo, 2017; Saxena et al., 2019; Binns, 2018). Esaminando la percezione pubblica delle diverse definizioni di equità algoritmica, Saxena e coautori (2019, p. 3) osservano che nel contesto delle decisioni relative alla concessione di un mutuo le persone sembrano preferire una “definizione di equità calibrata”, o una selezione basata sul merito, rispetto al “trattamento di persone simili in modo simile” e sostengono argomenti a favore del principio delle azioni positive. In uno studio analogo, Lee (2018) offre prove che suggeriscono che, quando si considerano compiti che richiedono abilità esclusivamente umane, le persone considerano le decisioni algoritmiche meno eque e gli algoritmi meno affidabili.

			Sulla base di lavori empirici condotti su trasparenza e interpretabilità algoritmiche, Webb e coautori (2019) mostrano che i riferimenti morali, in particolare sull’equità, sono coerenti tra i partecipanti che discutono le loro preferenze sugli algoritmi. Lo studio rileva che le persone tendono a oltrepassare le preferenze personali per concentrarsi invece sui “comportamenti giusti o sbagliati”, per indicare, in tal modo, la necessità di comprendere il contesto di implementazione dell’algoritmo nonché la difficoltà di capire l’algoritmo stesso e le sue conseguenze (ibidem). Nel contesto dei sistemi di raccomandazione, Burke (2017) propone un approccio multi-stakeholder e plurilaterale per definire l’equità, andando oltre le definizioni incentrate sugli utenti, al fine di includere gli interessi di altri stakeholder del sistema (vedi anche Milano, Taddeo, Floridi, 2019, 2020).

			È diventato chiaro che comprendere il punto di vista pubblico sull’equità algoritmica aiuterebbe i tecnologi a sviluppare algoritmi con principi di equità che siano più in linea con i sentimenti del pubblico in generale sulle nozioni prevalenti di equità (Saxena et al., 2019, p. 1). Fondare le decisioni di design dei fornitori di un algoritmo “su ragioni accettabili da parte di coloro che sono maggiormente interessati dai loro effetti negativi”, così come essere “disponibili ad adattamenti alla luce di nuove ragioni” (Wong, 2019, p. 15), è fondamentale per migliorare l’impatto sociale degli algoritmi. Tuttavia, è importante rendersi conto che le stime dell’equità sono spesso del tutto inadeguate quando cercano di corroborare modelli applicati a gruppi di persone che sono già svantaggiate nella società a causa della loro origine, livello di reddito o orientamento sessuale. Semplicemente, non si può “ottimizzare al riguardo” di dinamiche di potere economico, sociale e politico esistenti (Winner, 1980; Benjamin, 2019).





7.7 Effetti trasformativi che sollevano sfide per l’autonomia e la privacy informativa


			L’impatto collettivo degli algoritmi ha stimolato discussioni sull’autonomia accordata agli utenti finali (Ananny, Crawford, 2018; Beer, 2017; Möller, Trilling, Helberger et al., 2018; Malhotra, Kotwal, Dalal, 2018; Shin, Park, 2019; Hauer, 2019). I servizi basati su algoritmi sono sempre più presenti “all’interno di un ecosistema di problematiche sociotecniche complesse” (Shin, Park, 2019), che possono ostacolare l’autonomia degli utenti. I limiti all’autonomia degli utenti derivano da tre fonti:

			1.	la distribuzione pervasiva e la proattività degli algoritmi (di apprendimento) nel modellare le scelte degli utenti (Yang et al., 2018; Taddeo, Floridi, 2018a);

			2.	la comprensione limitata degli algoritmi da parte degli utenti;

			3.	la mancanza di potere di secondo ordine (o di appelli) nei confronti dei risultati algoritmici (Rubel, Castro, Pham, 2019).

			Nel considerare le sfide etiche dell’ia, Yang e coautori si concentrano sull’impatto degli algoritmi autonomi e di autoapprendimento sull’autodeterminazione umana e sottolineano che “il potere predittivo e l’incessante spinta gentile (nudging) dell’ia, anche se non intenzionale, dovrebbero favorire e non minare la dignità umana e l’autodeterminazione” (Yang, Bellinghamo, Dupont et al., 2018, p. 11). I rischi che i sistemi algoritmici possano ostacolare l’autonomia umana plasmando le scelte degli utenti sono stati ampiamente documentati in letteratura e hanno assunto un ruolo centrale nella maggior parte dei principi etici di alto livello per l’ia, inclusi, inter alia, quelli del Gruppo europeo sull’etica nella scienza e nelle tecnologie della Commissione europea e il Comitato per l’intelligenza artificiale della Camera dei Lord del Regno Unito (Floridi, Cowls, 2019). In una precedente analisi di questi principi di alto livello (ibidem), ho sottolineato che non è sufficiente che gli algoritmi promuovano l’autonomia delle persone: piuttosto, l’autonomia degli algoritmi dovrebbe essere vincolata e reversibile. Guardando oltre l’Occidente, anche i “Principi di ia di Pechino”, sviluppati da un consorzio di preminenti aziende e università cinesi per guidare la ricerca e lo sviluppo dell’ia, discutono dell’autonomia umana (Roberts, Cowls, Morley et al., 2021).

			L’autonomia umana può anche essere limitata dall’incapacità di un individuo di comprendere alcune informazioni o di prendere le decisioni appropriate. Come suggeriscono Shin e Park, gli algoritmi “non sono conformati in termini tali da consentire agli utenti di comprenderli o di percepire quale sia il modo migliore per utilizzarli, al fine di raggiungere i propri obiettivi” (Shin, Park, 2019, p. 279). Una questione chiave individuata nei dibattiti sull’autonomia degli utenti è la difficoltà di trovare un giusto equilibrio tra il processo decisionale delle persone e quello delegato agli algoritmi (Floridi, Cowls, Beltrametti et al., 2018). Come visto, ciò è ulteriormente complicato dall’assenza di trasparenza relativa al processo decisionale con cui determinate decisioni sono delegate agli algoritmi. Ananny e Crawford (2018) osservano che spesso questo processo non tiene conto di tutte le parti interessate e non è privo di disuguaglianze strutturali.

			Come metodo di ricerca e innovazione responsabile (rir), il “design partecipativo” è di regola menzionato per la sua attenzione al design degli algoritmi diretto a promuovere i valori degli utenti finali e a proteggere la loro autonomia (Whitman, Hsiang, Roark, 2018; Katell, Young, Dailey et al., 2020). Il design partecipativo mira a “includere la conoscenza tacita dei partecipanti e la loro esperienza sul campo” nel processo stesso di design (Whitman, Hsiang, Roark, 2018, p. 2). Per esempio, il quadro concettuale di “società-nel-processo” (Rahwan, 2018) intende consentire a differenti parti interessate nella società di disegnare sistemi algoritmici prima dell’implementazione e di modificare e invertire le decisioni dei sistemi algoritmici che sono già alla base delle attività sociali. Questo quadro mira ad alimentare il corretto funzionamento di un “contratto sociale algoritmico”, definito come “un patto tra le diverse parti interessate umane, mediato da macchine” (ibidem, p. 1). E raggiunge tale scopo identificando e negoziando i valori delle diverse parti interessate dai sistemi algoritmici come base per monitorare l’adesione al contratto sociale.

			La privacy informativa è intimamente legata all’autonomia degli utenti (Cohen, 2000; Rössler, 2015). La privacy informativa garantisce la libertà degli individui di pensare, comunicare e formare relazioni, tra le altre attività umane essenziali (Rachels, 1975; Allen, 2011). Tuttavia, la crescente interazione degli individui con i sistemi algoritmici ha effettivamente ridotto la loro capacità di controllare chi ha accesso alle informazioni che li riguardano e che cosa viene fatto con tali informazioni. Pertanto, le grandi quantità di dati sensibili richiesti nella profilazione e nelle previsioni algoritmiche, fondamentali per i sistemi di raccomandazione, sollevano molteplici problemi al riguardo della privacy informativa degli individui. La profilazione algoritmica avviene nel corso di un periodo di tempo indefinito, in cui gli individui sono categorizzati secondo la logica interna di un sistema, e i loro profili vengono aggiornati man mano che si acquisiscono nuove informazioni su di loro. Queste informazioni sono di regola ottenute direttamente, quando una persona interagisce con un dato sistema, o indirettamente, quando sono dedotte da gruppi di individui assemblati algoritmicamente (Paraschakis, 2018). In effetti, la profilazione algoritmica si basa anche su informazioni raccolte su altri individui e gruppi di persone che sono stati classificati in modo simile alla persona oggetto della profilazione. Ciò include informazioni che vanno da caratteristiche come la posizione geografica e l’età a informazioni su determinati comportamenti e preferenze, incluso il tipo di contenuto che è probabile che una persona ricerchi maggiormente su una determinata piattaforma (Chakraborty, Patro, Ganguly et al., 2019). Sebbene ciò ponga un problema di prove inconcludenti, indica anche che, se non viene assicurata la privacy di gruppo (Floridi, 2014c; Taylor, Floridi, van der Sloot, 2016), può risultare impossibile per gli individui sottrarsi al processo di profilazione e predizione algoritmiche (Milano, Taddeo, Floridi, 2019, 2020). In altri termini, la privacy informativa degli individui non può essere protetta senza garantire al contempo la privacy di gruppo.

			Gli utenti potrebbero non essere sempre a conoscenza, o avere la capacità di acquisire consapevolezza, del tipo di informazioni che sono detenute al loro riguardo e dell’utilizzo che di tali informazioni viene fatto. Dato che i sistemi di raccomandazione contribuiscono alla costruzione dinamica dell’identità degli individui intervenendo nelle loro scelte, l’assenza di controllo sulle proprie informazioni si traduce in una perdita di autonomia. Conferire agli individui la possibilità di contribuire al design di un sistema di raccomandazione può contribuire a creare profili più accurati che tengano conto di attributi e categorie sociali che altrimenti non sarebbero stati inclusi nella classificazione utilizzata dal sistema per analizzare gli utenti (Milano, Taddeo, Floridi, 2019, 2020). Sebbene l’opportunità di migliorare la profilazione algoritmica varierà con il contesto, l’affinamento del design algoritmico attraverso i feedback delle varie parti interessate dall’algoritmo è in linea con il già menzionato contributo teorico della rir e accresce la capacità degli utenti di autodeterminarsi (Whitman, Hsiang, Roark, 2018).

			Sapere chi possiede i nostri dati e che cosa ne fa può contribuire anche a modellare i trade-off tra la privacy informativa e i vantaggi derivanti dal trattamento delle informazioni (Sloan, Warner, 2018, p. 21). Per esempio, in contesti medici, è probabile che gli individui siano più disposti a condividere informazioni che possono contribuire a formare la propria diagnosi, o quella altrui, di quanto lo siano nel contesto della selezione del lavoro. Le norme di coordinamento delle informazioni, come sostengono Sloan e Warner (ibidem), possono far sì che questi trade-off si adattino correttamente a contesti diversi, senza imporre responsabilità e oneri eccessivi sui singoli individui. Per esempio, le informazioni personali dovrebbero circolare in modo diverso nel contesto delle procedure di applicazione del diritto rispetto a quello dei processi di selezione del lavoro. Il Regolamento generale sulla protezione dei dati (gdpr) dell’Unione Europea ha svolto un ruolo importante nel dare fondamento a tali norme (ibidem).

			Infine, un sapere crescente in tema di privacy differenziale sta fornendo nuovi metodi di protezione della privacy per le organizzazioni che cercano di proteggere la privacy dei propri utenti pur mantenendo una buona qualità del modello, nonché costi e complessità del software gestibili, trovando un equilibrio tra utilità e privacy (Abadi, Chu, Goodfellow et al., 2016; Wang, Jiang, Singh et al., 2017; Xian, Li, Huang et al., 2017). Progressi tecnici di questo tipo consentono alle organizzazioni di condividere pubblicamente un insieme di dati mantenendo segrete le informazioni sugli individui (impedendo la reidentificazione) e possono dimostrare di assicurare la protezione della privacy relativa a dati sensibili, come i dati genomici (Wang, Jiang, Singh et al., 2017). In effetti, la privacy differenziale è stata recentemente utilizzata da Social Science One e Facebook per rilasciare in sicurezza uno dei più grandi insiemi di dati (38 milioni di url condivisi pubblicamente su Facebook) per la ricerca accademica sugli impatti sociali dei social media (King, Persily, 2020).





7.8 Tracciabilità come presupposto della responsabilità morale


			Le limitazioni tecniche di vari algoritmi di ml, come la mancanza di trasparenza o di spiegabilità, minano la possibilità di sottoporli a esame ed evidenziano la necessità di nuovi approcci per tracciare la responsabilità morale e per rendere conto delle azioni poste in essere dagli algoritmi di ml. Per ciò che concerne la responsabilità morale, Reddy, Cakici e Ballestero (2019) rilevano una ricorrente confusione tra i limiti tecnici degli algoritmi e i più ampi limiti giuridici, etici e istituzionali in cui operano. Anche per gli algoritmi diversi da quelli di apprendimento le concezioni tradizionali e lineari di responsabilità offrono in realtà limitate indicazioni nei contesti sociotecnici contemporanei. Strutture sociotecniche più ampie rendono difficile attribuire la responsabilità per le azioni poste in essere da sistemi distribuiti e ibridi di agenti umani e artificiali (Floridi, 2012b; Crain, 2018).

			Inoltre, a causa della struttura e del funzionamento del mercato dell’intermediazione dei dati, è in molti casi impossibile “ricondurre un determinato dato alla sua fonte originale” una volta che sia stato introdotto sul mercato (Crain, 2018, p. 93). Le ragioni di ciò includono la protezione del segreto commerciale; mercati complessi che “separano” il processo di raccolta da quello di vendita e acquisto dei dati; e la commistione di grandi volumi di informazioni generate in modo computazionale, prive di una “‘reale’ fonte empirica”, combinate con dati autentici (ibidem, p. 94). La complessità tecnica e il dinamismo degli algoritmi di ml li rendono inclini a questioni di “riciclaggio dell’agire”: un errore morale che consiste nel prendere le distanze da azioni moralmente sospette, indipendentemente dal fatto che tali azioni siano o no intenzionali, dando la colpa all’algoritmo (Rubel, Castro, Pham, 2019). Questa pratica è attuata sia da organizzazioni sia da individui. Rubel e coautori forniscono un esempio diretto, e che fa venire i brividi, di riciclaggio dell’agire da parte di Facebook:

			Utilizzando il sistema automatizzato di Facebook, il team di ProPublica ha individuato una categoria generata dagli utenti chiamata “Jew hater” (“odiatore di ebrei”) con oltre 2200 membri. […] Per aiutare ProPublica a conseguire un pubblico più ampio (e quindi ad acquistare un annuncio migliore), Facebook ha suggerito una serie di categorie aggiuntive. […] ProPublica ha utilizzato la piattaforma per selezionare altri profili che mostravano categorie antisemite e Facebook ha approvato l’annuncio di ProPublica con piccole modifiche. Quando ProPublica ha rivelato le categorie antisemite e altre testate giornalistiche hanno riportato categorie altrettanto odiose, Facebook ha risposto chiarendo che gli algoritmi avevano creato le categorie in base alle risposte degli utenti negli ambiti ricercati [e che] “[Facebook] non aveva mai inteso o previsto che questa funzionalità fosse utilizzata in tal modo”. (Ibidem, pp. 1024-1025)

			Oggi, l’incapacità di cogliere gli effetti inintenzionali del trattamento e della commercializzazione di massa dei dati personali, un problema già noto nella storia della tecnologia (Wiener, 1960; Klee, 1996; Benjamin, 2019), si abbina alle spiegazioni limitate fornite dalla maggior parte degli algoritmi di ml. Questo approccio rischia di favorire l’elusione delle responsabilità attraverso la tipica forma di giustificazione: “Il computer ha detto così” (Karppi, 2018). Ciò può portare gli esperti sul campo, come i clinici, a evitare di mettere in discussione il suggerimento di un algoritmo anche quando può apparire loro strano (Watson, Krutzinna, Bruce et al., 2019). L’interazione tra esperti del settore e algoritmi di ml può stimolare “vizi epistemici” (Grote, Berens, 2020), come il dogmatismo o la creduloneria (Hauer, 2019), e ostacolare l’attribuzione di responsabilità nei sistemi distribuiti (Floridi, 2016a). Per affrontare questo problema, l’analisi di Shah (2018) sottolinea che il rischio che alcune parti interessate possano violare le proprie responsabilità può essere affrontato, per esempio, istituendo organismi separati per la supervisione etica degli algoritmi, come DeepMind Health, che aveva istituito un comitato di revisione indipendente con accesso illimitato all’azienda finché Google non l’ha dismesso nel 2019 (Murgia, 2018). Tuttavia, aspettarsi che un singolo ente di supervisione, come un comitato etico della ricerca o un comitato di revisione istituzionale, “sia il solo responsabile di garantire il rigore, l’utilità e l’integrità dei big data” non è realistico (Lipworth, Mason, Kerridge et al., 2017, p. 8). In effetti, alcuni hanno sostenuto che queste iniziative mancano di qualsiasi tipo di coerenza e possono piuttosto alimentare il “bluewashing etico”, che ho definito nel quinto capitolo come l’attuazione di “misure superficiali a favore dei valori etici e dei benefici di processi, prodotti, servizi o altre soluzioni digitali al fine di apparire più etici dal punto di vista digitale di quanto non si sia effettivamente”. Posti a confronto con regimi giuridici severi, gli attori dotati di risorse possono anche sfruttare il cosiddetto “dumping etico” mediante il quale “processi, prodotti o servizi” non etici vengono esportati in paesi con quadri giuridici e meccanismi di applicazione delle norme più lassisti, dopodiché i risultati di tali attività non etiche vengono “re-importati”.

			Esistono numerosi approcci dettagliati per stabilire l’accountability algoritmica. Sebbene gli algoritmi di ml richiedano un livello di intervento tecnico per migliorare la loro spiegabilità, la maggior parte degli approcci si concentra su interventi normativi. Per esempio, Ananny e Crawford (2018) sostengono che i fornitori di algoritmi dovrebbero, almeno, favorire il dibattito pubblico sulla loro tecnologia. Allo stesso modo, per affrontare la questione delle azioni etiche ad hoc, alcuni hanno affermato che l’accountability dovrebbe essere affrontata prima di tutto come una questione di accordo (Dignum, Baldoni, Baroglio et al., 2018; Reddy, Cakici, Ballestero, 2019). Per colmare la “mancanza” di accordo (Buhmann, Paßmann, Fieseler, 2019) prendono in prestito i sette principi per gli algoritmi stabiliti dalla Association for Computing Machinery, sostenendo che tramite, inter alia, la conoscenza dei propri algoritmi, processi di validazione e test, un’organizzazione dovrebbe assumersi la responsabilità dei propri algoritmi indipendentemente da quanto siano opachi (Malhotra, Kotwal, Dalal, 2018). Le decisioni relative all’implementazione degli algoritmi dovrebbero incorporare fattori come la loro desiderabilità e la considerazione del contesto più ampio in cui opereranno, che dovrebbero in tal modo generare una “cultura algoritmica” più responsabile (Vedder, Naudts, 2017, p. 219). Per cogliere il senso di tali considerazioni, “ambiti e processi interattivi e discorsivi” con le parti interessate rilevanti, come suggerito da Buhmann e coautori, possono rivelarsi un mezzo utile (Buhmann, Paßmann, Fieseler, 2019, p. 13).

			I problemi relativi al “riciclaggio dell’agire” e all’“elusione etica” derivano dall’inadeguatezza delle cornici concettuali esistenti nel tracciare e attribuire la responsabilità morale. Come ho rimarcato in passato, quando si considerano i sistemi algoritmici e l’impatto delle loro azioni

			abbiamo a che fare con amd [azioni morali distribuite] derivanti da interazioni moralmente neutre di reti di agenti (potenzialmente ibride)? In altre parole, chi è responsabile (responsabilità morale distribuita, rmd) delle amd? (Floridi, 2016a, p. 2)

			Ho suggerito di attribuire la piena responsabilità morale “per impostazione predefinita e in modo reversibile” a tutti gli agenti morali (per esempio, umani o costituiti da esseri umani, come le aziende) nella rete che sono causalmente rilevanti per una data azione della rete (Floridi, 2016a). L’approccio proposto si basa sui concetti di retropropagazione della teoria delle reti, di responsabilità oggettiva nel diritto e di conoscenza comune della logica epistemica. In particolare, questo approccio disgiunge la responsabilità morale dall’intenzionalità degli attori e dall’idea stessa di sanzione o ricompensa per l’esecuzione di una data azione, per concentrarsi invece sulla necessità di correggere gli errori (retropropagazione) e di migliorare il comportamento etico di tutti gli agenti in rete, sulla falsariga della cosiddetta responsabilità oggettiva in ambito giuridico.





7.9 Conclusione: l’uso buono e cattivo degli algoritmi


			Questo capitolo si basa, aggiornandola, sulla ricerca che abbiamo precedentemente svolto presso il Digital Ethics Lab (Mittelstadt, Allo, Taddeo et al., 2016), per esaminare la letteratura rilevante pubblicata dal 2016 in tema di etica degli algoritmi. Quel lavoro è ora inevitabilmente obsoleto in termini di riferimenti specifici e informazioni dettagliate sulla letteratura esaminata. Tuttavia, la mappa e le sei categorie che abbiamo elaborato sembrano aver resistito alla prova del tempo e rimangono uno strumento prezioso per delimitare l’ambito di ricerca dell’etica degli algoritmi, con un corpus crescente di letteratura incentrato su ciascuna delle sei categorie che contribuiscono ad affinare la nostra comprensione dei problemi esistenti o a fornire soluzioni per affrontarli.

			Dal 2016, l’etica degli algoritmi è diventata un tema centrale di discussione tra studiosi, fornitori di tecnologia e decisori politici. Il dibattito ha preso piede anche a causa della cosiddetta “estate dell’ia” (vedi capitolo 1), e con essa l’uso diffuso degli algoritmi di ml. Molte delle questioni etiche analizzate in questo capitolo e la letteratura che esamina sono state affrontate in linee guida e principi etici nazionali e internazionali (vedi capitoli 4 e 6), prodotti da organismi come il Gruppo europeo sull’etica della scienza e delle nuove tecnologie della Commissione europea, il Comitato per l’intelligenza artificiale della Camera dei Lord del Regno Unito (Floridi, Cowls, 2019), il Gruppo di esperti di alto livello sull’ia istituito dalla Commissione europea e l’ocse (ocse, 2019).

			Un aspetto che non è stato esplicitamente colto dalla mappa originale, e che sta diventando un tema centrale di discussione nella letteratura rilevante, è l’attenzione crescente riservata all’uso di algoritmi, ia e più in generale di tecnologie digitali, per produrre risultati socialmente buoni (Hager, Drobnis, Fang et al., 2019; Cowls, King, Taddeo et al., 2019; Cowls, Tsamados, Taddeo et al., 2021b). Ciò formerà oggetto dell’ottavo capitolo. Se è vero, almeno in linea di principio, che qualsiasi iniziativa volta a servirsi degli algoritmi per il bene sociale dovrebbe affrontare in modo soddisfacente i rischi identificati da ciascuna delle sei categorie nella mappa e quelli esaminati nel quinto capitolo, esiste anche un dibattito sempre più ampio sui principi e i criteri che dovrebbero informare il design e la governance degli algoritmi, e più in generale delle tecnologie digitali, per perseguire apertamente il bene sociale, come visto nel quarto capitolo. Occorrono analisi etiche per mitigare i rischi e sfruttare al contempo il potenziale di queste tecnologie per il bene, nella misura in cui tali analisi servono al duplice scopo di chiarire la natura dei rischi etici e del potenziale di algoritmi e tecnologie digitali per il bene e di tradurre (Taddeo, Floridi, 2018a; Morley, Floridi, Kinsey et al., 2020) tale comprensione in indicazioni valide e attuabili per la governance del design e dell’utilizzo di artefatti digitali. Prima di esaminare questi aspetti più positivi, dobbiamo comprendere in dettaglio, nel prossimo capitolo, come l’ia può essere usata per scopi malvagi e illeciti.



* * *





			 				 					1. https://www.techregister.co.uk/ruha-benjamin-on-deep-learning-computational-depth-senza-sociological-depth-is-superficial-learning/.





8


			Cattive pratiche: l’uso improprio dell’ia per il male sociale

			Sommario In precedenza, nel settimo capitolo, ho fornito una panoramica delle varie sfide etiche sollevate dall’uso diffuso degli algoritmi. In questo capitolo mi concentro sul lato negativo dell’impatto dell’ia (il lato positivo sarà discusso nel nono capitolo). La ricerca e la regolazione che concernono l’ia cercano di trovare un bilanciamento tra i vantaggi dell’innovazione e il rischio di potenziali danni o malfunzionamenti. Tuttavia, una conseguenza indesiderata del recente aumento della ricerca sull’ia risiede nella possibilità di riorientare le tecnologie dell’ia per agevolare atti criminali, che chiamerò qui crimini di ia (cia). Sappiamo che i cia sono teoricamente fattibili grazie agli esperimenti pubblicati sull’automazione delle frodi mirate agli utenti dei social media, nonché alle dimostrazioni di manipolazione condotta dall’ia di mercati simulati. Tuttavia, poiché i cia costituiscono un settore ancora relativamente giovane e intrinsecamente interdisciplinare, che spazia dagli studi socio-giuridici alla scienza formale, è tuttora incerto come potrebbe essere il futuro dei cia. In questo capitolo analizzo le prevedibili minacce dei cia, per offrire una sintesi dei problemi attuali e delineare un possibile ambito di soluzione.





8.1 Introduzione: l’uso criminale dell’ia


			L’ia può svolgere un ruolo sempre più essenziale negli atti criminali in futuro. Questa affermazione esige tre precisazioni. Innanzitutto, scrivo “essenziale” (invece di “necessario”) perché, sebbene vi sia una possibilità logica che i reati di cui parlerò in seguito possano verificarsi senza il supporto dell’ia, tale possibilità è trascurabile. Ciò significa che i crimini in questione probabilmente non si sarebbero verificati se non grazie all’uso dell’ia. La distinzione può essere chiarita con un esempio. Si potrebbe considerare il trasporto essenziale per viaggiare tra Parigi e Roma, ma non necessario, perché si potrebbe sempre camminare; per questo il trasporto non è, in senso stretto, necessario. Il secondo chiarimento è che i crimini di ia, come definiti in questo capitolo, coinvolgono l’ia come un fattore che contribuisce, ma non come un fattore investigativo, esecutivo o attenuante. In terzo luogo, in questo capitolo gli “atti criminali” sono definiti come qualsiasi atto (o omissione) che costituisce un reato punibile ai sensi del diritto penale inglese, in maniera idealmente riferibile alle giurisdizioni che definiscono in modo analogo tale crimine. In altre parole, la scelta del diritto penale inglese è dovuta soltanto alla necessità di radicare l’analisi in un quadro di riferimento concreto e pratico sufficientemente generalizzabile. L’analisi e le conclusioni del capitolo sono facilmente esportabili in altri ordinamenti.

			Esempi chiari di ciò che chiamerò “crimini di ia” (cia) sono forniti da due esperimenti di ricerca (teorici). Nel primo, due scienziati sociali computazionali (Seymour, Tully, 2016) hanno usato l’ia come strumento per convincere utenti di social media a cliccare su collegamenti di phishing all’interno di messaggi prodotti in serie. Poiché ogni messaggio è stato costruito con tecniche di ml applicate ai comportamenti passati e ai profili pubblici degli utenti, il contenuto è stato ritagliato su ciascun individuo, camuffando in tal modo l’intenzione alla base di ogni messaggio. Se la potenziale vittima avesse cliccato sul link di phishing e avesse compilato il successivo modulo web, allora, in circostanze reali, un criminale avrebbe ottenuto informazioni personali e private che avrebbero potuto essere usate per furti e frodi. La criminalità alimentata dall’ia può avere anche un impatto sul commercio. Nel secondo esperimento, tre scienziati informatici (Martínez-Miranda, McBurney, Howard, 2016; ma si veda anche il precedente McBurney, Howard, 2015) hanno simulato un mercato e hanno scoperto che gli agenti di scambio potevano apprendere ed eseguire una “vantaggiosa” campagna di manipolazione del mercato che includeva una serie di falsi ordini ingannevoli. Questi due primi esperimenti mostrano che l’ia fornisce una minaccia attuabile e fondamentalmente nuova, sotto forma di cia.

			L’importanza dei cia come fenomeno distinto, connesso ma differente dai crimini informatici, non è stata ancora pienamente riconosciuta ma sta emergendo (Broadhurst, Maxim, Brown et al., 2019; Lagioia, Sartor, 2019; Caldwell, Andrews,Tanay et al., 2020; Dremliuga, Prisekina, 2020; Hayward, Maas, 2020; Sibai, 2020). La prima letteratura sulle implicazioni etiche e sociali dell’ia si è concentrata sulla regolamentazione e il controllo degli usi civili dell’ia, piuttosto che considerare il suo possibile ruolo in ambito criminale (Kerr, Bornfreund, 2005). Inoltre, non è di aiuto il fatto che la ricerca disponibile sui cia sia sparpagliata tra discipline diverse, tra cui studi socio-giuridici, informatica, psicologia e robotica, solo per citarne alcune.

			Per fare un po’ di chiarezza sull’attuale conoscenza e comprensione dei cia, questo capitolo offre un’analisi sistematica e completa del relativo dibattito interdisciplinare. L’obiettivo è supportare un’analisi di previsione normativa più chiara e coesa, che contribuisca a individuare i cia come oggetto di interesse di studi futuri. L’analisi risponde a due domande principali:

			1.	Quali sono le minacce fondamentalmente peculiari e plausibili poste dai cia?

			Questa è la prima domanda a cui è necessario rispondere se si vogliono disegnare politiche preventive, mitiganti o correttive. La risposta a questa domanda identifica le potenziali aree dei cia secondo la letteratura e le preoccupazioni più generali che attraversano tali aree. Di qui segue naturalmente la seconda domanda:

			2.	Quali soluzioni sono disponibili o possono essere elaborate per affrontare i cia?

			In questo caso, ricostruisco le soluzioni tecnologiche e giuridiche esistenti suggerite finora nella letteratura accademica e discuto le ulteriori sfide che hanno di fronte.

			Dato che stiamo affrontando queste domande per supportare l’analisi di previsione normativa, in questo capitolo mi concentrerò solo sulle preoccupazioni realistiche e plausibili che concernono i cia. Non mi interesso a speculazioni non sostenute da conoscenze scientifiche o prove empiriche. Di conseguenza, l’analisi si basa sulla definizione classica di ia, fornita da McCarthy, Minsky, Rochester e Shannon nella loro seminale “Proposta per il progetto estivo di ricerca sull’intelligenza artificiale di Dartmouth”, che ho introdotto nel primo capitolo e discusso nel secondo, ma che può essere utile richiamare qui brevemente. Tale definizione identifica nelle applicazioni di ia una risorsa crescente di capacità di agire interattivo, autonomo e di autoapprendimento, per affrontare compiti che richiederebbero altrimenti l’intelligenza e l’intervento umani per essere eseguiti con successo. Tali agenti artificiali (aa) sono (come ho osservato in Floridi, Sanders, 2004)

			sufficientemente informati, “smart”, autonomi e in grado di compiere azioni moralmente rilevanti indipendentemente dagli esseri umani che li hanno creati […].

			Questa combinazione di autonomia e capacità di apprendimento è alla base (come discusso da Yang, Bellingham, Dupont et al., 2018) sia degli usi benefici sia di quelli dannosi dell’ia.1 Come nel resto del libro, tratterò dunque l’ia come una riserva di capacità di agire smart sempre disponibile. Purtroppo, tale riserva di capacità di agire può essere talvolta impiegata impropriamente a scopi criminali: quando questo accade, siamo di fronte a ciò che ho definito cia.

			Il resto del capitolo è organizzato come segue. Nei paragrafi 2 e 3, affronto la prima domanda concentrandomi sulle preoccupazioni di nuovo genere sollevate da diverse aree dei cia e mappo ciascuna area in relazione alle minacce rilevanti che le attraversano, offrendo una prima descrizione degli “studi relativi ai cia”. Nel paragrafo 4, affronto la seconda domanda, analizzando l’ampia gamma di soluzioni prospettate in letteratura per ciascuna minaccia trasversale. Infine, nel paragrafo 5, discuto le lacune più urgenti nella nostra comprensione del fenomeno, ciò che potremmo chiamare le nostre “incognite conosciute”, per ciò che concerne il compito di ridurre l’attuale incertezza sui cia. Il capitolo si conclude con un’osservazione che lo collega al capitolo successivo, dedicato alle applicazioni di ia socialmente buone.





8.2 Preoccupazioni


			L’analisi iniziale che abbiamo svolto presso il Digital Ethics Lab (King, Aggarwal, Taddeo et al., 2019) si è basata su una revisione sistematica della letteratura e si è concentrata sul ruolo strumentale che l’ia può svolgere in ciascuna area criminale identificata da Archbold (1991), che è il testo di riferimento per chi svolge la professione legale in ambito penale nel Regno Unito.2 Tale analisi ha filtrato i risultati relativi ad atti o omissioni criminali che:

			–	si sono verificati o probabilmente si verificheranno in base alle attuali tecnologie di ia (plausibilità);

			–	richiedono l’ia come fattore essenziale (unicità);3

			–	sono perseguiti nel diritto nazionale (i crimini internazionali come, per esempio, quelli legati alla guerra, sono stati esclusi).

			Tabella 8.1 Mappa delle minacce settoriali e trasversali, basata sulla revisione della letteratura.

			 				 					 					 					 					 					 				 				 					 						 							Aree criminali

						 						 							Motivi di preoccupazione



					 						 							Emergenza

						 						 							Responsabilità

						 						 							Monitoraggio

						 						 							Psicologia



					 						 							Commercio, mercati finanziari e insolvenza

						 						 							✓

						 						 							✓

						 						 							✓



					 						 							Droghe nocive o pericolose

						 						 						 						 							✓

						 						 							✓



					 						 							Reati contro la persona

						 						 							✓

						 						 							✓



					 						 							Reati sessuali

						 						 						 						 						 							✓



					 						 							Furto e frode, contraffazione e sostituzione di persona

						 						 						 						 							✓



				 			 			Ciò ha portato a individuare cinque aree criminali potenzialmente interessate dai cia (di cui si parlerà nel prossimo paragrafo dedicato alle “minacce”):

			1.	commercio, mercati finanziari e insolvenza (incluse compravendite, fallimenti);

			2.	droghe nocive o pericolose (compresi beni illeciti);

			3.	reati contro la persona (inclusi omicidio doloso o colposo, molestie, stalking, tortura);

			4.	reati sessuali (compresi stupro, aggressione sessuale);

			5.	furto e frode, contraffazione e sostituzione di persona.

			Le seguenti nove aree criminali non hanno restituito risultati significativi in letteratura: danneggiamento e reati affini; armi da fuoco o di offesa; reati contro la Corona e il governo; riciclaggio di denaro; reati contro la giustizia pubblica, l’ordine pubblico, la morale pubblica; reati automobilistici; cospirazione per commettere un crimine. Si noti che il riciclaggio di denaro è un’area in cui l’ia è utilizzata per combattere la criminalità, ma è anche un’area in cui i cia possono facilmente svilupparsi.

			Inoltre, le minacce plausibili e peculiari che circondano i cia possono essere comprese in modo specifico o generale. Le minacce più generali rappresentano ciò che rende possibile i cia rispetto ai reati del passato, ovvero le opportunità specifiche dell’ia, e peculiarmente problematiche, ovvero quelle che giustificano la concettualizzazione dei cia come fenomeno criminale distinto. Ciò ha portato all’identificazione di quattro ragioni principali di preoccupazione (che saranno chiarite di seguito): emergenza, responsabilità, monitoraggio, psicologia. La Tabella 8.1 mostra le sovrapposizioni tra preoccupazioni e aree criminali.4

			8.2.1 Emergenza

			La preoccupazione per l’emergenza si riferisce al fatto che un’analisi superficiale del design e dell’implementazione di un agente artificiale (aa) potrebbe suggerire un tipo particolare di comportamento relativamente semplice, ma la verità è che, a seguito dell’implementazione, l’aa può agire in modi potenzialmente più sofisticati che vanno oltre le nostre aspettative iniziali. Pertanto, azioni e piani coordinati possono emergere autonomamente, per esempio derivare da tecniche di ml applicate all’interazione ordinaria fra agenti in un sistema multiagente (mas). In alcuni casi, un designer può promuovere l’emergenza come proprietà che garantisce che soluzioni specifiche vengano scoperte in fase di esecuzione in base a obiettivi generali formulati in fase di progettazione. Un esempio è fornito da uno sciame di robot che sviluppano modi per coordinare il raggruppamento di rifiuti in base a regole semplici (Gauci, Chen, Li et al., 2014). Tale design relativamente semplice che porta a un comportamento più complesso è un’aspirazione fondamentale dei mas (Hildebrandt, 2008). In altri casi, un designer potrebbe voler prevenire l’emergenza, come nel caso in cui un agente autonomo di trading inavvertitamente si coordini e colluda con altri agenti di trading per perseguire un obiettivo condiviso (Martínez-Miranda, McBurney, Howard, 2016). Possiamo osservare in tal caso che il comportamento emergente potrebbe avere implicazioni criminali, nella misura in cui devia dal design originale. Come hanno affermato Alaieri e Vellino (2016):

			imprevedibilità e autonomia possono conferire un grado maggiore di responsabilità alla macchina ma possono anche renderla meno affidabile.

			8.2.2 Responsabilità

			La preoccupazione relativa alla responsabilità si riferisce al fatto che i cia potrebbero minare i modelli di responsabilità esistenti, minacciando così il potere dissuasivo e riparatore della legge.

			Gli attuali modelli di responsabilità potrebbero rivelarsi inadeguati per affrontare il ruolo futuro dell’ia nelle attività criminali. I limiti dei modelli di responsabilità possono quindi minare la certezza del diritto, in quanto può accadere che gli agenti, artificiali o no, compiano atti o omissioni criminali senza che ricorrano le condizioni di responsabilità affinché un determinato comportamento costituisca una determinata fattispecie di reato. La prima condizione per la responsabilità penale è l’actus reus: un atto o un’omissione criminale posta in essere volontariamente. Per le tipologie di cia definiti in modo tale che solo l’aa può realizzare l’atto o l’omissione criminale, l’aspetto volontario dell’actus reus potrebbe non essere mai soddisfatto poiché l’idea che un aa possa agire volontariamente è priva di fondamento:

			La condotta vietata da un dato reato deve essere compiuta volontariamente. Ciò che questo significa in realtà è qualcosa su cui deve ancora formarsi un consenso, poiché concetti come coscienza, volontà, volontarietà e controllo sono spesso confusi e dispersi tra argomenti di filosofia, psicologia e neurologia. (Freitas, Andrade, Novais, 2013, p. 9)

			Quando la responsabilità penale è basata sulla colpa, ha anche una seconda condizione, la mens rea (una mente colpevole), di cui esistono molti diversi tipi e soglie (di stato mentale) applicate a diversi reati. Nel contesto dei cia, la mens rea può comprendere l’intenzione di commettere l’actus reus utilizzando un’applicazione basata sull’ia (soglia di intenzione) o la conoscenza che l’impiego di un aa causerà o potrebbe causare la commissione di un’azione o un’omissione criminale (soglia di conoscenza).

			Per quanto riguarda la soglia di intenzione, se si ammette che un aa può compiere l’actus reus, in quelle tipologie di cia in cui l’intenzione costituisce (in parte) la mens rea, una maggiore autonomia dell’aa incrementa la possibilità che l’atto o l’omissione criminale sia disgiunto dallo stato mentale (intenzione di commettere l’atto o l’omissione):

			I robot autonomi [e gli aa] hanno la capacità unica di scindere un atto criminale, in cui un umano manifesta la mens rea e il robot [o l’aa] commette l’actus reus. (McAllister, 2016, p. 47)

			In base a una distinzione introdotta in Floridi, Sanders (2004), un agente artificiale può essere responsabile causalmente di un atto criminale, ma soltanto un agente umano può esserne moralmente responsabile.

			Per quanto riguarda la soglia di conoscenza, in alcuni casi la mens rea potrebbe addirittura mancare del tutto. La potenziale assenza di una mens rea basata sulla conoscenza è dovuta al fatto che, anche se sappiamo che un aa può eseguire l’actus reus in modo autonomo, la complessità della programmazione dell’aa fa sì che chi disegna, sviluppa o implementa (per esempio un agente umano) tale aa non conoscerà né predirà l’atto o l’omissione criminale dell’aa. Ne deriva che la complessità dell’ia

			fornisce un grande incentivo agli agenti umani per evitare di scoprire cosa sta facendo esattamente il sistema di ml, poiché meno gli agenti umani sanno, più saranno in grado di negare la responsabilità per entrambi questi motivi. (Williams, 2017, p. 25)

			In alternativa, i legislatori possono definire la responsabilità penale senza un requisito di colpa. Tale responsabilità senza colpa (Floridi, 2016a), che è sempre più utilizzata per la responsabilità da prodotto nel diritto della responsabilità civile (per esempio prodotti farmaceutici e beni di consumo), porterebbe ad attribuire la responsabilità alla persona giuridica senza colpa che ha attivato un aa nonostante il rischio che possa plausibilmente compiere un’azione o un’omissione criminale. Tali atti privi di colpa possono coinvolgere molti agenti umani che contribuiscono al crimine prima facie, per esempio attraverso la programmazione o l’implementazione di un aa. La determinazione di chi è responsabile può quindi essere basata sull’approccio della responsabilità senza colpa per le azioni morali distribuite (ibidem). In questo contesto distribuito, la responsabilità si applica agli agenti che fanno la differenza in un sistema complesso in cui i singoli agenti svolgono azioni neutrali che però sfociano in un crimine collettivo. Tuttavia, alcuni sostengono che la mens rea con intenzione o conoscenza

			è fondamentale per la vocazione del diritto penale a reprimere (Ashworth, 2010) e non possiamo semplicemente abbandonare quel requisito chiave [un requisito chiave comune] di responsabilità penale di fronte alla difficoltà di provarlo. (Williams, 2017, p. 25)

			Il problema è che, se la mens rea non è del tutto abbandonata e la soglia è solo abbassata, allora, per ragioni di bilanciamento, la pena può risultare troppo leggera (la vittima non è adeguatamente risarcita) e tuttavia al contempo sproporzionata (è stata proprio colpa dell’imputato?) in caso di reati gravi, come quelli contro la persona (McAllister, 2016).

			8.2.3 Monitoraggio

			La preoccupazione per il monitoraggio dei cia fa riferimento a tre tipi di problemi: attribuzione, fattibilità e azioni intersistemiche. Esaminiamoli separatamente.

			L’attribuzione del mancato rispetto della normativa vigente costituisce un problema di monitoraggio degli aa utilizzati come strumenti di reato, dovuta alla capacità di questa nuova tipologia di agenti smart di operare in modo indipendente e autonomo: due caratteristiche che tendono a confondere ogni tentativo di tracciare la responsabilità riconducendo gli effetti di un’azione all’autore del reato.

			Per quanto riguarda la fattibilità del monitoraggio, l’autore di un reato può trarre vantaggio dai casi in cui gli aa operano a velocità e livelli di complessità che vanno semplicemente al di là della capacità di monitorarne la conformità con le norme. Un caso esemplare è fornito dagli aa che si integrano in sistemi misti umani e artificiali in modi difficili da rilevare, come i bot dei social media. I siti di social media possono assumere esperti per identificare e bloccare bot dannosi (per esempio, nessun bot di social media è attualmente in grado di superare il test di Turing) (Floridi, Taddeo, Turilli, 2009; Wang, Mohanlal, Wilson et al., 2012; Neufeld, Finnestad, 2020; Floridi, Chiriatti, 2020).5 Tuttavia, poiché l’impiego di bot è molto più economico rispetto all’assunzione di persone per testare e identificare ogni bot, i difensori (siti di social media) sono facilmente superati dagli aggressori (criminali) che impiegano i bot (Ferrara, Varol, Davis et al., 2016, p. 5). È possibile rilevare i bot in modo economico utilizzando sistemi di ml come discriminatori automatici, come suggerito da Ratkiewicz e coautori (2011). Tuttavia, l’efficacia di un discriminatore di bot non è falsificabile (la mancanza di rilevazione della presenza di bot non è prova della loro assenza). Un discriminatore è sia addestrato sia dichiarato efficace impiegando dati che comprendono bot conosciuti, che possono essere sostanzialmente meno sofisticati di bot più evasivi adoperati da attori malevoli e pertanto non essere rilevati nell’ambiente (Ferrara, Varol, Davis et al., 2016). Tali bot potenzialmente sofisticati possono anche utilizzare tattiche di ml per adottare tratti umani, come pubblicare con realistici ritmi circadiani (Golder, Macy, 2011), eludendo così il rilevamento basato su ml. Tutto ciò può ovviamente condurre a una corsa agli armamenti in cui attaccanti e difensori si adattano gli uni agli altri (Alvisi, Clement, Epasto et al., 2013; Zhou, Kapoor, 2011), generando un serio problema in un ambiente dove la presenza di reati diviene persistente come il cyberspazio (Seymour, Tully, 2016; Taddeo, 2017a).

			Le azioni intersistemiche fanno riferimento a un problema per i sistemi di monitoraggio dei cia con visione a tunnel che si concentrano solo su un singolo sistema. Esperimenti tra sistemi (Bilge, Strufe, Balzarotti et al., 2009) mostrano che riprodurre automaticamente l’identità di un utente da un social network a un altro (un reato di furto di identità tra sistemi) è più efficace per ingannare gli altri utenti rispetto alla clonazione dell’identità all’interno di quella rete. In questo caso, la politica del social network potrebbe essere in difetto. Twitter, per esempio, assume un ruolo piuttosto passivo, vietando i profili clonati solo quando gli utenti inviano segnalazioni, piuttosto che intraprendere un processo di validazione tra siti.6

			8.2.4 Psicologia

			La psicologia fa riferimento alla preoccupazione che l’ia possa influenzare/manipolare negativamente lo stato mentale di un utente fino al punto di agevolare o causare (in tutto o in parte) il crimine.

			Un effetto psicologico si basa sulla capacità degli aa di ottenere la fiducia degli utenti, rendendo le persone vulnerabili alla manipolazione. Questo è stato dimostrato molto tempo fa da Weizenbaum (1976), dopo aver condotto i primi esperimenti sull’interazione uomo-bot in cui le persone hanno rivelato dettagli inaspettatamente personali sulle loro vite. Un secondo effetto psicologico riguarda gli aa antropomorfici che sono in grado di creare un contesto psicologico o informativo che normalizza reati sessuali e crimini contro la persona, come nel caso di alcuni sexbot (De Angeli, 2009). Tuttavia, a oggi, quest’ultima preoccupazione è ancora da investigare (Bartneck, Belpaeme, Eyssel et al., 2020).





8.3 Minacce


			Siamo ora pronti ad analizzare le aree più specifiche delle minacce dei cia, identificate nell’esame della letteratura (King, Aggarwal, Taddeo et al., 2019), e come queste interagiscono con le preoccupazioni più generali analizzate nel paragrafo precedente.

			8.3.1 Commercio, mercati finanziari e insolvenza

			Questa area di criminalità incentrata sull’economia è definita in Archbold (1991, cap. 30) e include i reati di cartello, come la fissazione dei prezzi e la collusione, l’abuso di informazioni privilegiate, come lo scambio di titoli sulla base di informazioni aziendali private, e la manipolazione del mercato. Attualmente, sorgono problemi nel caso del coinvolgimento dell’ia soprattutto in tre aree: manipolazione del mercato, fissazione dei prezzi e collusione.

			La manipolazione del mercato è definita come quelle “azioni e/o operazioni da parte di partecipanti al mercato che tentano di influenzare artificialmente i prezzi di mercato” (Spatt, 2014, p. 1), dove un criterio necessario è l’“intenzione di ingannare” (Wellman, Rajan, 2017, p. 11). Tuttavia, è stato dimostrato che tali forme di inganno emergono da un’implementazione apparentemente conforme di un aa progettato per operare per conto di un utente (cioè un agente artificiale di trading). Questo perché un aa,

			in particolare uno che apprende da osservazioni reali o simulate, può imparare a generare segnali che sono effettivamente ingannevoli. (Ibidem, p. 14)

			I modelli basati sulla simulazione dei mercati che comprendono agenti di trading artificiali hanno dimostrato (Martínez-Miranda, McBurney, Howard, 2016) che, attraverso l’apprendimento per rinforzo, un aa può apprendere la tecnica dello spoofing del registro delle commesse. Questo implica

			effettuare ordini senza alcuna intenzione di eseguirli, semplicemente per manipolare gli onesti partecipanti al mercato. (Lin, 2016, p. 1289)

			In questo caso, la manipolazione del mercato emerge da un aa che inizialmente esplora lo spazio di azione e, attraverso l’esplorazione, piazza ordini falsi che si rafforzano come strategia redditizia e in seguito sono sfruttati a scopo di lucro. Ulteriori modalità di sfruttare il mercato, che coinvolgono l’intento umano, includono anche la pratica di

			acquisire una posizione in uno strumento finanziario, come un titolo, per poi gonfiare artificialmente il titolo tramite la sua promozione fraudolenta prima di vendere tale posizione a soggetti ignari al prezzo gonfiato, che spesso crolla dopo la vendita. (Ibidem, p. 1285)

			Questo è noto in termini colloquiali come schema “pompa e sgonfia” (pump-and-dump). I social bot hanno dimostrato di essere strumenti efficaci nella realizzazione di tali schemi. Per esempio, in un recente caso importante la sfera di influenza di un social bot network è stata utilizzata per diffondere disinformazioni su un’azienda pubblica poco quotata. Il valore dell’azienda è cresciuto

			più del 36.000% quando le sue penny stocks (azioni comuni) sono passate da meno di $ 0,10 a oltre $ 20 per azione nel giro di poche settimane. (Ferrara, 2015, p. 2)

			Sebbene sia improbabile che tale spam sui social media possa influenzare la maggior parte dei trader umani, gli agenti di trading algoritmico agiscono proprio sulla base di tale opinione sui social media (Haugen, 2017, p. 3). Queste azioni automatizzate possono avere effetti significativi per titoli a basso valore (meno di un centesimo) e illiquidi, che sono suscettibili di oscillazioni di prezzi volatili (Lin, 2016).

			La collusione, sotto forma di fissazione dei prezzi, può emergere anche nei sistemi automatizzati grazie alle capacità di pianificazione e autonomia degli aa. La ricerca empirica ha individuato due condizioni necessarie per la collusione (non artificiale):

			(1) le condizioni che riducono la difficoltà di realizzare una collusione effettiva facilitando il coordinamento; e (2) le condizioni che elevano il costo della condotta non collusiva incrementando la potenziale instabilità del comportamento non collusivo. (Hay, Kelley, 1974, p. 3)

			Le informazioni quasi istantanee sui prezzi (per esempio tramite un’interfaccia di computer) soddisfano la condizione di coordinamento. Quando gli agenti sviluppano algoritmi che alterano il prezzo, qualsiasi azione per abbassare un prezzo da parte di un agente può essere istantaneamente accompagnata da quella di un altro. Di per sé, questo non è sempre un aspetto negativo e può effettivamente rappresentare un mercato efficiente. Tuttavia, la possibilità che l’abbassamento di un prezzo sia emulato è disincentivante e quindi soddisfa la condizione di punizione. Pertanto, se la strategia condivisa di coordinazione dei prezzi è risaputa,7 allora gli algoritmi (se sono razionali) manterranno prezzi più alti artificialmente e tacitamente concordati, in primo luogo non abbassandoli (Ezrachi, Stucke, 2017, p. 5). Cosa più importante, perché la collusione abbia luogo, non è necessario che un algoritmo sia specificamente disegnato per colludere. Come sostengono Ezrachi e Stucke:

			L’intelligenza artificiale gioca un ruolo sempre più importante nel processo decisionale; gli algoritmi, tramite tentativi ed errori, possono giungere a quel risultato [collusione]. (Ibidem)

			L’assenza di intenzionalità, l’intervallo decisionale molto breve e la probabilità che la collusione emerga a seguito delle interazioni tra aa sollevano anche serie preoccupazioni per quanto riguarda la responsabilità e il monitoraggio. I problemi di responsabilità si riferiscono alla possibilità che

			l’entità decisiva di un presunto schema [di manipolazione] sia un programma autonomo e algoritmico che utilizza l’intelligenza artificiale con un input umano minimo o nullo dopo l’installazione iniziale. (Lin, 2016)

			A sua volta, l’autonomia di un aa solleva la questione se

			i regolatori devono determinare se era nelle intenzioni dell’agente che l’azione avesse effetti manipolativi o se era nelle intenzioni del programmatore che l’agente intraprendesse tali azioni per tali scopi? (Wellman, Rajan, 2017, p. 4)

			Il monitoraggio diventa difficile nel caso di reati finanziari che coinvolgono l’ia, a causa della velocità e dell’adattamento degli aa. Il trading ad alta velocità

			incoraggia l’ulteriore utilizzo di algoritmi per poter prendere rapidamente decisioni automatiche, per poter piazzare ed eseguire ordini e per poter monitorare gli ordini dopo che sono stati piazzati. (van Lier, 2016, p. 41)

			Gli agenti artificiali di trading si adattano e “alterano la nostra percezione dei mercati finanziari attraverso questi cambiamenti” (ibidem, p. 45). Al contempo, la capacità degli aa di apprendere e affinare le loro capacità implica che questi agenti possano sviluppare nuove strategie, rendendo sempre più difficile rilevare le loro azioni (Farmer, Skouras, 2013). Inoltre, il problema del monitoraggio è intrinsecamente quello di monitorare un sistema di sistemi, poiché la capacità di rilevare la manipolazione del mercato è influenzata dal fatto che i suoi effetti

			in una o più delle sue componenti possono essere contenuti o possono propagarsi in una reazione a catena a effetto domino, analoga alla psicologia di massa del contagio. (Cliff, Northrop, 2012, p. 12)

			Possono emergere minacce di monitoraggio intersistemico se e quando gli agenti di trading sono impiegati con azioni più ampie, operando a un livello più elevato di autonomia tra i sistemi, per esempio leggendo o pubblicando post sui social media (Wellman, Rajan, 2017). Questi agenti possono, per esempio, imparare a progettare schemi di “pompa e sgonfia”, che non potrebbero essere percepiti dalla prospettiva di un singolo sistema.

			8.3.2 Droghe nocive o pericolose

			I crimini che rientrano in questa categoria includono il traffico, la vendita, l’acquisto e il possesso di droghe vietate (Archbold, 1991, cap. 27). In questo caso, l’ia può fungere da strumento per sostenere il traffico e la vendita di sostanze illecite.

			Il traffico business-to-business (b2b) di droga che utilizza l’ia è una minaccia dovuta ai criminali che adoperano veicoli senza equipaggio, che fanno leva sulla pianificazione dell’ia e sulle tecnologie di navigazione autonoma come strumenti per migliorare i tassi di successo del contrabbando. Poiché le reti di contrabbando vengono fermate dal monitoraggio e dall’intercettazione delle linee di trasporto, l’applicazione delle norme diventa più difficile quando vengono usati veicoli senza equipaggio per trasportare ciò che è contrabbandato. Secondo Europol,8 i droni rappresentano una minaccia orizzontale sotto forma di contrabbando automatizzato di droga. Sottomarini telecomandati per il traffico di cocaina sono già stati scoperti e sequestrati dalle forze dell’ordine statunitensi (Sharkey, Goodman, Ross, 2010). I veicoli subacquei senza equipaggio (vse) offrono un buon esempio dei rischi inerenti al duplice uso dell’ia, e quindi del potenziale di cia. I vse sono stati sviluppati per usi legittimi (come difesa, protezione delle frontiere, pattugliamento delle acque) e tuttavia si sono dimostrati efficaci anche per attività illecite, rappresentando, per esempio, una minaccia significativa per l’attuazione dei divieti relativi alla droga. I criminali possono, presumibilmente, evitare di essere coinvolti perché i vse sono in grado di agire indipendentemente da un operatore (Gogarty, Hagger, 2008, p. 3). Pertanto, non può essere accertato positivamente alcun collegamento con l’impiego dei vse, se il software (e l’hardware) non lascia qualche traccia che riconduca a chi l’ha acquisito e quando, o se le prove possono essere distrutte al momento dell’intercettazione del vse (Sharkey, Goodman, Ross, 2010). Il controllo della fabbricazione di sottomarini e dunque della tracciabilità non è senza precedenti, come illustrano i rapporti sulla scoperta nella giungla costiera colombiana di multimilionari sottomarini con equipaggio. Tuttavia, questi sottomarini rischiano di essere attribuiti proprio all’equipaggio e ai contrabbandieri, a differenza dei vse. A Tampa, in Florida, tra il 2000 e il 2016 sono state avviate con successo oltre 500 cause contro contrabbandieri che utilizzavano sottomarini con equipaggio, con una condanna media a 10 anni (Marrero, 2016). Pertanto, i vse presentano un netto vantaggio rispetto ai tradizionali approcci al contrabbando.

			Una questione correlata riguarda il lato business-to-consumer (b2c) del commercio di droga. Gli algoritmi di ml hanno già rilevato pubblicità di oppioidi venduti senza prescrizione su Twitter (ibidem). Poiché i social bot possono essere usati per pubblicizzare e vendere prodotti, Kerr e Bornfreund si sono chiesti se

			questi buddy bot [ovvero i social bot] potrebbero essere programmati per inviare e rispondere a e-mail o utilizzare la messaggistica istantanea, per avviare conversazioni individuali con centinaia di migliaia o addirittura milioni di persone ogni giorno, offrendo pornografia o droghe ai bambini, sfruttando le insicurezze tipiche degli adolescenti per vendere loro prodotti e servizi inutili. (Kerr, Bornfreund, 2005, p. 8)

			Come sottolineano gli autori, il rischio è che i social bot possano sfruttare la scalabilità economica degli strumenti di conversazione e di pubblicità individualizzata per facilitare la vendita di sostanze illecite.

			8.3.3 Reati contro la persona

			I crimini che rientrano nella categoria dei reati contro la persona vanno dall’omicidio alla tratta di esseri umani (Archbold, 1991, cap. 19), ma finora sembra che i cia riguardino solo molestie e torture.

			Le molestie comprendono comportamenti intenzionali e ripetitivi che generano allarme o causano disagio a una persona. La molestia, in base ai casi passati, è costituita da almeno due o più incidenti contro un individuo (ibidem, p. 354). Per quanto riguarda la tortura, Archbold afferma che:

			un pubblico ufficiale o una persona che agisce in veste ufficiale, qualunque sia la sua nazionalità, commette il reato di tortura se nel Regno Unito o altrove infligge intenzionalmente gravi dolori o sofferenze a un altro soggetto nell’esercizio o nel presunto esercizio delle sue funzioni ufficiali. (Ibidem, p. 435)

			Per quanto riguarda i cia basati sulle molestie, la letteratura fa riferimento ai social bot. Un malintenzionato può avvalersi di un social bot come strumento di molestia diretta o indiretta. La molestia diretta è costituita dalla diffusione di messaggi di odio contro la persona (McKelvey, Dubois, 2017, p. 16). I metodi indiretti possono includere il retwittare o mettere like a tweet negativi, distorcendo i risultati, per dare una falsa impressione di animosità su larga scala contro una persona (McKelvey, Dubois, 2017). Inoltre, un potenziale criminale può anche sovvertire il social bot di un altro attore, distorcendo le strutture di dati apprese di classificazione e generazione tramite l’interazione con gli utenti (cioè la conversazione). È quanto è accaduto nel caso dello sfortunato social bot Twitter di Microsoft “Tay”, che ha rapidamente appreso dalle interazioni con gli utenti a indirizzare “tweet osceni e provocatori” a un’attivista femminista (Neff, Nagy, 2016a). Poiché taluni casi di ciò che potrebbe considerarsi come molestia possono sovrapporsi e avere punti di incidenza con l’uso di social bot per esercitare la libertà di parola, la giurisprudenza deve distinguere queste due ipotesi per risolvere l’ambiguità (McKelvey, Dubois, 2017). Alcune di queste attività possono comprendere molestie nel senso di comportamenti socialmente ma non giuridicamente inaccettabili, mentre altre attività possono raggiungere la soglia delle molestie criminali.

			Anche in questi casi la responsabilità si rivela problematica. Nel caso di Tay, i critici “hanno deriso la decisione di rilasciare Tay su Twitter, una piattaforma con problemi di molestie molto evidenti” (Neff, Nagy, 2016a). Ma anche gli utenti sono da biasimare se “le tecnologie devono essere usate correttamente e per come sono state disegnate” (ibidem). Prospettive e opinioni divergenti sulle molestie da parte dei social bot sono inevitabili nei casi in cui la mens rea di un reato è considerata (rigorosamente) in termini di intenzione, perché l’attribuzione di un’intenzione è una funzione non prestabilita di ingegneria, contesto applicativo, interazione uomo-computer e percezione.

			Per quanto riguarda la tortura, il rischio di cia diventa plausibile se e quando gli sviluppatori integrano le capacità di pianificazione e autonomia dell’ia in un aa che svolge interrogatori. È il caso del rilevamento automatico dell’inganno in un prototipo di guardia robotica per il controllo delle frontiere degli Stati Uniti (Nunamaker, Derrick, Elkins et al., 2011). L’uso dell’ia per l’interrogatorio è motivato dalla sua capacità di rilevare meglio l’inganno, l’emulazione dei tratti umani (come la voce) e la modellazione affettiva per manipolare l’interrogato (McAllister, 2016). Tuttavia, un aa con queste capacità può imparare a torturare una vittima (ibidem, p. 19). Per il soggetto dell’interrogatorio, il rischio è che un aa possa essere impiegato per applicare tecniche di tortura psicologica (per esempio, imitando persone conosciute dal soggetto della tortura) o fisica. Nonostante le convinzioni errate, i professionisti esperti affermano che la tortura (in generale) è un metodo inefficace di estrazione delle informazioni (Janoff-Bulman, 2007). Tuttavia, alcuni attori malintenzionati possono percepire l’uso dell’ia come un modo per ottimizzare l’equilibrio tra la sofferenza e l’indurre l’interrogato a mentire, diventare confuso o insensibile. Tutto questo può avvenire indipendentemente dall’intervento umano. La distanza dell’autore dall’actus reus è una delle ulteriori ragioni per cui la tortura rientra nei cia come una minaccia peculiare, con tre fattori che possono motivare in particolare l’uso di aa per la tortura (McAllister, 2016, pp. 19-20). In primo luogo, l’interrogato probabilmente sa che l’aa non può comprendere il dolore o provare empatia, ed è pertanto improbabile che agisca con pietà e interrompa l’interrogatorio. Senza compassione la semplice presenza di un aa di interrogatorio può far capitolare il soggetto per paura, il che, secondo il diritto internazionale, potrebbe costituire – ma su ciò non vi è certezza – un crimine di tortura (minacciata) (Solis, 2016, pp. 437-485). In secondo luogo, chi si avvale di un aa può essere in grado di distaccarsi emotivamente. Infine, in terzo luogo, può anche distaccarsi fisicamente (cioè non eseguirà l’actus reus secondo le attuali definizioni di tortura). Perciò, diventa più facile ricorrere alla tortura, come esito di miglioramenti nell’efficacia (mancanza di compassione), nella motivazione di chi vi fa ricorso (meno emozioni) e nell’offuscare la propria responsabilità (distacco fisico). Fattori simili possono indurre attori statali o privati a impiegare aa per l’interrogatorio. Tuttavia, vietare l’ia per gli interrogatori (McAllister, 2016, p. 5) potrebbe incorrere in una reazione simile a quella osservata rispetto al divieto delle armi autonome. “Molti considerano [il divieto] una soluzione insostenibile o poco pratica” (Solis, 2016, p. 451), se l’ia offre un vantaggio percepito per la protezione e la sicurezza complessive di una popolazione, facendo sì che le limitazioni all’uso rappresentino un’opzione potenzialmente più probabile rispetto al suo divieto.

			La responsabilità è un problema urgente nel contesto della tortura guidata dall’ia (McAllister, 2016, p. 24). Come per qualsiasi altra forma di cia, un aa non può soddisfare di per sé il requisito della mens rea. Semplicemente, un aa non ha alcuna intenzionalità, né ha la capacità di attribuire un significato alle proprie azioni. Infatti, dato che i computer (che implementano gli aa) sono macchine sintattiche, e non semantiche, nel senso esaminato nella prima parte di questo libro, possono compiere azioni e manipolazioni senza attribuire a esse alcun significato, che resta una prerogativa esclusiva degli operatori umani (Taddeo, Floridi, 2005, 2007). In quanto sistemi non pensanti, privi di intenzioni e semplice riserva di capacità di agire, gli aa non possono assumersi la responsabilità morale o la responsabilità delle loro azioni (sebbene possano esserne causalmente responsabili, come ho già indicato in Floridi, Sanders, 2004). Tuttavia, adottare un approccio di responsabilità penale oggettiva, in base alla quale possono essere comminate sanzioni o imposti risarcimenti senza provare la colpa (Floridi, 2016a), può offrire una via d’uscita dal problema abbassando la soglia di intenzionalità per il crimine, invece di fare affidamento esclusivamente sulla responsabilità civile. A questo proposito, McAllister (2016, p. 38) sostiene che la responsabilità civile oggettiva è inappropriata a causa dell’irragionevole grado di previsione richiesto a un designer quando un aa impara a torturare in modi imprevedibili. Sviluppare e impiegare tali imprevedibili e complessi interrogatori autonomi basati sull’ia significa, come sottolinea Grut, che

			è ancora meno realistico aspettarsi che operatori umani [o coloro che impiegano aa] esercitino un controllo significativo per interdire le loro operazioni. (Grut, 2013, p. 11)

			Neanche il controllo è un problema, poiché la sanzione tipica per la responsabilità colposa per un prodotto è una multa, che può non essere equa né dissuasiva data la potenziale gravità di eventuali violazioni dei diritti umani che ne derivano (McAllister, 2016, p. 38). Pertanto, una grave responsabilità oggettiva per i cia richiederebbe lo sviluppo di linee guida di condanna specifiche per imporre pene adeguate al reato, anche se il reato non è intenzionale, come nel caso dell’omicidio colposo da parte di un’azienda, in cui le persone possono essere condannate alla detenzione. La questione di chi esattamente debba affrontare la reclusione per reati contro la persona causati dall’ia (come per molti usi dell’ia) è difficile e significativamente ostacolata dal “problema delle molte mani” (Van de Poel, Nihlén Fahlquist, Doorn et al., 2012) e della responsabilità distribuita (Floridi, 2012b). È chiaro che un aa non può essere ritenuto responsabile. Tuttavia, la molteplicità degli attori crea un problema nell’accertamento di dove risieda la responsabilità, se nella persona che ha commissionato ed eseguito l’aa, nei suoi sviluppatori o nei legislatori e responsabili politici che hanno autorizzato (o non proibito) l’implementazione nel mondo reale di tali agenti (McAllister, 2016, p. 39). I reati gravi, inclusi i danni fisici e mentali, che non sono stati previsti dai legislatori potrebbero plausibilmente rientrare nei cia, con tutta l’ambiguità che ne deriva e la mancanza di chiarezza giuridica. Ciò spinge a estendere o chiarire le dottrine esistenti sulla responsabilità solidale.

			8.3.4 Reati sessuali

			I reati sessuali discussi in letteratura in relazione all’ia sono i seguenti: stupro (cioè sesso penetrativo senza consenso), aggressione sessuale (cioè contatto sessuale senza consenso) e rapporti o attività sessuali con un minore. Il mancato consenso, nel contesto dello stupro e della violenza sessuale, è costituito da due condizioni:

			a) vi deve essere una mancanza di consenso da parte della vittima,

			b) l’autore del reato non deve avere un ragionevole convincimento nell’esistenza del consenso.

			(Archbold, 1991)

			Questi crimini coinvolgono l’ia quando, tramite un’interazione avanzata uomo-computer, quest’ultima promuove l’oggettivazione sessuale o l’abuso e la violenza sessualizzati, e potenzialmente (in senso molto lato) simula e quindi aumenta il desiderio sessuale per i reati sessuali. I social bot possono supportare la promozione di reati sessuali, e De Angeli sottolinea che

			l’abuso verbale e le conversazioni sessuali sono risultati elementi comuni dell’interazione anonima con agenti conversazionali [(De Angeli, Brahnam, 2008; Rehm, 2008; Veletsianos, Scharber, Doering 2008)]. (De Angeli, 2009, p. 4)

			La simulazione di reati sessuali è possibile con l’uso di robot sessuali fisici (d’ora in poi sexbot). In genere si ritiene che un sexbot sia dotato di:

			(i) una forma umanoide; (ii) capacità di movimento; e (iii) un certo grado di intelligenza artificiale (vale a dire, una certa capacità di percepire, elaborare e rispondere ai segnali nell’ambiente circostante). (Danaher, 2017)

			Il fenomeno dei sexbot è notevolmente cresciuto (Döring, Mohseni, Walter, 2020; González-González, Gil-Iranzo, Paderewski-Rodríguez 2021). Taluni sexbot sono disegnati per emulare reati sessuali, come lo stupro di adulti o bambini (Danaher, 2017, pp. 6-7). Ricerche suggeriscono che è comune per una persona voler provare i robot sessuali o avere fantasie di stupro (Danaher, 2017), sebbene non sia necessariamente comune che una persona abbia entrambi i desideri. L’ia potrebbe essere usata per facilitare le rappresentazioni di reati sessuali, sino a confondere realtà e fantasia, attraverso capacità di conversazione avanzate, e potenzialmente interazioni fisiche (sebbene niente indichi che vi sia qualche forma di reale fisicità nel futuro prossimo).

			L’interazione con social bot e sexbot è la preoccupazione principale per quanto riguarda il possibile ruolo causale di aa eccessivamente antropomorfici nel desensibilizzare l’autore nei confronti dei reati sessuali, o addirittura nell’accrescere il desiderio di commetterli (De Angeli, 2009, p. 7; Danaher, 2017, pp. 27-28). Tuttavia, come sostiene De Angeli (2009), si tratta di una “critica controversa rivolta di sovente ai videogiochi violenti (Freier, 2008; Whitby, 2008)”. Inoltre, possiamo presumere che, se la pornografia estrema può incoraggiare reati sessuali, allora a fortiori lo stupro simulato, dove per esempio un sexbot non mostra di consentire o esplicitamente mostra di dissentire, porrebbe lo stesso problema. Tuttavia, un meta-meta-studio (Ferguson, Hartley, 2009) conclude che dobbiamo “scartare l’ipotesi che la pornografia contribuisca a incrementare i comportamenti di aggressione sessuale”. Tale incertezza suggerisce che, come sostiene Danaher (2017, pp. 27-28), i sexbot (e presumibilmente anche i social bot) possono aumentare, diminuire o addirittura non avere alcun effetto sui reati sessuali fisici che danneggiano direttamente le persone. I danni indiretti non hanno dunque portato alla criminalizzazione dei sexbot.9 Perciò, i reati sessuali come area dei cia restano una questione aperta.

			8.3.5 Furto e frode, contraffazione e sostituzione di persona

			Contraffazione e sostituzione di persona sono collegate tramite i cia a furti e frodi extra-aziendali, con implicazioni anche per l’uso di ml nelle frodi aziendali.

			Per quanto riguarda il furto e la frode extra-aziendale, il processo prevede due fasi. Inizia con l’utilizzo dell’ia per raccogliere dati personali e procede con l’utilizzo dei dati personali rubati e di altri metodi di ia per forgiare un’identità che induca le autorità bancarie a effettuare una transazione (ovvero furto e frode bancaria). Nella prima fase di realizzazione dei cia per furti e frodi, ci sono tre modi in cui le tecniche di ia contribuiscono a raccogliere i dati personali.

			La prima tecnica prevede l’uso di bot di social media per prendere di mira gli utenti su larga scala e a basso costo, sfruttando la propria capacità di generare post, imitare le persone e successivamente acquisire fiducia con richieste di amicizia o diventando “follower” su siti come Twitter, LinkedIn e Facebook (Bilge, Strufe, Balzarotti et al., 2009). Quando un utente accetta una richiesta di amicizia, un potenziale criminale ottiene informazioni personali, come la posizione, il numero di telefono o la cronologia delle relazioni dell’utente, a cui ha accesso normalmente solo la cerchia degli amici accettati dall’utente (ibidem). Poiché molti utenti aggiungono cosiddetti amici che non conoscono, compresi i bot, non sorprende che tali attacchi che compromettono la privacy abbiano un alto tasso di successo. Esperimenti passati con un social bot hanno sfruttato il 30-40% degli utenti in generale (ibidem) e il 60% degli utenti che hanno condiviso un amico comune con il bot (Boshmaf, Muslukhov, Beznosov et al., 2013). Inoltre, i bot di clonazione dell’identità sono riusciti, in media, a far accettare su LinkedIn il 56% delle loro richieste di amicizia (Bilge, Strufe, Balzarotti et al., 2009). Tale clonazione dell’identità può sollevare sospetti a causa del fatto che un utente sembra avere più account sullo stesso sito (uno reale e uno contraffatto da una terza parte). Perciò, clonare un’identità da un social network a un altro aggira questi sospetti e, di fronte a un monitoraggio inadeguato, la clonazione dell’identità tra siti si rivela una tattica efficace (ibidem), come abbiamo osservato in precedenza.

			La seconda tecnica per raccogliere dati personali, che è compatibile con la fiducia acquisita tramite l’amicizia con gli utenti dei social media, e talora può anche basarsi su di essa, utilizza parzialmente i social bot conversazionali per l’ingegneria sociale (Alazab, Broadhurst, 2016, p. 12). Ciò si verifica quando l’ia

			tenta di manipolare il comportamento costruendo un rapporto con la vittima, e quindi sfruttando l’emergere di tale relazione per ottenere informazioni dalla vittima o avere accesso al suo computer. (Chantler, Broadhurst, 2008, p. 65)

			Sebbene la letteratura sembri sostenere l’efficacia di questa ingegneria sociale basata sui bot, è giustificato lo scetticismo riguardo alla manipolazione automatizzata su base individuale e a lungo termine, date le capacità attualmente limitate dell’ia conversazionale. Tuttavia, come soluzione a breve termine, un criminale può lanciare una rete ingannevole di social bot in modo sufficientemente ampio da scoprire individui pronti a cadere nella rete. La manipolazione iniziale basata sull’ia può mettere insieme i dati personali raccolti e riutilizzarli per produrre “casi più intensi di familiarità, empatia e intimità simulate, che portano a rivelare quantità maggiori di dati” (Graeff, 2013, p. 5). Dopo aver acquisito fiducia, familiarità e dati personali da un utente, il criminale (umano) può spostare la conversazione in un altro contesto, come la messaggistica privata, in cui l’utente presume che le norme sulla privacy siano rispettate (Graeff, 2014). Sostanzialmente, da questo momento, è possibile superare le carenze conversazionali dell’ia per interagire con l’utente utilizzando un cyborg; cioè un umano assistito da un bot (o viceversa) (Chu, Gianvecchio, Wang et al., 2010). Perciò, un criminale può fare un uso giudizioso delle capacità di conversazione altrimenti limitate dell’ia come mezzo plausibile per raccogliere dati personali.

			La terza tecnica per raccogliere dati personali dagli utenti è il phishing (la pesca) automatizzato. Di norma, il phishing non ha successo se il criminale non personalizza in modo sufficiente i messaggi diretti verso l’utente che ha di mira. Gli attacchi di phishing mirati e personalizzati (noti come spear phishing), che si sono dimostrati quattro volte più efficaci di un approccio generico (Jagatic, Johnson, Jakobsson et al., 2007), richiedono molto lavoro. Tuttavia, lo spear phishing a basso costo è possibile utilizzando l’automazione (Bilge, Strufe, Balzarotti et al., 2009), e i ricercatori hanno dimostrato che è fattibile utilizzando tecniche di ml per creare messaggi personalizzati per un utente specifico (Seymour, Tully, 2016).

			Nella seconda fase delle frodi bancarie supportate dall’ia, quest’ultima può contribuire alla creazione di un’identità, anche grazie ai recenti progressi nelle tecnologie di sintesi vocale (Bendel, 2019). Usando le capacità di classificazione e di generazione del ml, il software Adobe è in grado di apprendere in modo avversario e riprodurre lo schema del discorso personale e individuale di qualcuno a partire da una registrazione di venti minuti della sua voce (ibidem, p. 1). Bendel sostiene che la sintesi vocale assistita dall’ia solleva una minaccia eccezionale di furti e frodi, dal momento che

			potrebbe utilizzare VoCo e Co [il software di editing e generazione vocale di Adobe] per i processi di sicurezza biometrica e per sbloccare porte, casseforti, veicoli e così via, ed entrarvi o utilizzarli. Con la voce del cliente, [i criminali] potrebbero parlare con la sua banca o con altre istituzioni per raccogliere dati sensibili o per effettuare transazioni critiche o dannose. Tutti i tipi di sistemi di sicurezza basati sulla voce potrebbero essere violati. (Ibidem, p. 3)

			Da anni, ormai, le frodi con carta di credito costituiscono principalmente un reato online,10 che si consuma quando “la carta di credito viene utilizzata da remoto; sono necessari solo i dati della carta di credito” (Delamaire, Abdou, Pointon, 2009, p. 65). Poiché le frodi con carta di credito in genere non richiedono l’interazione fisica né la presenza materiale, l’ia può agevolare le frodi fornendo sintesi vocali o contribuendo a raccogliere sufficienti dettagli personali.

			In caso di frode aziendale, l’ia utilizzata per il rilevamento può anche rendere più facile commettere la frode. Nello specifico,

			quando i dirigenti coinvolti in frodi finanziarie sono ben consapevoli delle tecniche e dei software di rilevamento delle frodi, che di solito sono informazioni pubbliche e sono facili da ottenere, è probabile che adattino i metodi con cui commettono frodi e ne rendano difficile l’individuazione, soprattutto con le tecniche esistenti. (Zhou, Kapoor, 2011, p. 571)

			Più che identificare un caso specifico di cia, questo uso dell’ia mette in evidenza i rischi di un’eccessiva dipendenza dall’ia per l’individuazione delle frodi, che può agevolare i truffatori. Questi tipi di furti e di frodi riguardano denaro reale. Una minaccia relativa al mondo virtuale concerne l’ipotesi che i social bot possano commettere crimini in contesti di giochi online multiplayer di massa (mmog). Questi giochi online hanno spesso economie complesse, in cui la fornitura di oggetti di gioco è artificialmente limitata e i beni immateriali di gioco possono avere un valore reale se i giocatori sono disposti a pagare per loro; oggetti che in alcuni casi costano più di $ 1000 (Chen, Chen, Song et al., 2004, p. 1). Perciò, non sorprende che, da un campione casuale di 613 procedimenti penali nel 2002 per crimini di gioco online a Taiwan, i ladri di proprietà virtuali abbiano sfruttato le credenziali compromesse degli utenti 147 volte e le identità rubate 52 volte (Chen, Chen, Hwang et al., 2005). Tali crimini sono analoghi all’uso dei social bot per realizzare furti e frodi su larga scala sui siti di social media, e la questione è se l’ia possa essere implicata in questo spazio criminale virtuale.





8.4 Possibili soluzioni


			Dopo aver delineato ed esaminato le potenzialità dei cia, è giunto il momento di analizzare le soluzioni attualmente disponibili. Questo è il compito del presente paragrafo.

			8.4.1 Affrontare l’emergenza

			Esistono numerose soluzioni giuridiche e tecnologiche che possono essere prese in considerazione per affrontare il problema dei comportamenti emergenti. Le soluzioni giuridiche possono comportare la limitazione dell’autonomia degli agenti o del loro impiego. Per esempio, la Germania ha creato contesti deregolamentati in cui è consentita la sperimentazione di automobili a guida autonoma, se i veicoli rimangono al di sotto di un livello di autonomia inaccettabile, al fine di

			raccogliere dati empirici e conoscenze sufficienti per prendere decisioni razionali per una serie di questioni critiche. (Pagallo, 2017, p. 7)

			Dunque, la soluzione consiste nel fatto che, se la legislazione non vieta livelli più elevati di autonomia per un dato aa, la legge impone che tale libertà sia abbinata a rimedi tecnologici per prevenire atti o omissioni criminali emergenti una volta posti in contesti non strutturati.

			Una possibilità è richiedere agli sviluppatori di rilasciare gli aa solo quando dispongono di livelli di conformità normativa in fase di esecuzione, che accettano specifiche dichiarative delle regole giuridiche e impongono vincoli al comportamento in esecuzione degli aa. Sebbene siano ancora al centro della ricerca in corso, gli approcci alla conformità normativa in fase di esecuzione includono architetture per regolare i piani non conformi degli aa (Meneguzzi, Luck, 2009; Vanderelst, Winfield, 2018a); e quadri formali basati sulla logica temporale provatamente corretti che selezionano, regolano o generano piani per gli aa di conformità alle norme (Van Riemsdijk, Birna, Dennis et al., 2013; Van Riemsdijk, Birna, Dennis, Fisher, Slavkovik et al., 2015; Dennis, Fisher, Slavkovik et al., 2016). In un ambiente multiagente (mas), i cia possono emergere dal comportamento collettivo; pertanto, i livelli di conformità a livello del mas possono modificare i piani di un singolo aa, al fine di prevenire azioni collettive illecite (Uszok, Bradshaw, Jeffers et al., 2003; Bradshaw, Dutfield, Benoit et al., 1997; Tonti, Bradshaw, Jeffers et al., 2003). In sostanza, tali soluzioni tecniche propongono di disciplinare la conformità (rendendo impossibile la non conformità, almeno nella misura in cui qualsiasi prova formale risulti applicabile a contesti del mondo reale) con regole giuridiche predefinite all’interno di un singolo aa o di un mas (Andrighetto, Governatori, Noriega et al., 2013, p. 105).

			Tuttavia, il passaggio di questi approcci dalla mera regolamentazione, per cui la deviazione dalla norma resta fisicamente possibile, all’irreggimentazione, può non essere auspicabile se si considera l’impatto sulla democrazia e sul sistema giuridico. Tali approcci implementano il concetto per cui il “codice è la legge” (code-as-law) di Lessig (1999), che considera

			il codice software come regolatore in sé e per sé affermando che l’architettura che produce può fungere da strumento di controllo sociale su coloro che lo utilizzano. (Graeff, 2013, p. 4)

			Tuttavia, come giustamente obietta Hildebrandt:

			mentre il codice informatico genera una sorta di normatività simile al diritto, gli manca – proprio perché non è diritto – […] la possibilità di contestarne l’applicazione in sede giudiziaria. Si tratta di un grave deficit nel rapporto tra diritto, tecnologia e democrazia. (Hildebrandt, 2008, p. 175)

			Se il “codice è la legge” implica un deficit di contestazione democratica e giuridica, allora a fortiori affrontare i cia emergenti con un livello di ragionamento giuridico che comprende un codice normativo ma incontestabile, rispetto al diritto contestabile da cui deriva, presenta gli stessi problemi.

			La simulazione sociale può affrontare un problema ortogonale, per cui il proprietario di aa può scegliere di operare al di fuori della legge e di ogni requisito proprio del livello di ragionamento giuridico (Vanderelst, Winfield 2018b, p. 4). L’idea di base è utilizzare la simulazione come banco di prova prima di rilasciare gli aa nell’ambiente. Per esempio, in un contesto di mercato, i regolatori dovrebbero

			agire come “autorità di certificazione”, che eseguono nuovi algoritmi di trading nel simulatore di sistema per valutare il loro probabile impatto sul comportamento sistemico complessivo prima di consentire al proprietario/sviluppatore dell’algoritmo di eseguirlo “dal vivo”. (Cliff, Northrop, 2012, p. 19)

			Le società private potrebbero finanziare simulazioni sociali così estese, come bene comune, e in sostituzione di (o in aggiunta a) misure di sicurezza proprietarie (ibidem, p. 21). Tuttavia, una simulazione sociale è un modello di un sistema intrinsecamente caotico, che lo rende uno strumento inadeguato per previsioni specifiche (Edmonds, Gershenson, 2015, p. 12). Nondimeno, l’idea potrebbe ancora risultare efficace, poiché si concentra sulla rilevazione della possibilità strettamente qualitativa di eventi precedentemente imprevisti ed emergenti in un mas (ibidem, p. 13).

			8.4.2 Affrontare la responsabilità

			Sebbene la responsabilità sia un argomento particolarmente ampio, quattro modelli sono frequentemente esaminati in relazione ai cia (Hallevy, 2011, p. 13): responsabilità diretta, perpetrazione per mezzo di altri, responsabilità di comando e conseguenza naturale e probabile.

			Il modello di responsabilità diretta attribuisce gli elementi fattuali e mentali a un aa, rappresentando un passaggio radicale dalla visione antropocentrica degli aa come strumenti agli aa come decisori (potenzialmente uguali) (van Lier, 2016). Alcuni propongono di considerare un aa direttamente responsabile perché “il processo di analisi nei sistemi di ia è analogo a quello della comprensione umana” (Hallevy, 2011, p. 15), con cui viene concepita la nozione di autore nel senso che, come sostiene Dennett (1987), possiamo trattare qualsiasi agente, a fini pratici, come se possedesse stati mentali. Tuttavia, un limite fondamentale di questo modello risiede nel fatto che gli aa non hanno (separata) personalità giuridica e capacità di agire, e quindi non possiamo ritenere un aa legalmente responsabile in quanto tale (a prescindere dal fatto che ciò sia o no auspicabile nella pratica). Al contempo, è stato osservato che gli aa non possono contestare un verdetto di colpevolezza e che

			se un soggetto non può stare in giudizio non può impugnare l’incriminazione, cosa che trasformerebbe la punizione in disciplina. (Hildebrandt, 2008, p. 178)

			Inoltre, giuridicamente, gli aa non possono soddisfare l’elemento mentale; nel senso che

			il punto di vista giuridico comune esclude i robot da qualsiasi tipo di responsabilità penale perché privi di componenti psicologiche come intenzioni o coscienza. (Pagallo, 2011, p. 349)

			Questa mancanza di stati mentali reali diventa evidente se si considera che la comprensione di un simbolo (cioè un concetto) da parte di un aa è limitata al suo radicamento in ulteriori simboli sintattici (Taddeo, Floridi, 2005, 2007), lasciando così la mens rea nel limbo. La mancanza di una mente colpevole non impedisce che lo stato mentale sia imputato all’aa (così come un’azienda può vedersi imputato lo stato mentale dei suoi dipendenti e quindi, in quanto organizzazione, può essere ritenuta responsabile), ma la responsabilità di un aa richiederebbe comunque che tale agente abbia personalità giuridica. Un ulteriore problema risiede nel fatto che ritenere un aa l’unico responsabile può rivelarsi inaccettabile, poiché porterebbe a una deresponsabilizzazione degli agenti umani dietro l’aa (per esempio, un ingegnere, un utente o una società), che rischia di indebolire il potere dissuasivo del diritto penale.

			Per assicurare che la legge penale sia efficace, possiamo trasferire il peso della responsabilità sugli esseri umani – persone fisiche o giuridiche – che hanno fatto una differenza (criminalmente sanzionabile) per il sistema, come i vari ingegneri, utenti, fornitori ecc., per cui se il design è imperfetto e l’esito illecito, allora tutti gli agenti (umani) coinvolti devono ritenersi responsabili (Floridi, 2016a). I successivi due modelli discussi in letteratura si muovono in questa direzione, concentrandosi sulla responsabilità delle persone umane o di altre persone giuridiche coinvolte nella produzione e nell’utilizzo dell’aa.

			Il modello della perpetrazione per mezzo di altri (Hallevy, 2011, p. 4), che utilizza l’intenzione come standard di mens rea, inquadra l’aa come strumento del crimine in cui “la parte che orchestra il reato (l’autore per mezzo di altri) è il vero autore”. La perpetrazione per mezzo di altri lascia

			tre candidati umani alla responsabilità di fronte a un tribunale penale: programmatori, produttori e utilizzatori di robot [di aa]. (Pagallo, 2017, p. 21)

			Chiarire l’intenzione è fondamentale per applicare la perpetrazione per mezzo di altri. Per quanto riguarda i social media, “gli sviluppatori che creano consapevolmente social bot per intraprendere azioni non etiche sono chiaramente colpevoli” (de Lima Salge, Berente, 2017, p. 30). Per maggiore chiarezza, come sostiene Arkin (2008), designer e programmatori dovrebbero essere tenuti ad assicurare che gli aa rifiutino di eseguire un ordine criminale (che solo l’operatore può esplicitamente disabilitare), il che eliminerebbe l’ambiguità dell’intenzione e pertanto la responsabilità (Arkin, Ulam, 2012). Ciò significa che, per essere responsabile, l’operatore di un aa deve volere la realizzazione del fatto illecito, rimuovendo la posizione predefinita dell’aa di “può, ma non farà del male”. Quindi, insieme ai controlli tecnologici e considerando un aa come un mero strumento del cia, la perpetrazione per mezzo di altri affronta quei casi in cui un operatore intende utilizzare un aa per commettere un cia.

			Il modello di responsabilità di comando, che utilizza la conoscenza come standard di mens rea, attribuisce la responsabilità a qualsiasi ufficiale militare che fosse a conoscenza (o avrebbe dovuto conoscere) e non sia riuscito a prendere misure ragionevoli per prevenire i crimini commessi dalle forze sotto il proprio comando, che in futuro potrebbero includere degli aa (McAllister, 2016). Perciò, la responsabilità di comando è compatibile con la perpetrazione per mezzo di altri o può anche essere concepita come una sua fattispecie, e trova applicazione in contesti in cui esiste una catena di comando, come all’interno delle forze armate o di polizia. Questo modello è di regola chiaro sul modo in cui

			la responsabilità dovrebbe essere distribuita, tra i comandanti, agli ufficiali incaricati dell’interrogatorio e ai designer del sistema. (Ibidem, p. 39)

			Tuttavia

			questioni relative a livelli di crescente complessità nella programmazione, relazioni robo-umane e integrazione in strutture gerarchiche, mettono in discussione la sostenibilità di queste teorie. (Ibidem)

			Il modello di responsabilità per conseguenza naturale e probabile, che usa la negligenza o l’imprudenza come standard di mens rea, concerne i casi di cia in cui uno sviluppatore o un utente di aa non intendono né hanno conoscenza a priori di un reato (Hallevy, 2011). La responsabilità è attribuita allo sviluppatore o all’utente se il danno è conseguenza naturale e probabile della loro condotta, esponendo gli altri in modo imprudente o negligente al rischio (ibidem), come nei casi di manipolazione del mercato emergente causata dall’ia (Wellman, Rajan, 2017).

			Conseguenza naturale e probabile e responsabilità di comando non sono concetti nuovi; sono entrambi analoghi al principio per cui il superiore risponde del fatto (respondeat superior) previsto da norme antiche quanto il diritto romano, secondo cui il proprietario di una persona schiava era responsabile di qualsiasi danno causato da quella persona (Floridi, 2017c). Tuttavia, potrebbe non essere sempre ovvio

			quale programmatore era responsabile di una particolare riga di codice, o addirittura in che misura il programma risultante era il risultato del codice iniziale o lo sviluppo successivo di quel codice da parte del sistema di ml. (Williams, 2017, p. 41)

			Tale ambiguità implica che, come alcuni suggeriscono, quando un cia può configurarsi, gli aa dovrebbero essere vietati “per fare fronte a esigenze di controllo, sicurezza e responsabilità” (Joh, 2016, p. 18), il che almeno renderebbe chiara la responsabilità per la violazione di tale divieto. Tuttavia, altri sostengono che un eventuale divieto motivato dal rischio di cia emergenti dovrebbe essere attentamente bilanciato con il rischio di ostacolare l’innovazione; diviene pertanto fondamentale fornire una definizione adeguata dello standard di negligenza (Gless, Silverman, Weigend, 2016), per assicurare che un divieto totale non sia considerato l’unica soluzione, dato che finirebbe per scoraggiare il design di aa che risultino preferibili alle persone in termini di sicurezza.

			8.4.3 Controllo del monitoraggio

			Ci sono quattro meccanismi principali per affrontare il monitoraggio dei cia. Il primo è elaborare predittori dei cia utilizzando la conoscenza del dominio. Ciò supererebbe il limite di metodi di classificazione di ml più generici, vale a dire quelli in cui le funzionalità utilizzate per il rilevamento possono essere utilizzate anche per l’elusione. I predittori specifici della frode finanziaria possono considerare le proprietà istituzionali (Zhou, Kapoor, 2011) come obiettivi (per esempio, se i benefici superano i costi), struttura (per esempio, la mancanza di un comitato di revisione) e (assenza di) valori morali del management (gli autori non dicono quali, se presenti, di questi valori siano effettivamente predittivi). I predittori per il furto di identità (per esempio, la clonazione del profilo) hanno incluso la richiesta agli utenti di considerare se la posizione dell’“amico” che sta inviando loro messaggi soddisfa le loro aspettative (Bilge, Strufe, Balzarotti et al., 2009).

			Il secondo meccanismo utilizza la simulazione sociale per scoprire schemi ricorrenti di criminalità (Wellman, Rajan, 2017). Tuttavia, l’individuazione di schemi ricorrenti (patterns) deve fare i conti con la capacità, talora limitata, di collegare le identità offline con le attività online. Per esempio, nei mercati, è necessario uno sforzo considerevole per correlare più ordini con unico soggetto giuridico e, pertanto,

			algoritmi manipolativi possono essere impossibili da rilevare nella pratica. (Farmer, Skouras, 2013, p. 17)

			Inoltre, sui social media

			un avversario controlla più identità online e si unisce a un sistema mirato sotto queste identità al fine di sovvertire un particolare servizio. (Boshmaf, Muslukhov, Beznosov et al., 2012, p. 4)

			Il terzo meccanismo affronta la tracciabilità lasciando indizi rivelatori nelle componenti che costituiscono gli strumenti dei cia. Per esempio, le tracce fisiche lasciate dai produttori nell’hardware degli aa, come i vse usati per il traffico di droga, o le impronte digitali nel software di ia di terze parti (Sharkey, Goodman, Ross, 2010). Il software di riproduzione vocale di Adobe adotta tale approccio, posizionando una filigrana elettronica nell’audio generato (Bendel, 2019). Tuttavia, la mancanza di conoscenza e di controllo su chi sviluppa le componenti degli strumenti di ia (usati per i cia) limita la tracciabilità conseguibile con l’inclusione di filigrane elettroniche e tecniche simili.

			Il quarto meccanismo mira al monitoraggio intersistemico e si avvale dell’auto-organizzazione tra sistemi (van Lier, 2016). L’idea, che ha le sue radici in Luhmann e Bednarz (1995), richiede di concepire (Floridi, 2008b) un sistema (per esempio, un sito di social media) che assume il ruolo di agente morale11 e un secondo sistema (per esempio, un mercato) che assume il ruolo di paziente morale. Il paziente morale è qualsiasi destinatario di azioni morali (Floridi, 2012b). La concettualizzazione scelta da van Lier (2016) prevede che i seguenti siano tutti sistemi: al livello atomico più basso un agente artificiale o umano; a un livello superiore qualsiasi mas come una piattaforma di social media, un mercato e così via; e, generalizzando ulteriormente, qualsiasi sistema di sistemi. Quindi, qualsiasi sistema umano, artificiale o misto può qualificarsi come paziente o agente morale. Se un agente sia realmente un agente morale dipende dal fatto che l’agente possa compiere azioni moralmente qualificabili, ma non dal fatto che l’agente morale possa o debba essere ritenuto moralmente responsabile di tali azioni (Floridi, 2012b). Adottando questa distinzione tra agente morale e paziente morale, van Lier propone un processo per monitorare e affrontare i crimini e gli effetti che attraversano i sistemi, che prevede quattro fasi che elenchiamo genericamente ed esemplifichiamo nello specifico: l’informazione-selezione delle azioni interne dell’agente morale in ragione della loro rilevanza per il paziente morale (per esempio, post che gli utenti fanno sui social media); l’enunciazione delle informazioni selezionate dall’agente morale al paziente morale (per esempio, la notifica a un mercato finanziario di post sui social media); la valutazione da parte del paziente morale della normatività delle azioni enunciate (per esempio, se i post sui social media fanno parte di uno schema “pompa e sgonfia”); e il feedback dato dal paziente morale all’agente morale (per esempio, notificare a un sito di social media che un utente sta conducendo uno schema di “pompa e sgonfia”, sul quale il sito di social media dovrebbe agire). Questo passaggio finale completa un

			processo di feedback [che] può creare un ciclo di apprendimento automatico in cui gli elementi morali sono inclusi contemporaneamente. (van Lier, 2016, p. 11)

			Potrebbe trattarsi, per esempio, di un sito di social media che apprende e allinea la normatività dei propri comportamenti a quella del mercato. Un simile processo di auto-organizzazione potrebbe essere utilizzato per affrontare altre aree dei cia. La creazione di un profilo su Twitter (l’agente morale) potrebbe essere rilevante per Facebook (il paziente morale) per quanto riguarda il furto di identità (selezione delle informazioni). Notificando a Facebook i dettagli del profilo appena creato (enunciazione), Facebook potrebbe determinare se si tratta di un furto di identità interrogando l’utente pertinente (comprensione) e notificando a Twitter di intraprendere l’azione appropriata (feedback).

			8.4.4 Affrontare la psicologia

			Ci sono due preoccupazioni principali relative all’elemento psicologico dei cia: la manipolazione degli utenti e (nel caso dell’ia antropomorfica) la creazione in un utente del desiderio di compiere un crimine. Attualmente i ricercatori hanno elaborato soluzioni solo per quest’ultimo problema ma non per il primo.

			Se gli aa antropomorfici costituiscono un problema, allora possono esserci due approcci. Uno è quello di vietare o limitare gli aa antropomorfici che consentono di simulare il crimine. Questa posizione sottolinea l’importanza di limitare gli aa antropomorfici in generale, perché “sono precisamente il tipo di robot [aa] di cui è più probabile l’abuso” (Whitby, 2008, p. 6). I casi in cui i social bot sono

			disegnati, intenzionalmente o meno, con un genere in mente, […] l’attrattiva e il realismo degli agenti femminili [sollevano la domanda:] gli eca [ovvero i social bot] incoraggiano gli stereotipi di genere, questo avrà un impatto sulle donne reali online? (De Angeli, 2009)

			Il suggerimento è di rendere inaccettabile che i social bot emulino proprietà antropomorfiche, come avere un genere o un’etnia riconoscibile. Per quanto riguarda i sexbot che emulano i reati sessuali, un ulteriore suggerimento è quello di emanare un divieto sotto forma di un “pacchetto di leggi che aiutano a migliorare la moralità sessuale sociale” e rendono chiare le norme sull’intolleranza (Danaher, 2017, pp. 29-30).

			Un secondo suggerimento (sebbene incompatibile con il primo) è quello di servirsi di aa antropomorfici come modo per respingere i reati sessuali simulati. Per esempio, riguardo all’abuso di agenti pedagogici artificiali,

			raccomandiamo che le risposte degli agenti siano programmate per prevenire o ridurre ulteriori abusi sugli studenti. (Veletsianos, Scharber, Doering, 2008, p. 8)

			Come sostiene Darling,

			non soltanto ciò contrasterebbe la desensibilizzazione e le esternalità negative del comportamento delle persone, ma preserverebbe i vantaggi terapeutici e educativi dell’utilizzo di alcuni robot più come compagni che come strumenti. (Darling, 2015, p. 14)

			L’attuazione di questi suggerimenti richiede di scegliere se criminalizzare il lato della domanda o dell’offerta della transazione, o entrambi. Gli utenti possono ritrovarsi compresi nell’ambito di applicazione delle sanzioni. Al contempo, si potrebbe obiettare che,

			come per altri reati che coinvolgono il “vizio” personale, anche fornitori e distributori potrebbero essere presi di mira in quanto facilitano e incoraggiano gli atti illeciti. In effetti, potremmo prenderli di mira in modo esclusivo o preferenziale, come avviene oggi per le droghe illecite in molti paesi. (Danaher, 2017, p. 33)





8.5 Sviluppi futuri


			C’è ancora molta incertezza su ciò che già sappiamo dei cia, in termini di minacce specifiche per area, minacce generali e soluzioni. La ricerca sui cia è ancora agli inizi ma, sulla base delle precedenti analisi, si possono delineare cinque dimensioni della futura ricerca sui cia.

			8.5.1 Aree dei cia

			Per comprendere meglio le aree dei cia è necessario ampliare le conoscenze attuali, in particolare per quanto riguarda l’uso dell’ia negli interrogatori, i furti e le frodi negli spazi virtuali, per esempio i giochi online con beni immateriali che hanno un valore reale; e gli aa che commettono una manipolazione del mercato emergente, che la ricerca ha apparentemente studiato soltanto in simulazioni sperimentali. Gli attacchi di ingegneria sociale sono una preoccupazione plausibile, ma per il momento non ci sono ancora prove sufficienti di casi occorsi nel mondo reale. Omicidio e terrorismo sembrano essere particolarmente assenti nella letteratura sui cia, sebbene richiedano attenzione in considerazione delle tecnologie alimentate dall’ia come il riconoscimento di schemi ricorrenti (per esempio, per identificare e manipolare potenziali autori, o quando membri di gruppi vulnerabili sono ingiustamente presi di mira come sospetti), i droni armati e i veicoli a guida autonoma, dal momento che tutto ciò può avere usi sia legali sia criminali.

			8.5.2 Uso duplice

			La natura digitale dell’ia facilita il suo uso duplice (Moor, 1985; Floridi, 2010a), rendendo possibile che applicazioni progettate per usi legittimi possano poi essere implementate per commettere reati. Questo è, per esempio, il caso dei vse. Quanto più l’ia è sviluppata e le sue implementazioni diffuse, tanto maggiore è il rischio di usi dannosi o criminali. Se non sono affrontati, tali rischi possono condurre a un sentimento di rifiuto da parte della società e a una regolamentazione eccessivamente rigorosa delle tecnologie basate sull’ia. A loro volta, i vantaggi tecnologici per gli individui e le società possono essere erosi dal momento che l’uso e lo sviluppo dell’ia vengono sempre più limitati (Floridi, Taddeo, 2016). Tali limiti sono già stati previsti per la ricerca di ml su elementi visuali di discriminazione di omosessuali ed eterosessuali (Wang, Kosinski 2018), che è stata considerata troppo pericolosa per essere rilasciata integralmente (cioè con il codice sorgente e le strutture di dati apprese) nelle mani della più ampia comunità di ricerca, a scapito della sua riproducibilità scientifica. Anche quando queste costose limitazioni al rilascio dell’ia non sono necessarie, come Adobe ha dimostrato incorporando filigrane elettroniche nella tecnologia di riproduzione vocale, sviluppatori esterni e malintenzionati potrebbero comunque riprodurre la tecnologia in futuro. Anticipare il duplice uso dell’ia al di là delle tecniche generali e l’efficacia delle politiche per limitare il rilascio delle tecnologie di ia richiede ulteriori ricerche. Questo è in particolare il caso dell’implementazione dell’ia per la cybersicurezza.

			8.5.3 Sicurezza

			La letteratura sui cia mostra che, all’interno della sfera della cybersicurezza, l’ia sta assumendo un ruolo malevolo e offensivo, in parallelo con lo sviluppo e l’impiego di sistemi difensivi di ia per migliorarne la resilienza (negli attacchi duraturi) e la robustezza (nell’evitare gli attacchi), e per contrastare le minacce man mano che emergono (Yang, Bellingham, Dupont et al., 2018). La grande sfida cyber del darpa12 del 2016 è stata un punto di svolta per dimostrare l’efficacia di un approccio combinato offensivo e difensivo di ia, con sette sistemi di ia che si sono dimostrati in grado di identificare e correggere le proprie vulnerabilità, sondando e sfruttando anche quelle dei sistemi concorrenti. Successivamente, ibm ha lanciato il Cognitive soc (“Cognitive Security – Watson for Cyber Security ibm”, 2018). Si tratta di un’applicazione di un algoritmo di ml che utilizza i dati di sicurezza strutturati e non strutturati di un’organizzazione, “compreso il linguaggio umano impreciso contenuto in blog, articoli, report”, per elaborare informazioni su temi e minacce riguardo alla sicurezza, con l’obiettivo di migliorare il modo di identificare, mitigare e rispondere alle minacce. Naturalmente, mentre le politiche svolgeranno ovviamente un ruolo chiave nel mitigare e porre rimedio ai rischi del duplice uso nella fase di implementazione (per esempio, definendo meccanismi di supervisione), è nella fase di progettazione che questi rischi vengono affrontati adeguatamente. Tuttavia, contrariamente ai recenti rapporti sull’ia dannosa come Brundage e coautori (2018), che suggeriscono che “una delle migliori speranze che abbiamo per difenderci dall’hackeraggio automatizzato consiste anche nell’utilizzo dell’ia” (p. 65), l’analisi sviluppata in questo capitolo indica come anche un eccessivo affidamento sull’ia possa essere controproducente. Tutto ciò evidenzia la necessità di ulteriori ricerche sull’ia nella cybersicurezza, ma anche sulle alternative all’ia, come i fattori individuali e sociali.

			8.5.4 Persone

			Sebbene l’attuale dibattito sollevi la possibilità di fattori psicologici, come la fiducia, nel ruolo criminale dell’ia, mancano ancora ricerche sui fattori personali che potrebbero creare autori del reato, come programmatori e utenti di ia per i cia, in futuro. Ora è il momento di investire in studi longitudinali e analisi multivariate che abbraccino i contesti educativi, geografici e culturali delle vittime e degli autori di reato o anche di sviluppatori benevoli di ia, e che contribuiranno a prevedere come gli individui si uniscono per commettere i cia. Per esempio, comprendere l’efficacia dei corsi di etica nei programmi di informatica e la capacità di educare gli utenti a essere meno fiduciosi nei confronti di agenti potenzialmente guidati dall’ia nel cyberspazio.

			8.5.5 Organizzazioni

			Già nel 2017 la relazione quadriennale di Europol su gravità e organizzazione delle minacce della criminalità ha evidenziato come la tipologia di reato tecnologico tenda a correlarsi con particolari topologie di organizzazione criminale. La letteratura sui cia indica che l’ia può svolgere un ruolo nelle organizzazioni criminali come i cartelli della droga, che dispongono di risorse adeguate e sono altamente organizzate. Al contrario, l’organizzazione criminale ad hoc sul dark web ha già luogo nelle forme di ciò che Europol definisce nei termini di crimine-come-servizio. Tali servizi criminali sono venduti direttamente tra acquirente e venditore, potenzialmente come elemento circoscritto all’interno di un crimine complessivo, che l’ia potrebbe alimentare (per esempio, rendendo possibile l’hackeraggio del profilo) in futuro.13 Nello spettro che va dalle organizzazioni di cia fortemente strutturate a quelle più fluide, esistono molte possibilità di interazione criminale. Identificare le organizzazioni essenziali o che appaiono correlate con le differenti tipologie di cia consentirà di comprendere meglio come i cia siano strutturati e operino in pratica. Sviluppare la nostra comprensione di queste quattro dimensioni è cruciale se vogliamo tracciare e interrompere con successo l’inevitabile crescita futura dei cia. Si spera che King e coautori (2019) e questo capitolo stimoleranno ulteriori ricerche sulle preoccupazioni molto serie e crescenti, ma ancora relativamente inesplorate, relative ai cia. Quanto prima comprenderemo questo nuovo fenomeno criminale, tanto prima saremo in grado di mettere in atto politiche di prevenzione, mitigazione, disincentivazione e riparazione.





8.6 Conclusione: dagli usi malvagi dell’ia all’ia socialmente buona


			In questo capitolo, ho fornito un’analisi sistematica dei crimini di ia (cia), per comprendere le minacce sostanzialmente specifiche e attuabili poste dai cia. Ho affrontato tale argomento sulla base della classica definizione controfattuale di ia discussa nel primo capitolo e alla luce dell’ipotesi che l’ia introduca una nuova forma dell’agire, e non di intelligenza. Perciò, mi sono concentrato sull’ia come riserva di capacità di agire smart e autonomo, come illustrato nella prima parte del libro, e sulla responsabilità di fondo degli esseri umani nello sfruttamento di questa forma di agire artificiale in modo contrario all’etica e al diritto. Ho descritto tali minacce area per area – in termini di crimini specifici – e, più in generale, in termini di qualità dell’ia e di questioni di emergenza, responsabilità, monitoraggio e psicologia. Ho dunque esaminato quali soluzioni siano disponibili o possano essere elaborate per contrastare i cia. Ho affrontato questo argomento concentrandomi su temi sia generali sia trasversali, fornendo un quadro aggiornato delle soluzioni sociali, tecnologiche e giuridiche esistenti e dei loro limiti. Con ciò si conclude l’analisi degli usi criminali o “cattivi” dell’ia. Ora possiamo dedicarci a qualcosa di molto più positivo e costruttivo, gli usi socialmente buoni dell’ia, che costituiscono il tema del prossimo capitolo.



* * *





			 				 					1. Poiché gran parte dell’ia è alimentata dai dati, alcune delle sue sfide sono radicate nella governance dei dati (Cath, Glorioso, Taddeo 2017; Roberts, Cowls, Morley et al., 2021), in particolare questioni di consenso, discriminazione, equità, proprietà, privacy, sorveglianza e fiducia (Floridi, Taddeo, 2016).



				 					2. Per maggiori dettagli sulla metodologia e l’analisi quantitativa il lettore può consultare King, Aggarwal, Taddeo et al. (2019). Qui riassumo brevemente solo le informazioni utili per comprendere il resto del capitolo.



				 					3. Tuttavia, il ruolo di ia non doveva essere sufficiente per il reato perché è probabile che di regola fossero necessari altri elementi tecnici e non tecnici. Per esempio, se la robotica svolge un ruolo strumentale (per esempio coinvolgendo veicoli autonomi) o causale nel crimine, allora qualsiasi sottostante componente di ia deve essere essenziale affinché il crimine sia incluso nella nostra analisi.



				 					4. Dove l’assenza di preoccupazione in letteratura e nella nostra successiva analisi non implica che questa dovrebbe essere assente negli studi relativi ai cia.



				 					5. Affermazioni contrarie possono essere respinte come mero clamore, risultato di vincoli specifici, ad hoc, o solo trucchi, vedi per esempio il chatterbot chiamato “Eugene Goostman”, https://en.wikipedia.org/wiki/Eugene_Goostman.



				 					6. Twitter, politica di impersonificazione, https://help.twitter.com/en/rules-and-policies/twitter-impersonation-policy. Consultato il 7 gennaio 2020.



				 					7. La conoscenza comune è una proprietà che si trova nella logica epistemica riguardo a una proposizione P e un insieme di agenti. P è conoscenza comune se e solo se ogni agente sa che P, ogni agente sa che gli altri agenti sanno che P, e così via. Gli agenti possono acquisire conoscenze comuni attraverso comunicazioni, che forniscono agli agenti una base razionale per agire in coordinamento (per esempio, presentandosi collettivamente a una riunione dopo la trasmissione dell’ora e del luogo della riunione).



				 					8. Europol, Valutazione delle minacce gravi e della criminalità organizzata, https://www.europol.europa.eu/socta/2017/.



				 					9. D’Arcy, Pugh, “Aumento dei pedofili arrestati per aver importato bambole sessuali realistiche di bambini”, in The Independent, 31 luglio 2017, http://www.independent.co.uk/news/uk/crime/paedophiles-uk-arress-child-sex-dolls-lifelike-border-officers-aids-silicone-amazon-ebay-online-nca-a7868686.html.



				 					10. Office for National Statistics (2016), “Criminalità in Inghilterra e Galles, anno conclusosi a giugno 2016”, Appendice Tabelle, giugno 2017, pp. 1-60, https://www.ons.gov.uk/peoplepopulationandcommunity/crimeandjustice/methodologies/crimeandjusticemethodology.



				 					11. L’aggettivo “morale” è tratto dall’opera citata, che considera il comportamento contrario all’etica come un superamento dei confini del sistema, mentre qui si tratta di atti o omissioni criminali, che possono avere una valutazione etica negativa, neutra o positiva. Usiamo “morale” per evitare di travisare l’opera citata, e non per suggerire che il diritto penale coincida con l’etica.



				 					12. darpa (Defense Advanced Research Projects Agency) è l’Agenzia statunitense per lo sviluppo dei progetti di ricerca avanzati nell’ambito della difesa. [NdT]



				 					13. A tal fine, una rapida ricerca di “Intelligenza Artificiale” su importanti mercati del dark web ha fornito un risultato negativo. Nello specifico, durante lo sviluppo dell’analisi presentata in King, Aggarwal, Taddeo et al. (2019) abbiamo controllato: “Dream Market”, “Silk Road 3.1” e “Wallstreet Market”. L’esito negativo non è indicativo dell’assenza di cia come servizio nel dark web, che potrebbe esistere sotto altra veste o su mercati più specializzati. Per esempio, alcuni servizi offrono l’estrazione di informazioni personali dal computer di un utente e, anche se tali servizi sono autentici, la tecnologia sottostante (per esempio il riconoscimento di modelli alimentato dall’ia) resta sconosciuta.





9


			Buone pratiche: l’uso dell’ia per il bene sociale

			Sommario Nel precedente capitolo ho passato in rassegna le principali problematiche relative all’uso illegale dell’ia, quella che potrebbe definirsi come l’ia per il male sociale. In questo capitolo mi concentro sull’ia per il bene sociale (ai4sg).1 L’ai4sg sta guadagnando terreno all’interno delle società dell’informazione in generale e della comunità di ia in particolare. Ha il potenziale per affrontare i problemi sociali attraverso lo sviluppo di soluzioni basate sull’ia. Tuttavia, a oggi, c’è solo una comprensione limitata di ciò che rende l’ia socialmente buona nella teoria, di ciò che conta come ai4sg nella pratica e di come riprodurre i suoi successi iniziali in termini di politiche. Questo capitolo affronta tale lacuna, offrendo una definizione di ai4sg e identificando, in seguito, sette fattori etici essenziali per le future iniziative di ai4sg. L’analisi è supportata da alcuni casi esemplificativi di progetti di ai4sg, tema su cui tornerò nel tredicesimo capitolo, quando discuterò dell’uso dell’ia a sostegno degli obiettivi di sviluppo sostenibile delle Nazioni Unite. Alcuni dei fattori esaminati in questo capitolo sono quasi del tutto nuovi per l’ia, mentre l’importanza di altri fattori è accresciuta dall’uso dell’ia. In relazione a ciascuno di questi fattori, sono formulate le migliori pratiche corrispondenti che, contestualizzate e bilanciate, possono fungere da linee guida preliminari per assicurare che un’ia ben disegnata abbia maggiori probabilità di essere posta al servizio del bene sociale.





9.1 Introduzione: l’idea di ia per il bene sociale


			L’idea di “intelligenza artificiale per il bene sociale” (d’ora in poi ai4sg) sta diventando popolare in molte società dell’informazione e sta guadagnando terreno nella comunità di ia (Hager, Drobnis, Fang et al., 2019). I progetti che cercano di usare l’ia per il bene sociale variano in modo significativo. Si va dai modelli per prevedere lo shock settico (Henry, Hager, Pronovost et al., 2015) ai modelli della teoria dei giochi per prevenire la caccia di frodo (Fang, Nguyen, Pickles et al., 2016); dall’apprendimento per rinforzo online al fine di indirizzare i giovani senzatetto all’educazione sull’aids (Yadav, Chan, Xin Jiang et al., 2016b) ai modelli probabilistici per prevenire azioni di polizia dannose (Carton, Helsby, Joseph et al., 2016) o per contrastare la dispersione scolastica (Lakkaraju, Aguiar, Shan et al., 2015). Pressoché quotidianamente, infatti, compaiono nuove applicazioni di ai4sg, che rendono possibile e facilitano il raggiungimento di risultati socialmente positivi prima irrealizzabili, inaccessibili o semplicemente meno fattibili in termini di efficienza ed efficacia. L’ai4sg offre opportunità senza precedenti in molti ambiti e potrebbe rivelarsi di grande importanza, in un momento in cui i problemi sono sempre più globali, complessi e interconnessi. Per esempio, l’ia può fornire un sostegno estremamente necessario per migliorare i risultati in ambito sanitario e mitigare i rischi ambientali (Wang, Khosla, Gargeya et al., 2016; Davenport, Kalakota, 2019; Puaschunder, 2020; Rolnick, Donti, Kaack et al., 2019; Luccioni, Schmidt, Vardanyan et al., 2021; Zhou, Wang, Tang et al., 2020). Anche questa è una questione di sinergie: l’ai4sg si basa su altri recenti esempi (incrementandoli) di tecnologie digitali adottate per promuovere obiettivi socialmente benefici, come i “big data per lo sviluppo” (Hilbert, 2016; Taylor, Schroeder, 2015). Di conseguenza, l’ai4sg sta facendo presa all’interno della comunità di ia e nei luoghi di elaborazione delle strategie politiche.

			Forse a causa della sua novità e della sua rapida crescita, l’ai4sg è ancora poco compresa come fenomeno globale e manca di un quadro convincente per valutare il valore e il successo di progetti rilevanti. Chiaramente, le metriche esistenti, come la redditività o la produttività commerciale, misurano bene la domanda nel mondo reale, ma rimangono inadeguate. L’ai4sg deve essere valutata rispetto a risultati socialmente validi, proprio come accade per la certificazione “B Corporation” nel contesto for-profit (a scopo di lucro) o per le imprese sociali che operano nel settore non-profit. L’ai4sg dovrebbe essere valutata adottando metriche basate sul benessere umano e ambientale, opposte a quelle finanziarie.

			Di recente, sono emersi quadri di riferimento per la progettazione, lo sviluppo e l’implementazione dell’“ia etica” in generale (vedi capitolo 4), che offrono alcune linee guida al riguardo. Tuttavia, i guardrail etici e sociali attorno alle applicazioni dell’ia che sono esplicitamente orientate verso risultati socialmente buoni sono solo parzialmente definiti. Ciò è dovuto al fatto che, a oggi, vi è una comprensione limitata di ciò che costituisce l’ai4sg (Taddeo, Floridi, 2018a; Vinuesa, Azizpour, Leite et al., 2020; Chui, Manyika, Miremadi et al., 2018) e di quale sarebbe un criterio di valutazione affidabile per misurarne il successo. I migliori sforzi, in particolare il vertice annuale dell’Unione internazionale delle telecomunicazioni (itu) sull’ia per il bene e il database del progetto associato (“ai for Good Global Summit”, 2019;2 “Repository ai”, 20183), si concentrano sulla raccolta di informazioni e sulla descrizione di casi di ai4sg, ma ignorano gli approcci normativi a questo fenomeno e non sono diretti a offrire un’analisi sistematica. In questo capitolo (e nel tredicesimo) intendo colmare tale lacuna, formalizzando una definizione delle iniziative di ai4sg e dei fattori che le caratterizzano. Nel tredicesimo capitolo, tornerò su questo punto per sostenere che i 17 obiettivi di sviluppo sostenibile (oss) delle Nazioni Unite forniscono un valido quadro di riferimento per mettere a confronto gli usi socialmente buoni delle tecnologie di ia. In quel capitolo, approfondisco l’analisi fornita qui, introduco un database di progetti di ai4sg raccolti utilizzando tale criterio di valutazione ed esamino diverse intuizioni chiave, inclusa la misura in cui sono affrontati i vari oss. Rimanderò questi compiti alla fine del libro perché l’uso dell’ia a sostegno degli oss delle Nazioni Unite fornisce una conclusione preziosa e una via da seguire per quanto riguarda lo sviluppo futuro dell’ia.

			Affrontare l’ai4sg ad hoc, analizzando aree di applicazione specifiche, come il soccorso in caso di carestia o la gestione dei disastri, come hanno fatto i vertici annuali per il governo e l’industria dell’ia,4 è indice della presenza di un fenomeno, ma non lo spiega, né suggerisce come altre soluzioni di ai4sg potrebbero e dovrebbero essere disegnate per sfruttare appieno il potenziale dell’ia. Inoltre, molti progetti che generano risultati socialmente buoni avvalendosi dell’ia non si (auto)descrivono in questi termini (Moore, 2019). Tali carenze sollevano almeno due rischi principali: fallimenti imprevisti e opportunità mancate.

			Consideriamo prima i fallimenti imprevisti. Come qualsiasi altra tecnologia, le soluzioni di ia sono modellate da valori umani. Tali valori, se non sono accuratamente selezionati e promossi, possono generare scenari di “buona ia andata storta”. L’ia può “fare più male che bene”, laddove amplifica invece di mitigare i mali della società, per esempio ampliando anziché restringendo le disuguaglianze esistenti o esacerbando i problemi ambientali. L’ia può semplicemente non essere in grado di servire il bene sociale. Per esempio, si consideri il fallimento del software di supporto oncologico di ibm, che ha tentato di utilizzare il ml per identificare i tumori cancerosi. Il sistema è stato addestrato utilizzando dati sintetici e protocolli medici statunitensi, che non sono applicabili in tutto il mondo. Di conseguenza, ha faticato a interpretare le cartelle cliniche ambigue, sfumate o altrimenti “disordinate” dei pazienti (Strickland, 2019) e ha fornito diagnosi imprecise e suggerimenti terapeutici errati. Ciò ha spinto medici e ospedali a rifiutare il sistema (Ross, Swetlitz, 2017).

			Consideriamo ora le opportunità perse. Risultati socialmente buoni dell’ia possono in realtà sorgere in modo del tutto accidentale, per esempio attraverso l’applicazione fortuita di una soluzione di ia in un contesto diverso. Questo è stato il caso dell’uso di una versione diversa del sistema cognitivo di ibm di cui abbiamo appena parlato. In tal caso, il sistema Watson era stato originariamente disegnato per identificare meccanismi biologici, ma quando è stato utilizzato in aula ha ispirato studenti di ingegneria a risolvere problemi di design (Goel, Creeden, Kumble et al., 2015). In questa circostanza positiva, l’ia ha fornito una modalità di istruzione unica. Ma l’assenza di una chiara comprensione dell’ai4sg ha fatto sì che tale successo fosse rubricato come “accidentale”, e quindi impossibile da ripetere sistematicamente o su larga scala. Per ogni “successo accidentale”, ci possono essere dunque innumerevoli esempi di opportunità mancate per sfruttare i benefici dell’ia nel promuovere risultati socialmente buoni in contesti diversi, specialmente quando gli interventi basati sull’ia sono sviluppati separatamente da quelli che saranno più direttamente soggetti ai loro effetti, che questo sia definito in termini di area, per esempio residenti di una determinata regione, o di ambito, per esempio insegnanti o medici.

			Al fine di evitare fallimenti inutili e opportunità mancate, l’ai4sg trarrebbe vantaggio da un’analisi dei fattori essenziali che supportano e assicurano il design e l’implementazione di ai4sg di successo. In questo capitolo (basato su Floridi, Cowls, King et al., 2020), analizzo questi fattori. L’obiettivo non è documentare ogni singola considerazione etica per un progetto di ai4sg. Per esempio, è essenziale, e si spera di per sé evidente, che un progetto di ai4sg non debba agevolare la proliferazione di armi di distruzione di massa, imperativo che non esamino nel contesto presente (ma si veda al riguardo Taddeo, Floridi, 2018b). Allo stesso modo, è importante rilevare fin dal principio che ci sono molte circostanze in cui l’ia non è il modo più efficace per affrontare un determinato problema sociale (Abebe, Barocas, Kleinberg et al., 2020), per cui il suo intervento sarebbe ingiustificato. Ciò può essere dovuto al fatto che esistono approcci alternativi meno costosi e più efficaci (cioè “No ia per il bene sociale”) o a causa dei rischi inaccettabili che l’implementazione dell’ia potrebbe generare (per esempio, “ia per un bene sociale insufficiente” se paragonato ai rischi). Ecco perché l’uso del termine “bene” per descrivere tali sforzi è stato esso stesso sottoposto a critica (Green, 2019). In effetti, l’ia non è una panacea e non dovrebbe essere trattata come una soluzione che possa risolvere da sola un problema sociale complesso, ovvero è improbabile che “Solo l’ia per il bene sociale” funzioni. Ciò che è essenziale riguardo ai fattori e alle relative migliori pratiche non è la loro incorporazione in ogni circostanza. Farò notare infatti diversi esempi in cui sarebbe moralmente difendibile la scelta di non incorporare un determinato fattore. Invece, ciò che è essenziale è che ogni migliore pratica sia: (i) considerata in modo proattivo e (ii) non incorporata se e soltanto se esiste una ragione chiara, dimostrabile e moralmente difendibile per cui non dovrebbe esserlo.

			Sulla base delle precedenti precisazioni, nel resto del capitolo mi concentrerò sull’identificazione di fattori particolarmente rilevanti per l’ia come infrastruttura tecnologica, nella misura in cui viene disegnata e utilizzata per il progresso del bene sociale. Per anticipare, tali fattori sono i seguenti:

			1.	falsificabilità e implementazione incrementale;

			2.	garanzie contro la manipolazione dei predittori;

			3.	intervento contestualizzato in ragione del destinatario;

			4.	spiegazione contestualizzata in ragione del destinatario e finalità trasparenti;

			5.	tutela della privacy e consenso dell’interessato;

			6.	equità concreta;

			7.	semantizzazione adatta all’umano.

			Una volta identificati questi fattori, le domande che possono formularsi sono a loro volta le seguenti: in che modo questi fattori dovrebbero essere valutati e trattati, da chi e con quale meccanismo di sostegno, per esempio attraverso regolamenti o codici di condotta. Tali domande, che non rientrano nell’ambito di questo capitolo (vedi capitoli 4-5 e 11), sono connesse a questioni etiche e politiche più ampie riguardanti la legittimità del processo decisionale guidato dall’ia o che concerne l’ia.

			Il resto del capitolo è strutturato come segue. Nel secondo paragrafo, spiego come abbiamo identificato i sette fattori (Floridi, Cowls, King et al., 2020). Nel terzo paragrafo, analizzo singolarmente i sette fattori. Illustro ciascuno di essi facendo riferimento a uno o più casi di studio e traggo da ciascun fattore una corrispondente best practice che i creatori di ai4sg possono seguire. Nel paragrafo conclusivo, esamino i fattori e suggerisco come risolvere le tensioni che esistono tra loro.





9.2 Una definizione di ai4sg


			Un modo efficace per identificare e valutare i progetti di ai4sg è analizzarli sulla base dei loro risultati. Un progetto di ai4sg ha successo nella misura in cui contribuisce a ridurre, mitigare o eliminare un determinato problema sociale o ambientale, senza introdurre nuovi danni o amplificare quelli esistenti. Questa interpretazione suggerisce la seguente definizione di ia per il bene sociale (tutti gli “o” sottostanti sono “e/o” inclusivi):

			ai4sg = def. il design, lo sviluppo e l’implementazione di sistemi di ia in modo da (i) prevenire, mitigare o risolvere i problemi che incidono negativamente sulla vita umana e/o sul benessere del mondo naturale e/o (ii) consentire sviluppi preferibili dal punto di vista sociale e/o sostenibili dal punto di vista ambientale.5

			Come detto, tornerò su questa definizione nel tredicesimo capitolo per identificare i problemi effettivi che si ritiene influenzino negativamente la vita umana o il benessere dell’ambiente. Come altri ricercatori in precedenza (Vinuesa, Azizpour, Leite et al., 2020), a tal fine userò i 17 oss delle Nazioni Unite come parametro di valutazione. Nel contesto presente mi baso sulla precedente definizione per analizzare i fattori essenziali che qualificano i progetti di ai4sg di successo.

			Sulla base della definizione di ai4sg, in Floridi e coautori (2020) abbiamo esaminato un insieme di 27 progetti, ottenuti tramite una rassegna sistematica della letteratura rilevante, per identificare casi chiari e significativi di esempi di successo e insuccesso di ai4sg. Il lettore interessato ai dettagli è invitato a prendere visione dell’articolo. La rassegna ha permesso di identificare un insieme di 27 casi, tra i quali sette sono stati selezionati (vedi Tabella 9.1) come i più rappresentativi in termini di portata, varietà, impatto e per la loro potenzialità di corroborare i fattori essenziali che ritengo dovrebbero caratterizzare il design dei progetti di ai4sg.

			Tabella 9.1 Sette iniziative del più ampio campione analizzato in Cowls et al. (2019) che sono particolarmente rappresentative in termini di portata, varietà, impatto e per la loro potenzialità di evidenziare i fattori che dovrebbero caratterizzare il design dei progetti di AI4SG.

			 				 					 					 					 					 				 				 					 						 							Nome

						 						 							Riferimento

						 						 							Aree

						 						 							Fattori rilevanti



					 						 							Ottimizzazione di dominio dell’assistente di protezione per la sicurezza della fauna selvatica

						 						 							(Fang et al., 2016)

						 						 							Sostenibilità ambientale

						 						 							1, 3



					 						 							Identificazione degli studenti a rischio di esiti accademici negativi

						 						 							(Lakkaraju et al., 2015)

						 						 							Formazione scolastica

						 						 							4



					 						 							Informazioni sanitarie per i giovani senzatetto per ridurre la diffusione dell’AIDS

						 						 							(Yadav et al., 2016b; Yadav et al., 2018)

						 						 							Povertà, benessere pubblico, salute pubblica

						 						 							4



					 						 							Riconoscimento di attività interattive e suggerimenti per assistere le persone con disabilità cognitive

						 						 							(Chu et al., 2012)

						 						 							Disabilità, salute pubblica

						 						 							3, 4, 7



					 						 							Esperimento di assistente didattico virtuale

						 						 							(Eicher, Polepeddi, Goel, 2017)

						 						 							Formazione scolastica

						 						 							4, 6



					 						 							Rilevamento delle frodi sui rendiconti finanziari evolutivi

						 						 							(Zhou, Kapoor, 2011)

						 						 							Finanza, criminalità

						 						 							2



					 						 							Tracciamento e monitoraggio della conformità all’igiene delle mani

						 						 							(Haque et al., 2017)

						 						 							Salute

						 						 							5



				 			 			Come risulterà chiaro nel resto del capitolo, i sette fattori sono stati individuati in consonanza con l’analisi di carattere più generale svolta nell’ambito dell’etica dell’ia. Ciascun fattore si riferisce, infatti, ad almeno uno dei cinque principi etici dell’ia – beneficenza, non maleficenza, giustizia, autonomia e spiegabilità – individuati nell’analisi comparativa presentata nel quarto capitolo. Questa coerenza è cruciale: l’ai4sg non può essere incoerente con il quadro etico che guida il design e la valutazione dell’ia in generale. Il principio di beneficenza è di particolare importanza. Afferma che l’uso dell’ia dovrebbe fornire benefici alle persone (preferibilità sociale) e al mondo naturale (sostenibilità); difatti, i progetti di ai4sg non dovrebbero semplicemente rispettare ma concretizzare questo principio, in modo tale che i benefici di ai4sg dovrebbero essere preferibili (equi, vedi Figura 6.6) e sostenibili, in accordo con la definizione di cui sopra. La beneficenza è quindi una condizione necessaria dell’ai4sg, ma insufficiente, anche perché l’impatto benefico di un progetto di ai4sg può essere “compensato” dalla creazione o dall’amplificazione di altri rischi o danni.6 Inoltre, mentre taluni di questi principi etici, come l’autonomia e la spiegabilità, ricorrono nel corso del dibattito, i fattori evidenziati di seguito sono più strettamente associati a considerazioni di design inerenti all’ai4sg e possono essere attuati attraverso le corrispondenti buone pratiche fornite per ciascun fattore. In tal modo, l’analisi etica che informa il design e l’implementazione delle iniziative di ai4sg riveste un ruolo centrale nel mitigare i rischi prevedibili di conseguenze indesiderate e possibili abusi della tecnologia.

			Prima di analizzare i fattori, è importante chiarire tre caratteristiche generali dell’intero insieme: dipendenza, ordine e coerenza. I sette fattori sono spesso connessi e co-dipendenti, ma per semplicità li tratterò separatamente. Nulla dovrebbe essere dedotto da questa scelta. Allo stesso modo, i fattori sono tutti essenziali, nessuno di loro è “più importante” di un altro, quindi li introdurrò non in termini di priorità, ma per così dire storicamente, partendo dai fattori che precedono l’ia, e che tuttavia assumono maggiore importanza quando sono utilizzate tecnologie di ia, proprio in conseguenza della peculiarità delle capacità e dei rischi dell’ia (Yang, Bellingham, Dupont et al., 2018).7 Questi includono la falsificabilità e la diffusione incrementale e le garanzie contro la manipolazione dei dati. Ci sono anche fattori che fanno riferimento più intrinsecamente alle caratteristiche sociotecniche dell’ia così come è concepita oggi, come l’equità concreta e la semantizzazione adatta all’umano.

			I fattori sono eticamente robusti e pragmaticamente applicabili, nel senso che danno luogo a considerazioni di design sotto forma di buone pratiche che dovrebbero essere eticamente promosse. È fondamentale sottolineare qui che i sette fattori non sono di per sé sufficienti per un’ia socialmente buona, ma è necessario considerare attentamente ciascuno di essi. Perciò, l’insieme di fattori individuati in questo capitolo non dovrebbe essere considerato una “lista di controllo” che, se meramente rispettata, garantisce risultati socialmente buoni grazie all’uso dell’ia in un particolare ambito. Allo stesso modo, è necessario trovare un equilibrio tra i diversi fattori, e tra le tensioni che possono sorgere anche all’interno di un singolo fattore. Ne consegue che cercare di inquadrare un progetto come “diretto al bene sociale” o “non diretto al bene sociale” in modo binario sembra inutilmente riduttivo, per non dire soggettivo. Lo scopo di questo capitolo non è quello di identificare o di offrire i mezzi per identificare i progetti di ai4sg, ma quello di mostrare le caratteristiche eticamente rilevanti dei progetti che potrebbero essere descritti in termini di ai4sg.





9.3 Sette fattori essenziali per il successo dell’ai4sg


			Come anticipato, i fattori sono: (1) falsificabilità e implementazione incrementale; (2) garanzie contro la manipolazione dei predittori; (3) intervento contestualizzato in ragione del destinatario; (4) spiegazione contestualizzata in ragione del destinatario e finalità trasparenti; (5) tutela della privacy e consenso dell’interessato; (6) equità concreta; e (7) semantizzazione adatta all’umano. In questo paragrafo, chiarirò ciascun fattore separatamente, con uno o più esempi, e offrirò una corrispondente best practice.

			9.3.1 Falsificabilità e implementazione incrementale

			L’affidabilità è essenziale affinché la tecnologia in generale (Taddeo, 2009, 2010; Taddeo, Floridi, 2011; Taddeo, 2017c; Taddeo, McCutcheon, Floridi, 2019) e le applicazioni di ai4sg in particolare siano adottate e abbiano un significativo impatto positivo sulla vita umana e sul benessere ambientale. L’affidabilità di un’applicazione di ia comporta un’elevata probabilità che l’applicazione rispetti il principio di beneficenza, o quantomeno il principio di non maleficenza. Sebbene non esistano regole o linee guida universali che possano assicurare o garantire l’affidabilità, la falsificabilità è un fattore cruciale per migliorare l’affidabilità delle applicazioni tecnologiche in generale e di quelle di ai4sg in particolare.

			La falsificabilità implica la specificazione, e la possibilità di verifica empirica, di uno o più requisiti critici, cioè di una condizione, risorsa o mezzo necessari affinché una capacità sia pienamente operativa, di modo tale che qualcosa non potrebbe o dovrebbe funzionare senza di essa. La sicurezza è un requisito critico ovvio. Dunque, affinché un sistema di ai4sg sia affidabile, la sua sicurezza dovrebbe essere falsificabile.8 Se la falsificabilità non è possibile, i requisiti critici non possono essere verificati e quindi il sistema non dovrebbe essere considerato affidabile. Ecco perché la falsificabilità è un fattore cruciale per tutti i progetti di ai4sg immaginabili.

			Sfortunatamente, non possiamo sapere con certezza che una data applicazione di ai4sg sia sicura a meno di poterla testare in tutti i possibili contesti. In questo caso, la mappa dei test equivarrebbe semplicemente all’ambito di implementazione. Come chiarisce questa reductio ad absurdum, la certezza assoluta è fuori portata. Ciò che è invece a portata di mano, in un mondo incerto e caotico con molte situazioni impreviste, è la possibilità di sapere quando un determinato requisito critico non viene implementato o potrebbe non funzionare correttamente. Pertanto, se i requisiti critici sono falsificabili, possiamo sapere quando l’applicazione di ai4sg non è affidabile, ma non se è affidabile.

			I requisiti critici dovrebbero essere testati con un ciclo di implementazione incrementale. Effetti pericolosi inintenzionali possono manifestarsi solo a seguito dei test. Al contempo, il software dovrebbe essere testato nel mondo reale, se è sicuro farlo. Ciò richiede l’adozione di un ciclo di implementazione in base al quale gli sviluppatori: (a) assicurano che i presupposti o i requisiti più critici dell’applicazione siano falsificabili, (b) effettuano test di verifica delle ipotesi relative ai presupposti e requisiti più critici in contesti sicuri e protetti e, se tali ipotesi non risultano confutate in relazione a un piccolo insieme di contesti idonei, (c) conducono dunque test in contesti sempre più ampi e/o controllano un insieme più esteso di requisiti meno critici, e tutto ciò (d) essendo pronti a interrompere o modificare l’implementazione non appena possono manifestarsi effetti pericolosi o indesiderati.

			Le applicazioni di ai4sg possono utilizzare approcci formali per provare a testare requisiti critici. Per esempio, possono includere l’uso di una verifica formale per assicurare che veicoli autonomi e sistemi di ia in altri contesti critici per la sicurezza compiano la scelta eticamente preferibile (Dennis, Fisher, Slavkovik et al., 2016). Tali metodi offrono controlli di sicurezza che, in termini di falsificabilità, possono essere dimostrati corretti. Le simulazioni possono fornire garanzie più o meno simili. Una simulazione consente di verificare se i requisiti critici (pensiamo di nuovo alla sicurezza) sono soddisfatti in base a una serie di ipotesi formali. A differenza di una dimostrazione formale, una simulazione non può indicare che le proprietà richieste siano sempre necessariamente soddisfatte. Spesso, però, una simulazione consente di testare un insieme molto più ampio di casi che non possono essere trattati formalmente, per esempio a causa della complessità della dimostrazione.

			Sarebbe sbagliato affidarsi esclusivamente a proprietà formali o a simulazioni per falsificare un’applicazione di ai4sg. Le ipotesi di questi modelli imprigionano l’applicabilità nel mondo reale di qualsiasi conclusione si possa trarre. E le ipotesi possono risultare errate nella realtà. Ciò che si può dimostrare corretto tramite una prova formale, o probabilmente corretto tramite una verifica condotta per mezzo di una simulazione, può essere smentito in seguito con l’implementazione del sistema nel mondo reale. Per esempio, gli sviluppatori di un modello di teoria dei giochi per la sicurezza degli animali selvatici hanno ipotizzato una topografia relativamente pianeggiante priva di grossi ostacoli. Perciò, il software sviluppato originariamente includeva una definizione errata del percorso di pattugliamento ottimale. I test incrementali dell’applicazione hanno consentito di affinare il percorso di pattugliamento ottimale smentendo l’ipotesi di una topografia pianeggiante (Fang, Nguyen, Pickles et al., 2016).

			Se nuovi dilemmi in contesti del mondo reale richiedono di modificare le ipotesi precedentemente formulate in laboratorio, una soluzione consiste nel rettificare le ipotesi a priori dopo l’implementazione. In alternativa, si può adottare un sistema “al volo” o in corso di esecuzione per l’aggiornamento costante dell’elaborazione (“comprensione”) di un programma dei suoi input. Tuttavia, vi sono numerosi problemi anche con questo approccio. Per esempio, il famigerato bot Twitter di Microsoft, Tay, già esaminato nel quinto capitolo, ha acquisito significati, in un senso molto generico, in fase di esecuzione, poiché ha appreso dagli utenti di Twitter come rispondere ai tweet. Dopo l’attivazione nel mondo reale, e spesso aggressivo, dei social media, tuttavia, la capacità del bot di adattare costantemente la sua “comprensione concettuale” è diventata uno sfortunato bug, poiché Tay ha “appreso” e rigurgitato un linguaggio offensivo e associazioni immorali tra i concetti di altri utenti (Neff, Nagy, 2016b).

			L’uso di un approccio retrodittivo, vale a dire il tentativo di comprendere taluni aspetti della realtà attraverso informazioni a priori, per affrontare la falsificabilità dei requisiti presenta problemi simili. Ciò è degno di nota, dal momento che la retrodizione è il metodo principale degli approcci di ml supervisionati che apprendono dai dati (per esempio, l’apprendimento di una funzione di trasformazione continua nel caso delle reti neurali).

			Dall’analisi precedente discende che il fattore essenziale di falsificabilità e di implementazione incrementale comprende un ciclo: requisiti ingegneristici falsificabili (cosicché sia almeno possibile sapere se i requisiti non sono soddisfatti); test di falsificazione per migliorare progressivamente i livelli di affidabilità; correzione delle ipotesi a priori; allora e soltanto allora implementazione in un contesto sempre più ampio e critico. L’approccio tedesco alla regolamentazione dei veicoli autonomi, già menzionato nell’ottavo capitolo, offre un buon esempio di questo approccio incrementale. Zone deregolamentate consentono la sperimentazione di forme di limitata autonomia e, dopo aver aumentato i livelli di affidabilità, i costruttori possono testare veicoli con livelli di autonomia più elevati (Pagallo, 2017). In effetti, la creazione di queste zone deregolamentate, o Teststrecken, è raccomandata a livello europeo per promuovere politiche di ia più etiche, come vedremo nel decimo capitolo. L’identificazione di questo fattore essenziale produce la seguente best practice:

			1.	I progettisti di ai4sg dovrebbero identificare i requisiti falsificabili e testarli in fasi incrementali dal laboratorio al “mondo esterno”.

			9.3.2 Garanzie contro la manipolazione dei predittori

			L’uso dell’ia per prevedere tendenze o pattern futuri è molto popolare nei contesti di ai4sg, dall’applicazione della previsione automatizzata per porre rimedio agli insuccessi accademici (Lakkaraju, Aguiar, Shan et al., 2015), alla prevenzione delle attività di polizia illegali (Carton, Helsby, Joseph et al., 2016), fino all’individuazione di frodi aziendali (Zhou, Kapoor, 2011). Il potere predittivo dell’ai4sg affronta due rischi: la manipolazione dei dati di input e l’eccessiva dipendenza da indicatori non causali.

			La manipolazione dei dati non è un problema nuovo né è circoscritto ai soli sistemi di ia. Risultati ben consolidati come la legge di Goodhart (Goodhart, 1984), per la quale, in sintesi, “quando una misura diventa un obiettivo, cessa di essere una buona misura” (Strathern, 1997, p. 308), precedono di gran lunga l’adozione diffusa di sistemi di ia. Ma nel caso dell’ia, il problema della manipolazione dei dati può essere esacerbato (Manheim, Garrabrant, 2018) e condurre a esiti ingiusti che violano il principio di giustizia. In quanto tale, è un rischio degno di attenzione per qualsiasi iniziativa di ai4sg, perché può compromettere il potere predittivo dell’ia e spingere a evitare interventi socialmente buoni a livello individuale. Si consideri la preoccupazione sollevata da Ghani per gli insegnanti che devono essere valutati rispetto

			alla percentuale di studenti della propria classe che supera una certa soglia di rischio. Se il modello fosse trasparente (per esempio, dipendesse fortemente dalla media dei voti in matematica) l’insegnante potrebbe gonfiare i voti di matematica e ridurre i punteggi di rischio intermedio dei propri studenti. (Ghani, 2016)

			Come sostiene Ghani, la stessa preoccupazione si applica ai predittori di interazioni avverse tra agenti di polizia:

			Questi sistemi [sono] molto facili da capire e interpretare, ma ciò li rende anche facili da aggirare. Un ufficiale che ha fatto uso della forza due volte negli ultimi 80 giorni può scegliere di stare un po’ più attento nei successivi 10 giorni, finché il conteggio non torna a zero. (Ibidem)

			Questi ipotetici esempi chiariscono che, quando il modello utilizzato è facile da comprendere “sul campo”, si presta ad abusi o “manipolazioni”, indipendentemente dal fatto che sia utilizzata l’ia. L’introduzione dell’ia complica le cose, a causa della dimensione a cui l’ia viene di regola applicata.9 Come abbiamo visto, se sono note le informazioni utilizzate per prevedere un dato risultato, un agente con tali informazioni (che si prevede intraprenderà una determinata azione) può modificare il valore di ciascuna variabile predittiva per evitare un intervento. In questo modo si riduce il potere predittivo del modello complessivo, come è stato mostrato dalla ricerca empirica nel campo delle frodi aziendali (Zhou, Kapoor, 2011). Tale fenomeno potrebbe essere esteso dal rilevamento delle frodi agli ambiti che le iniziative di ai4sg cercano di affrontare, come abbiamo visto nel quinto capitolo.

			Al contempo, c’è il rischio che un’eccessiva dipendenza da indicatori non causali – cioè dati che sono correlati con, ma non causa di, un fenomeno – possa distogliere l’attenzione dal contesto in cui il designer di ai4sg sta cercando di intervenire. Per essere efficace, qualsiasi intervento di questo tipo dovrebbe modificare le cause alla base di un dato problema, come i problemi domestici di uno studente o l’inadeguata governance di un’azienda, piuttosto che i predittori non causali. Agire diversamente significa rischiare di affrontare solo un sintomo, piuttosto che la causa principale di un problema.

			Questi rischi suggeriscono la necessità di considerare l’utilizzo di garanzie come fattore di design per i progetti di ai4sg. Tali garanzie possono vincolare la selezione di indicatori da adoperare nel design dei progetti di ai4sg; la misura in cui questi indicatori dovrebbero modellare gli interventi; e/o il livello di trasparenza che dovrebbe applicarsi al modo in cui gli indicatori influenzano la decisione. Ciò produce la seguente best practice:

			2.	I designer di ai4sg dovrebbero adottare garanzie che (i) assicurino che gli indicatori non causali non distorcano in modo inappropriato gli interventi e (ii) limitino, quando appropriato, la conoscenza di come gli input influenzano gli output dei sistemi di ai4sg, per prevenire la manipolazione.

			9.3.3 Intervento contestualizzato in ragione del destinatario

			È essenziale che il software intervenga nella vita degli utenti solo in modi rispettosi della loro autonomia. Ancora una volta, non si tratta di un problema che si pone soltanto al riguardo degli interventi diretti dall’ia, ma certamente l’uso dell’ia introduce nuove considerazioni. In particolare, una sfida cruciale per i progetti di ai4sg è elaborare interventi che bilancino i benefici attuali e futuri. Il problema del bilanciamento, che è noto nelle ricerche relative alla sollecitazione delle preferenze (Boutilier, 2002; Faltings, Pu, Torrens et al., 2004; Chajewska, Koller, Parr, 2000), si riduce a un’interdipendenza temporale delle scelte. Un intervento nel presente può suscitare le preferenze dell’utente che poi consentono al software di contestualizzare gli interventi futuri nei confronti di tale utente. Di conseguenza, una strategia di intervento che non ha impatto sull’autonomia dell’utente (per esempio priva di interventi) può risultare inefficace nell’estrarre le informazioni necessarie per interventi futuri correttamente contestualizzati. Viceversa, un intervento che viola eccessivamente l’autonomia dell’utente può indurlo a rifiutare la tecnologia, rendendo impossibili interventi futuri.

			L’attenzione prestata al bilanciamento è comune per le iniziative di ai4sg. Consideriamo, per esempio, il software interattivo di riconoscimento delle attività per le persone affette da disabilità cognitive (Chu, Song, Levinson et al., 2012). Il software è disegnato per indurre i pazienti a osservare un programma giornaliero di attività (per esempio l’assunzione di farmaci), riducendo al minimo le interruzioni dei loro obiettivi più ampi. Ogni intervento è contestualizzato in modo tale che il software apprenda i tempi degli interventi futuri dalle risposte agli interventi passati. Inoltre, vengono effettuati solo interventi importanti, anche se tutti gli interventi sono parzialmente facoltativi, perché rifiutare un suggerimento porta allo stesso suggerimento in un secondo momento. C’era il timore che i pazienti rifiutassero una tecnologia eccessivamente invadente: per questo, si è cercato un punto di equilibrio. Equilibrio che manca nel nostro secondo esempio. Un’applicazione della teoria dei giochi interviene nei percorsi di pattugliamento della fauna selvatica da parte degli agenti di sicurezza suggerendo determinati percorsi (Fang, Nguyen, Pickles et al., 2016). Tuttavia, se un percorso presenta ostacoli fisici, il software non è in grado di fornire suggerimenti alternativi. Gli ufficiali possono ignorare il consiglio prendendo una strada diversa, ma non senza svincolarsi dall’applicazione. È essenziale allentare tali vincoli, cosicché gli utenti possano ignorare un intervento, ma accettare in seguito interventi più appropriati (sotto forma di consiglio).

			Questi esempi sottolineano l’importanza di considerare gli utenti come pari interlocutori sia nel design sia nell’implementazione di sistemi decisionali autonomi. L’adozione di questa mentalità avrebbe potuto contribuire a prevenire la tragica perdita di due aerei di linea Boeing 737 Max. Pare che i piloti di tali voli abbiano incontrato serie difficoltà nell’invertire il malfunzionamento del software causato da sensori difettosi, dovuto in parte all’assenza di “funzioni di sicurezza opzionali” che la Boeing ha venduto separatamente (Tabuchi, Gelles, 2019).

			Il rischio di falsi positivi (intervento non necessario, creazione di disillusione) è spesso altrettanto problematico dei falsi negativi (nessun intervento dove necessario, limitazione dell’efficacia). Per questo, un adeguato intervento contestualizzato in ragione del destinatario è quello che raggiunge il giusto livello di perturbazione, rispettando al contempo l’autonomia tramite le opzioni che offre. Questa contestualizzazione trova fondamento nelle informazioni relative a capacità, preferenze e finalità degli utenti, e nelle circostanze in cui l’intervento avrà luogo.

			Si possono considerare cinque dimensioni rilevanti per un intervento contestualizzato in ragione del destinatario. Quattro di queste emergono dalla tassonomia della ricerca interdisciplinare di McFarlane sulle dirompenti interazioni umano-computer (McFarlane, 1999; McFarlane, Latorella, 2002). Sono:

			a)	le caratteristiche individuali del destinatario dell’intervento;

			b)	le modalità di coordinamento tra destinatario e sistema;

			c)	il significato o la finalità dell’intervento;

			d)	gli effetti complessivi dell’intervento.10

			Una quinta dimensione rilevante è la possibilità di disporre di opzioni: un utente può scegliere se ignorare tutti i consigli offerti o indirizzare il processo e richiedere un intervento differente più adatto alle proprie esigenze.

			Possiamo ora riassumere queste cinque dimensioni sotto forma della seguente best practice per l’intervento contestualizzato in ragione del destinatario:

			3.	I designer di ai4sg dovrebbero costruire sistemi decisionali in dialogo con gli utenti che interagiscono con questi sistemi e ne sono influenzati; sulla base della comprensione delle caratteristiche degli utenti, delle modalità di coordinamento, delle finalità e degli effetti di un intervento; e nel rispetto del diritto degli utenti di ignorare o modificare gli interventi.

			9.3.4 Spiegazione contestualizzata in ragione del destinatario e finalità trasparenti

			Le applicazioni di ai4sg dovrebbero essere disegnate in modo tale da rendere spiegabili le operazioni e i risultati di tali sistemi e trasparenti i loro scopi. Naturalmente, questi due requisiti sono intrinsecamente collegati, poiché le operazioni e i risultati dei sistemi di ia riflettono gli scopi più ampi dei designer umani. In questo paragrafo, mi occupo di entrambi tali aspetti.

			Rendere spiegabili i sistemi di ia è un importante principio etico, come abbiamo visto nel quarto capitolo. Si tratta di un obiettivo che è stato al centro della ricerca almeno dal 1975 (Shortliffe, Buchanan, 1975). E ha riacquistato maggiore attenzione di recente (Mittelstadt, Allo, Taddeo et al., 2016; Wachter, Mittelstadt, Floridi, 2017a, 2017b; Thelisson, Padh, Celis, 2017; Watson, Floridi, 2020; Watson, Gultchin, Taly et al., 2021), con l’impiego sempre più diffuso di sistemi di ia. Come abbiamo osservato in precedenza, i progetti di ai4sg dovrebbero offrire interventi contestualizzati in ragione del destinatario. Inoltre, la spiegazione di un intervento dovrebbe essere contestualizzata in modo tale da risultare adeguata e tutelare l’autonomia del destinatario.

			I designer dei progetti di ai4sg hanno cercato di incrementare la spiegabilità dei sistemi decisionali in vari modi. Per esempio, i ricercatori hanno utilizzato il ml per prevedere le difficoltà accademiche (Lakkaraju, Aguiar, Shan et al., 2015). Questi predittori adoperavano i concetti che i funzionari scolastici che interpretavano il sistema trovavano familiari e rilevanti, come i punteggi relativi alla media dei voti e le classificazioni per categorie socioeconomiche. I ricercatori si sono anche avvalsi dell’apprendimento per rinforzo per aiutare i funzionari dei rifugi per senzatetto a fornire indicazioni sull’aids ai giovani senza fissa dimora (Yadav, Chan, Jiang et al., 2016a; Yadav, Chan, Xin Jiang et al., 2016b). Il sistema apprende come massimizzare l’influenza dell’educazione sull’aids, scegliendo quali giovani senza fissa dimora educare, sulla base del fatto che i giovani senzatetto possono trasmettere le loro conoscenze. Una versione del sistema ha spiegato quale giovane è stato scelto, rivelando il grafico della loro rete sociale. Tuttavia, i funzionari del rifugio per senzatetto hanno rilevato che tali spiegazioni erano controintuitive, condizionando potenzialmente la comprensione di come funzionasse il sistema e, quindi, la fiducia degli utenti nel sistema. Questi due casi esemplificano l’importanza di concepire in termini corretti la spiegazione di una decisione basata sull’ia.

			È probabile che il modo corretto di concepirla vari tra i progetti di ai4sg, poiché questi differiscono notevolmente quanto a obiettivi, contenuto, contesto e parti interessate. Il quadro concettuale, ossia il livello di astrazione (LdA), dipende da cosa viene spiegato, a chi e per quale scopo (Floridi, 2008b, 2017b). Un LdA è un elemento chiave di una teoria e dunque di ogni spiegazione. Una teoria comprende cinque elementi costitutivi:

			1.	un sistema, che è il referente o l’oggetto analizzato da una teoria;

			2.	uno scopo, che è il “per che cosa” motivante l’analisi di un sistema (si noti che ciò risponde alla domanda “A che cosa serve l’analisi?” e non deve essere confuso con lo scopo di un sistema, che risponde alla domanda “A che cosa serve il sistema?”. Di seguito uso il termine “obiettivo” per lo scopo del sistema ogniqualvolta ci possa essere un rischio di confusione);

			3.	un livello di astrazione, che fornisce una lente attraverso la quale un sistema viene analizzato, e che genera:

			4.	un modello, cioè alcune informazioni rilevanti e attendibili sul sistema analizzato, che identifica:

			5.	una struttura del sistema, che comprende le caratteristiche che appartengono al sistema in esame.

			Esiste un’interdipendenza tra la scelta dello scopo specifico, il LdA rilevante che può soddisfare lo scopo, il sistema analizzato e il modello ottenuto analizzando il sistema a un determinato LdA per uno scopo particolare. Il LdA fornisce la concettualizzazione del sistema (per esempio, i punteggi della media dei voti o i background socioeconomici). Lo scopo, tuttavia, limita la costruzione dei LdA. Per esempio, se scegliamo di spiegare il sistema decisionale stesso (come l’uso di particolari tecniche di ml), allora il LdA può concettualizzare solo quelle tecniche di ia. A sua volta, il LdA genera il modello che spiega il sistema. Il modello identifica le strutture del sistema, come il punteggio della media dei voti, lo scarso tasso di frequenza e il background socioeconomico di un determinato studente in quanto predittori del suo insuccesso accademico. Di conseguenza, i designer devono scegliere con attenzione lo scopo e il LdA corrispondente, in modo tale che il modello esplicativo possa fornire la corretta spiegazione del sistema in questione in relazione a un dato destinatario.

			Un LdA è scelto per uno scopo preciso: per esempio, un LdA scelto per spiegare una decisione presa sulla base dei risultati ottenuti attraverso una procedura algoritmica varia a seconda che la spiegazione sia destinata al destinatario di tale decisione o a un ingegnere responsabile del design della procedura algoritmica. Ciò è dovuto al fatto che, in ragione dello scopo e della sua granularità (per esempio, una spiegazione adatta al cliente rispetto a quella adatta a un ingegnere), non tutti i LdA sono appropriati per un dato destinatario. Talora, la visione del mondo di un destinatario può differire da quella su cui si basa la spiegazione. In altri casi, un destinatario e una spiegazione possono essere concettualmente allineati, ma il destinatario può non essere d’accordo sul livello di granularità (LdA) delle informazioni fornite (ciò che abbiamo chiamato più precisamente il modello). Il disallineamento concettuale significa che il destinatario può trovare la spiegazione irrilevante, incomprensibile o, come vedremo in seguito, discutibile. Per quanto riguarda l’(in)intelligibilità, un LdA può utilizzare qualificazioni sconosciute (i cosiddetti osservabili) o qualificazioni che hanno significati diversi per utenti diversi.

			Studi empirici (Gregor, Benbasat, 1999) suggeriscono che l’adeguatezza di una spiegazione differisce tra i destinatari in ragione della loro esperienza. I destinatari possono richiedere spiegazioni su come il software di ia ha preso una decisione, specialmente quando devono intraprendere un’azione basata su tale decisione (Gregor, Benbasat, 1999; Watson, Krutzinna, Bruce et al., 2019). Il modo in cui il sistema di ia è giunto a una conclusione può essere tanto importante quanto la giustificazione per tale conclusione. Di conseguenza, i designer devono contestualizzare anche il metodo di spiegazione in ragione del destinatario.

			Il caso del software che usa algoritmi di massimizzazione dell’influenza per indirizzare i giovani senzatetto verso l’educazione all’aids fornisce un buon esempio della rilevanza della contestualizzazione dei concetti in ragione del destinatario (Yadav, Chan, Jiang et al., 2016a; Yadav, Chan, Xin Jiang et al., 2016b). I ricercatori coinvolti in questo progetto hanno considerato tre possibili LdA quando hanno disegnato il modello di spiegazione: il primo LdA includeva calcoli dell’utilità; il secondo LdA si concentrava sulla connettività del grafo sociale; e un terzo LdA era incentrato su finalità pedagogiche. Il primo LdA ha messo in evidenza l’utilità di rivolgersi a un giovane senza fissa dimora piuttosto che a un altro. Secondo i ricercatori, in questo caso, i funzionari del rifugio per senzatetto (i destinatari) potevano aver frainteso i calcoli dell’utilità o averli trovati irrilevanti. I calcoli dell’utilità hanno un limitato potere esplicativo al di là della decisione stessa, perché spesso mostrano semplicemente che è stata fatta la scelta “migliore” e quanto è stata buona. Le spiegazioni basate sul secondo LdA hanno affrontato un problema diverso: i destinatari presumevano che i nodi più centrali della rete fossero i nodi migliori per massimizzare l’influenza dell’educazione, mentre la scelta ottimale consiste spesso in un insieme di nodi che sono collegati meno bene. Questa disgiunzione potrebbe essere derivata dalla natura della connettività tra i membri della rete di giovani senzatetto, che riflette l’incertezza nella vita reale riguardo alle amicizie. Poiché chi conta come “amico” è spesso qualcosa di vago e mutevole nel tempo, i ricercatori hanno classificato i margini della rete come “certi” o “incerti” in base alla conoscenza del dominio. Per le relazioni “incerte”, la probabilità che esistesse un’amicizia tra due giovani è stata determinata da esperti del settore.11 Alla fine è stato scelto il terzo LdA, dopo successivi test da parte degli utenti di diverse cornici esplicative. Alla luce dell’obiettivo dichiarato di giustificare le decisioni in modo intuitivo per i funzionari del rifugio per senzatetto, i ricercatori hanno ritenuto opportuno omettere i riferimenti ai calcoli dell’utilità massima attesa, anche se in realtà questo è ciò che ha fondato le decisioni prese dal sistema. Invece, i ricercatori hanno ritenuto preferibile giustificare le decisioni usando concetti più familiari e accettabili per i funzionari, come la centralità dei nodi (cioè i giovani) che il sistema ha segnalato come prioritari nell’intervento dei funzionari. In tal modo, i ricercatori hanno cercato di fornire le informazioni più rilevanti contestualizzate in ragione del destinatario.

			Come mostra questo esempio, dato un particolare sistema, lo scopo che si sceglie di perseguire quando si cerca di dare una spiegazione del sistema, lo specifico LdA e il modello derivante che si ottiene sono variabili cruciali che influiscono sull’efficacia della spiegazione. La spiegazione genera fiducia e favorisce l’adozione delle soluzioni di ai4sg (Herlocker, Konstan, Riedl, 2000; Swearingen, Sinha, 2002; Bilgic, Mooney, 2005). Questo è il motivo per cui è essenziale che il software utilizzi argomentazioni persuasive per il pubblico a cui si rivolge. Ciò comprende verosimilmente informazioni sia sul modo generale di funzionamento e sulla logica impiegata da un sistema sia sulle ragioni per cui una determinata decisione è stata presa (Wachter, Mittelstadt, Floridi, 2017b).

			Anche la trasparenza sull’obiettivo del sistema (cioè lo scopo del sistema) è cruciale, poiché deriva direttamente dal principio di autonomia. Si consideri, per esempio, lo sviluppo di soluzioni di ia per indurre le persone affette da disabilità cognitive ad assumere i propri farmaci (Chu, Song, Levinson et al., 2012). A prima vista, tale applicazione può sembrare invasiva, in quanto coinvolge utenti vulnerabili e limita l’efficacia della spiegazione concepita in ragione del destinatario. Tuttavia, il sistema non è disegnato per costringere i pazienti a tenere un determinato comportamento, né è disegnato per assomigliare a un essere umano. I pazienti hanno l’autonomia per non interagire con il sistema di ia in questione. Questo caso mostra l’importanza della trasparenza negli obiettivi, in particolare in contesti in cui la spiegazione di operazioni e risultati è impraticabile o indesiderata. La trasparenza negli obiettivi è, dunque, alla base di altre garanzie relative alla protezione di specifici gruppi di riferimento e può contribuire ad assicurare il rispetto della normativa e dei precedenti rilevanti (Reed, 2018).

			Al contrario, obiettivi opachi possono provocare incomprensioni e potenziali danni. Per esempio, quando gli utenti di un sistema di ia non percepiscono con chiarezza con quale tipo di agente hanno a che fare (umano, artificiale o una combinazione ibrida dei due) possono presumere erroneamente che le norme tacite dell’interazione sociale umana siano rispettate, per esempio, non registrando ogni dettaglio di una conversazione (Kerr, 2003). Come sempre, il contesto sociale in cui si svolge un’applicazione di ai4sg influisce sulla misura in cui i sistemi di ia dovrebbero essere trasparenti nelle loro operazioni. Poiché la trasparenza è la posizione predefinita ma non assoluta, potrebbero sussistere valide ragioni per i designer per evitare di informare gli utenti degli obiettivi del software. Per esempio, il valore scientifico di un progetto o le condizioni di salute e sicurezza di uno spazio pubblico possono giustificare temporaneamente obiettivi opachi. È il caso di uno studio che ha ingannato gli studenti facendo credere loro che stessero interagendo con l’assistente umano di un corso che, nel tempo, si è rivelato essere un bot (Eicher, Polepeddi, Goel, 2017). L’inganno relativo al bot, come sostengono gli autori, era volto a rendere possibile il “gioco dell’imitazione”, per far sì che gli studenti non formulassero domande in linguaggio naturale più semplici e meno umane, fondate su preconcetti relativi alle capacità dell’ia. In tali casi, la scelta tra opacità e trasparenza può essere informata da preesistenti basi di consenso informato per esperimenti su soggetti umani previste nel Codice di Norimberga, nella Dichiarazione di Helsinki e nel Rapporto Belmont (Nijhawan, Janodia, Krishna et al., 2013).

			Più in generale, la capacità di evitare l’utilizzo di un sistema di ia diventa più probabile quando il software di ia rivela i suoi obiettivi endogeni, come la classificazione dei dati su una persona. Per esempio, il software di ia potrebbe informare il personale in un reparto ospedaliero che ha l’obiettivo di classificare i loro livelli di igiene (Haque, Guo, Alahi et al., 2017). In tal caso, il personale può decidere di evitare queste classificazioni se esistono ragionevoli azioni alternative che possono essere intraprese. In altri casi, rivelare un obiettivo rende meno probabile che venga raggiunto.

			Rendere trasparenti gli obiettivi e le motivazioni degli stessi sviluppatori di ai4sg è un fattore cruciale per il successo di qualsiasi progetto, ma può contrastare con lo scopo stesso del sistema. Ecco perché è fondamentale valutare, in fase di design, qual è il livello di trasparenza (ossia quanta trasparenza, di che tipo, per chi e su cosa) che il progetto adotterà, dato il suo obiettivo generale e il contesto di implementazione. Unitamente all’esigenza di una spiegazione concepita in relazione al destinatario, questa considerazione produce la seguente serie di best practice:

			4.	I designer di ai4sg dovrebbero scegliere un livello di astrazione per la spiegazione dell’ia che soddisfi lo scopo esplicativo auspicato e sia appropriato al sistema e ai destinatari; quindi dovrebbero fornire argomenti che siano razionalmente e adeguatamente persuasivi affinché i destinatari forniscano la spiegazione; e assicurare che l’obiettivo (lo scopo del sistema) per cui viene sviluppato e implementato un sistema di ai4sg sia conoscibile per impostazione predefinita ai destinatari dei suoi risultati.

			9.3.5 Tutela della privacy e consenso dell’interessato

			Dei sette fattori, la privacy è quello con la letteratura più voluminosa. Ciò non dovrebbe sorprendere, poiché la privacy è considerata, tra le altre cose, una condizione essenziale per la sicurezza e la coesione sociali (Solove, 2008) e, inoltre, perché le prime ondate di tecnologia digitale hanno già avuto un notevole impatto sulla privacy (Nissenbaum, 2009). La sicurezza delle persone può essere compromessa quando uno Stato o un attore malintenzionato ottiene il controllo sugli individui tramite violazioni della privacy (Taddeo, 2014; Lynskey, 2015). Il rispetto della privacy è anche una condizione necessaria della dignità umana, poiché possiamo considerare le informazioni personali come elementi costitutivi di un individuo, cosicché la sottrazione di dati senza consenso può costituire una violazione della dignità umana (Floridi, 2016c). La concezione della privacy individuale come diritto fondamentale è alla base di recenti azioni legislative, per esempio in Europa (con il suo regolamento generale sulla protezione dei dati) e in Giappone (con la sua legge sulla protezione delle informazioni personali), nonché di decisioni giudiziarie in giurisdizioni come l’India (Mohanty, Bhatia, 2017). La privacy consente alle persone di tenere comportamenti diversi da quanto previsto dalle norme sociali senza recare offesa ad altri e alle comunità di conservare le proprie strutture sociali: per questo la privacy è anche alla base della coesione sociale.

			Nel caso dell’ai4sg, è particolarmente importante sottolineare la rilevanza del consenso degli utenti per l’utilizzo dei dati personali. Possono infatti sorgere tensioni tra diverse soglie di consenso (Price, Cohen, 2019). La tensione è spesso più acuta in situazioni di “vita o di morte” come emergenze nazionali o pandemie. Si consideri l’epidemia di Ebola in Africa occidentale nel 2014, che ha posto un complesso dilemma etico (The Economist, 2014). In tal caso, la rapida comunicazione e analisi dei dati sulle chiamate provenienti dai telefoni cellulari degli utenti nella regione interessata avrebbe potuto consentire agli epidemiologi di monitorare la diffusione della malattia mortale. Tuttavia, il rilascio dei dati è stato ritardato per valide preoccupazioni riguardanti la privacy degli utenti, nonché il valore dei dati per i concorrenti industriali. Considerazioni analoghe sono state sollevate riguardo alla pandemia da Covid-19 (Morley, Cowls, Taddeo et al., 2020).

			In circostanze in cui l’urgenza non è così pressante, è possibile ottenere il previo consenso di un soggetto all’utilizzo dei suoi dati. Il livello o il tipo di consenso richiesto può variare in ragione del contesto. In ambito sanitario, si può adottare una soglia di consenso presunto, per cui segnalare un problema medico a un dottore costituisce un presunto consenso da parte di un paziente. In altre circostanze, una soglia di consenso informato sarà più appropriata. Tuttavia, poiché il consenso informato richiede che i ricercatori ottengano il consenso specifico di un paziente prima di utilizzare i suoi dati per uno scopo non autorizzato, i professionisti possono scegliere una soglia di consenso esplicito per il trattamento generale dei dati, per esempio, per qualsiasi uso medico. Questa soglia non richiede di informare il paziente su tutte le possibili modalità con cui i ricercatori possono utilizzare i suoi dati (Etzioni, 1999). Un’alternativa ulteriore consiste nella nozione in evoluzione di “consenso dinamico”, in base al quale gli individui possono monitorare e regolare le proprie preferenze sulla privacy a livello granulare (Kaye, Whitley, Lund et al., 2015).

			In altri casi, il consenso informato può essere del tutto revocato. Tale è stato il caso della recente creazione del software di ml per prevedere la prognosi dei malati di cancro ovarico attingendo all’analisi retrospettiva di immagini anonimizzate (Lu, Arshad, Thornton et al., 2019). Anche l’uso dei dati sulla salute dei pazienti nello sviluppo di soluzioni di ia senza il consenso dei pazienti ha attirato l’attenzione delle autorità di regolazione della protezione dei dati. Nel 2017, il garante per le informazioni del Regno Unito ha stabilito che il Royal Free nhs Foundation Trust ha violato la legge sulla protezione dei dati quando ha fornito i dettagli dei pazienti a Google DeepMind, allo scopo di addestrare un sistema di ia per diagnosticare una patologia renale acuta (Burgess, 2017). Il garante ha rilevato l’esistenza di una “grave omissione” nel fatto che “i pazienti non erano stati adeguatamente informati che i loro dati sarebbero stati utilizzati come parte del test” (Information Commissioner’s Office, 2017).

			Tuttavia, è possibile trovare un equilibrio tra il rispetto della privacy del paziente e la creazione di un’ai4sg efficace. Questa è stata la sfida affrontata dai ricercatori nel lavoro di Haque e coautori (2017), che volevano creare un sistema per tracciare il rispetto delle regole sull’igiene delle mani negli ospedali, allo scopo di prevenire la diffusione delle infezioni. Nonostante i chiari vantaggi tecnici di un approccio al problema basato sulla visione artificiale, l’uso della registrazione video va contro le norme sulla privacy che lo vincolano. Anche nei casi in cui la registrazione video è consentita, l’accesso alle registrazioni (al fine di addestrare un algoritmo) è spesso rigidamente circoscritto. Invece, i ricercatori hanno fatto ricorso a “immagini di profondità”, che de-identificano i soggetti, preservandone la privacy. Anche se questa scelta di design comportava di “perdere importanti indicazioni sull’aspetto visivo nel processo”, rispettava le regole sulla privacy, e il sistema non intrusivo dei ricercatori risultava comunque avere prestazioni migliori delle soluzioni esistenti.

			Infine, problematico è anche il consenso nello spazio online. Gli utenti spesso non hanno la possibilità di scegliere e quando accedono ai servizi online sono posti di fronte all’alternativa “prendere o lasciare” (Nissenbaum, 2011; Taddeo, Floridi, 2015). La relativa mancanza di protezione o di consenso per gli usi secondari di dati personali condivisi pubblicamente online consente che si sviluppino software di ia eticamente problematici. Per esempio, un recente articolo ha utilizzato immagini pubblicamente disponibili di volti caricati su un sito di incontri come modo per addestrare un software di ia a riconoscere il genere di qualcuno sulla base di un esiguo numero di foto (Wang, Kosinski, 2018). Sebbene lo studio abbia ricevuto l’approvazione del comitato etico, solleva ulteriori domande sul consenso, dal momento che non è plausibile che gli utenti del sito di incontri potessero o volessero necessariamente acconsentire all’uso dei loro dati per questo scopo particolare.

			La privacy non è un problema nuovo, ma la centralità dei dati personali in molte applicazioni di ia (e di ai4sg) ne accresce il significato etico e crea problemi relativamente al consenso (Taddeo, Floridi, 2018a). Da ciò possiamo derivare la seguente best practice:

			5.	I designer di ai4sg devono rispettare la soglia di consenso stabilita per il trattamento delle raccolte di dati personali.

			9.3.6 Equità concreta

			Gli sviluppatori di ia si affidano di regola ai dati, che possono essere distorti in modo tale da avere effetti socialmente rilevanti. Tale pregiudizio (bias) può estendersi al processo decisionale algoritmico che è alla base di molti sistemi di ia, con conseguenze che sono inique per i soggetti del processo decisionale (Caliskan, Bryson, Narayanan, 2017) e, pertanto, possono violare il principio di giustizia. Queste decisioni possono basarsi su fattori significativi dal punto di vista etico (per esempio, motivi etnici, di genere o religiosi) ma irrilevanti per il processo decisionale in corso, oppure possono essere rilevanti ma legalmente protetti in quanto caratteristiche non discriminatorie (Friedman, Nissenbaum, 1996). Inoltre, le decisioni guidate dall’ia possono essere compenetrate da fattori che non hanno una chiara rilevanza etica, ma che costituiscono collettivamente un processo decisionale ingiustamente distorto (Pedreschi, Ruggieri, Turini, 2008; Floridi, 2012b).

			Le iniziative di ai4sg che si basano su dati distorti possono propagare tale pregiudizio attraverso un circolo vizioso (Yang, Bellingham, Dupont et al., 2018). Questo ciclo inizierebbe con un insieme di dati distorto che informa una prima fase del processo decisionale dell’ia, con conseguenti azioni discriminatorie, che a loro volta portano alla raccolta e all’uso di dati distorti. Consideriamo l’uso dell’ia per prevedere la nascita prematura negli Stati Uniti, dove le condizioni sanitarie delle donne incinte sono state a lungo influenzate dalla loro etnia. Il vecchio pregiudizio contro le donne afroamericane bisognose di cure, a causa di stereotipi storici dannosi, contribuisce a un tasso di morbilità materna che è oltre tre volte superiore a quello delle donne bianche (cdc, 2019). In tale contesto, l’ia può offrire un grande contributo per ridurre questo netto divario razziale, ma solo se la stessa discriminazione storica non è replicata nei sistemi di ia (Banjo, 2018). Oppure si consideri l’uso di software predittivi di polizia. Gli sviluppatori possono addestrare software predittivi di polizia su dati di polizia che contengono pregiudizi profondamente radicati. Quando la discriminazione concerne i tassi di arresto, viene a essere incorporata nei dati relativi al perseguimento dei reati (Lum, Isaac, 2016). Tali pregiudizi possono causare decisioni discriminatorie, per esempio diffide o arresti, che si ripercuotono su insiemi di dati sempre più distorti (Crawford, 2016), completando così il circolo vizioso.

			Gli esempi precedenti riguardano l’uso dell’ia per migliorare i risultati in ambiti in cui i dati sono già stati raccolti. Eppure, in molti altri contesti, i progetti di ai4sg (o altre iniziative simili) stanno in realtà rendendo i cittadini “visibili” come non lo erano in precedenza, anche nei contesti del Sud globale (Taylor, Broeders, 2015). Questa maggiore visibilità sottolinea l’importanza di proteggersi dalla possibile amplificazione dei pregiudizi nocivi da parte delle tecnologie di ia.

			Chiaramente, i designer devono sterilizzare gli insiemi di dati adoperati per addestrare l’ia. Tuttavia, c’è anche il rischio di applicare un disinfettante troppo forte, per così dire, rimuovendo importanti sfumature contestuali che potrebbero migliorare il processo decisionale etico. Perciò, i designer devono anche assicurare che il processo decisionale dell’ia resti sensibile ai fattori importanti per l’inclusione. Per esempio, si dovrebbe garantire che un elaboratore di testi interagisca in modo identico con un utente umano indipendentemente dal genere e dall’etnia di tale utente, ma anche attendersi che sappia funzionare in modo diverso e tuttavia equo, aiutando le persone con disabilità visive.

			Tali aspettative non sono sempre soddisfatte nel contesto del ragionamento guidato dall’ia. Rispetto all’elaborazione di testi, l’ia rende possibile uno spettro molto più ampio di modalità decisionali e di interazione, molte delle quali sono dirette da dati potenzialmente distorti. Gli insiemi di dati di addestramento possono contenere un linguaggio naturale che veicola associazioni inique tra genere e parole che, a loro volta, esercitano un potere normativo (Caliskan, Bryson, Narayanan, 2017). In altri contesti e casi d’uso, un approccio equo può richiedere differenze nella comunicazione, basate su fattori come il genere. Si consideri il caso dell’assistente didattico virtuale che non è riuscito a distinguere in modo sufficientemente corretto tra uomini e donne nelle sue risposte, quando ha appreso che un utente stava aspettando un bambino e si è congratulato con gli uomini, ignorando le donne (Eicher, Polepeddi, Goel, 2017). Un’indagine della bbc News ha evidenziato un esempio ancora più eclatante: un chatbot di salute mentale ritenuto idoneo a essere usati da bambini non è stato in grado di comprendere che un bambino segnalava esplicitamente di avere subito abusi sessuali (White, 2018). Come chiariscono questi casi, l’uso di ia nelle interazioni umano-computer, come i chatbot, richiede la corretta comprensione sia dei gruppi salienti a cui appartiene un utente sia delle caratteristiche di cui gli utenti sono provvisti quando interagiscono con un software.

			Il rispetto dell’equità nel caso concreto è cruciale per il successo dell’implementazione dell’ai4sg. Per conseguirla, i progetti di ai4sg devono rimuovere i fattori (e i loro proxy) che sono rilevanti dal punto di vista etico ma irrilevanti per il risultato e includere gli stessi fattori quando sono richiesti per motivi di inclusione, sicurezza o per altre considerazioni etiche. Il problema dei pregiudizi storici che influenzano il futuro processo decisionale è antico. La novità consiste nella possibilità che tali pregiudizi siano incorporati, rafforzati e perpetuati nuovamente da errati meccanismi di apprendimento per rinforzo. Questo rischio è particolarmente pronunciato se considerato insieme al rischio di opacità nei sistemi decisionali di ia e nei loro risultati. Torneremo su questo argomento nel prossimo paragrafo.

			L’identificazione dell’equità del caso concreto come fattore essenziale permette, ora, di formulare la seguente best practice:

			6.	I designer di ai4sg dovrebbero rimuovere dagli insiemi di dati rilevanti le variabili e i proxy che sono irrilevanti per un risultato, tranne nel caso in cui la loro introduzione supporti inclusione, sicurezza o altri imperativi etici.

			9.3.7 Semantizzazione adatta all’umano

			L’ai4sg deve consentire agli esseri umani di curare e promuovere il proprio “capitale semantico”, ovvero, come ho scritto,

			qualsiasi contenuto che può incrementare il potere di qualcuno di dare significato e conferire senso a (semantizzare) qualcosa. (Floridi, Cowls, Beltrametti et al., 2018)

			Questo è fondamentale per mantenere e promuovere l’autonomia umana. Abbiamo spesso la capacità tecnica di automatizzare la creazione di significato e senso (semantizzazione) tramite l’ia, ma possono anche manifestarsi sfiducia o ingiustizia se lo facciamo con noncuranza. Da ciò emergono due problemi.

			Il primo problema è che il software di ia può definire la semantizzazione in modo divergente dalle nostre scelte. Ciò accade se una procedura definisce arbitrariamente i significati, per esempio in base al lancio di una moneta. Lo stesso problema potrebbe sorgere se il software di ia supporta un qualche tipo di semantizzazione basata su usi preesistenti. Per esempio, i ricercatori hanno sviluppato un’applicazione che prevede il significato giuridico di “violazione” sulla base di casi precedenti (Al-Abdulkarim, Atkinson, Bench-Capon, 2015). Se si utilizza il software per definire il significato di “violazione”,12 allora si finisce per limitare il ruolo dei giudici e della giustizia. I giudici non sarebbero più in grado di semantizzare (affinare e ridefinire il significato, e la possibilità di dare un senso alla) “violazione”, quando interpretano la legge. Ciò costituisce un problema, perché l’uso passato non sempre prevede come semantizzeremmo gli stessi concetti o fenomeni in futuro.

			Il secondo problema consiste nel fatto che, in un contesto sociale, sarebbe inattuabile per il software di ia definire tutti i significati e i sensi. La semantizzazione è in una certa misura soggettiva, perché chi o che cosa è coinvolto nella semantizzazione è anche in parte costitutivo del processo e del suo esito. Per esempio, soltanto gli operatori giuridici possono definire il significato giuridico di “violazione”. Allo stesso modo, il significato e il senso dei simboli affettivi, come le espressioni facciali, dipende anche dal tipo di agente che mostra una data espressione. L’ia affettiva può rilevare un’emozione (Martı́nez-Miranda, Aldea, 2005), un agente artificiale può affermare con precisione che un essere umano appare triste, ma non può mutare il significato della tristezza.

			La soluzione a questi due problemi si basa sulla distinzione tra i compiti che dovrebbero o non dovrebbero essere delegati a un sistema artificiale. L’ia dovrebbe essere impiegata per facilitare la semantizzazione adatta all’umano, ma non per fornirla di per sé. Questo è vero, per esempio, quando si considerano i pazienti affetti da Alzheimer. La ricerca sulle relazioni tra assistente e paziente evidenzia tre punti (Burns, Rabins, 2000). In primo luogo, gli assistenti svolgono un ruolo critico, ma gravoso, nel ricordare ai pazienti le attività a cui partecipano, per esempio l’assunzione di farmaci. In secondo luogo, gli assistenti svolgono un ruolo essenziale anche nel fornire ai pazienti un’interazione significativa. E in terzo luogo, quando gli assistenti ricordano ai pazienti di assumere i farmaci, la relazione paziente-assistente può indebolirsi, poiché il paziente si infastidisce, con l’assistente che perde una certa capacità di fornire empatia e supporto significativo. Di conseguenza, i ricercatori hanno sviluppato un software di ia che bilancia il compito di ricordare rispetto al fastidio generato al paziente (Chu, Song, Levinson et al., 2012). L’equilibrio viene appreso e ottimizzato tramite l’apprendimento per rinforzo. I ricercatori hanno progettato il sistema in modo tale che gli assistenti sanitari possano trascorrere la maggior parte del loro tempo fornendo supporto empatico e preservando una relazione significativa con il paziente. Come mostra questo esempio, è possibile utilizzare l’ia per eliminare compiti banali, sostenendo invece una semantizzazione adatta all’umano.

			La semantizzazione incentrata sull’umano, come fattore essenziale per l’ai4sg, è alla base della best practice finale:

			7.	I designer di ai4sg non dovrebbero ostacolare la capacità delle persone di semantizzare (cioè di dare significato e conferire senso a) qualcosa.





9.4 Conclusione: fattori di bilanciamento per l’ia per il bene sociale


			In questo capitolo ho analizzato sette fattori a sostegno dell’ai4sg e le relative best practices. La Tabella 9.2 fornisce un riepilogo. Ho sostenuto che il principio di beneficenza è assunto come precondizione per l’ai4sg, quindi i fattori fanno riferimento a uno o più degli altri quattro principi dell’etica dell’ia: non maleficenza, autonomia, giustizia e spiegabilità identificati nel quarto capitolo.

			Tabella 9.2 Sette fattori a sostegno dell’AI4SG e delle relative best practices.

			 				 					 					 					 				 				 					 						 							Fattori

						 						 							Migliori pratiche

						 						 							Principi etici



				 				 					 						 							Falsificabilità e implementazione incrementale

						 						 							Identificare i requisiti falsificabili e testarli in fasi incrementali dal laboratorio al “mondo esterno”.

						 						 							Beneficenza

							Non maleficenza



					 						 							Garanzie contro la manipolazione dei predittori

						 						 							Adottare garanzie che (i) assicurino che gli indicatori non causali non distorcano in modo inappropriato gli interventi e (ii) limitino, quando appropriato, la conoscenza di come gli input influenzano gli output dei sistemi di AI4SG, per prevenire la manipolazione.

						 						 							Beneficenza

							Non maleficenza



					 						 							Intervento contestualizzato in ragione del destinatario

						 						 							Costruire sistemi decisionali in dialogo con gli utenti che interagiscono con questi sistemi e ne sono influenzati; sulla base della comprensione delle caratteristiche degli utenti, delle modalità di coordinamento, delle finalità e degli effetti di un intervento; e nel rispetto del diritto degli utenti di ignorare o modificare gli interventi.

						 						 							Beneficenza

							Autonomia



					 						 							Spiegazione contestualizzata in ragione del destinatario

							e finalità trasparenti

						 						 							Scegliere un livello di astrazione per la spiegazione dell’IA che soddisfi lo scopo esplicativo auspicato e sia appropriato al sistema e ai destinatari; quindi fornire argomenti che siano razionalmente e adeguatamente persuasivi affinché i destinatari forniscano la spiegazione; e assicurare che l’obiettivo (lo scopo del sistema) per cui viene sviluppato e implementato un sistema di AI4SG sia conoscibile per impostazione predefinita ai destinatari dei suoi risultati.

						 						 							Beneficenza

							Spiegabilità



					 						 							Tutela della privacy e consenso dell’interessato

						 						 							Rispettare la soglia di consenso stabilita per il trattamento delle raccolte di dati personali.

						 						 							Beneficenza

							Autonomia

							Non maleficenza



					 						 							Equità concreta

						 						 							Rimuovere dagli insiemi di dati rilevanti le variabili e i proxy irrilevanti per un risultato, tranne nel caso in cui la loro introduzione supporti inclusione, sicurezza o altri imperativi etici.

						 						 							Beneficenza

							Giustizia



					 						 							Semantizzazione adatta all’umano

						 						 							Non ostacolare la capacità delle persone di semantizzare (cioè di dare significato e conferire senso a) qualcosa.

						 						 							Beneficenza

							Autonomia



				 			 			I sette fattori suggeriscono che, per creare un’ai4sg di successo, sia necessario porre in essere due tipi di bilanciamento: intra e inter. Da un lato, ogni singolo fattore può richiedere di per sé un intrinseco bilanciamento, per esempio, tra il rischio di un intervento sovradimensionato e quello di un intervento sottodimensionato nell’elaborazione di interventi contestuali; o tra protezione per offuscamento e protezione per enumerazione delle differenze salienti tra persone, in ragione delle finalità e del contesto di un sistema. Dall’altro, i bilanciamenti non sono solo specifici di un singolo fattore; sono anche sistemici, perché devono essere operati tra più fattori. Si consideri la tensione tra impedire agli attori malintenzionati di comprendere come “ingannare” i dati di input dei sistemi di previsione di ia e consentire agli esseri umani di ignorare i risultati realmente inesatti; o la tensione tra garantire l’effettiva divulgazione dei motivi alla base di una decisione e rischiare di compromettere l’anonimato consensuale dei soggetti interessati.

			La domanda principale che deve affrontare la comunità di ai4sg è, per ogni dato caso, se si sia moralmente obbligati o no a progettare, sviluppare e implementare uno specifico progetto di ai4sg. Questo capitolo non cerca di rispondere a tale domanda in astratto. Risolvere le tensioni che possono sorgere al riguardo o tra fattori dipende fortemente dal contesto, e l’analisi fin qui svolta non pretende di applicarsi a tutti i potenziali contesti, non ultimo perché ciò sarebbe incoerente con l’idea sostenuta in questo capitolo di testare le ipotesi falsificabili e di dare luogo a un’implementazione incrementale; né sarebbe sufficiente una lista di controllo delle cose puramente tecniche “da fare o non fare”. Piuttosto, l’analisi ha elaborato una serie di fattori essenziali che devono essere considerati, interpretati e valutati contestualmente ogni volta che si progetta, sviluppa e implementa uno specifico progetto di ai4sg. Il futuro dell’ai4sg fornirà probabilmente maggiori opportunità per arricchire questo insieme di fattori essenziali. E la stessa ia può contribuire a gestire il proprio ciclo di vita fornendo, in maniera metariflessiva, strumenti per valutare come operare al meglio i bilanciamenti individuali e sistemici sopra indicati.

			È probabile che le questioni più rilevanti che emergono dai fattori descritti in questo capitolo riguardino proprio la sfida di bilanciare le esigenze e istanze in competizione introdotte dai fattori e dalle corrispondenti best practices. Ciò concerne quello che legittima il processo decisionale attuato con l’ia o che riguarda l’ia. Tornerò su questo tema nell’undicesimo capitolo in quanto questione di dibattito etico (per una disamina delle sue implicazioni politiche in termini di sovranità digitale vedi Floridi, 2020b). Qui offro alcune osservazioni a titolo di conclusione.

			Questioni relative a trade-off, bilanciamenti e la loro legittimità sono inevitabilmente intrecciate con sfide etiche e politiche più ampie, che riguardano chi ha il potere o la levatura per partecipare a questo processo di valutazione, nonché il modo in cui le preferenze multiple vengono misurate e aggregate, come mostra lo schema tripartito di Baum (2020). Se riteniamo dunque che la sfida nel bilanciare i fattori dovrebbe avere almeno in parte una dimensione partecipata, la rassegna dei teoremi della scelta sociale rilevante in Prasad (2018) individua diverse condizioni di fondo per sostenere un processo decisionale di gruppo efficace. Come suggerito da queste analisi, è probabile che l’incorporazione di molteplici prospettive nel design dei sistemi decisionali di ia sia un passo eticamente importante tanto per l’ia in generale quanto per l’ai4sg in particolare. Ciò vale anche per gli sforzi diretti a implementare forme di co-design. C’è chiaramente ancora molto lavoro da fare per garantire che i progetti di ai4sg siano disegnati con modalità che non solo promuovano obiettivi benefici e affrontino le sfide sociali, ma che lo facciano con modalità socialmente preferibili (eque) e sostenibili. In questo capitolo abbiamo esaminato le basi per disegnare buone pratiche e politiche, nonché per svolgere ulteriori ricerche sulle considerazioni etiche che dovrebbero essere poste a fondamento dei progetti di ai4sg, e quindi del “progetto di ai4sg” più in generale.

			È giunto il momento di discutere in che modo le considerazioni etiche, giuridiche e politiche interagiscono in questo sviluppo. Ho esaminato nel sesto capitolo cosa succede quando etica e diritto si intrecciano e ho sostenuto l’importanza di un approccio di etica soft quando possibile. Nei capitoli seguenti, mi affiderò al lavoro svolto finora per concentrarmi su aree specifiche in cui l’impatto etico dell’ia è più evidente.



* * *





			 				 					1. Mantengo l’acronimo di Artificial Intelligence for Social Good. [NdT]



				 					2. “ai for Good Global Summit”, 28-31 maggio 2019, Ginevra, https://aiforgood.itu.int/.



				 					3. “Repository ai”, https://www.itu.int/en/ITU-T/AI/Pages/ai-repository.aspx.



				 					4. itu, “ai for Good Global Summit”, 2017, https://www.itu.int/en/ITU-T/AI/Pages/201706-default.aspx; itu, “ai for Good Global Summit”, 2018, https://www.itu.int/en/ITU-T/AI/2018/Pages/default.aspx. “ai for Good Global Summit”, cit.



				 					5. Sebbene fornire tale valutazione per ogni caso particolare ecceda lo scopo del presente capitolo, è importante riconoscere fin dal principio che nella pratica è probabile che ci siano notevole disaccordo e discussione su ciò che costituirebbe un risultato socialmente buono.



				 					6. Ciò non va inteso come un calcolo utilitaristico: l’impatto benefico di un dato progetto può essere “compensato” dalla violazione di qualche imperativo categorico. Pertanto, anche se un progetto di ai4sg facesse “più bene che male”, il danno potrebbe essere eticamente intollerabile. In tale caso ipotetico, non saremmo moralmente obbligati a sviluppare e implementare il progetto in questione.



				 					7. Come notato nell’introduzione, non è possibile documentare ogni singola considerazione etica per un progetto di bene sociale, quindi anche i fattori meno nuovi qui sono quelli che assumono nuova rilevanza nel contesto dell’ia.



				 					8. È certamente probabile che, in pratica, una valutazione della sicurezza di un sistema di ia debba tenere conto anche di convinzioni culturali e valori sociali più ampi che, per esempio, possono richiedere diversi trade-off tra i requisiti di fattori critici come sicurezza e altre norme e aspettative potenzialmente concorrenti.



				 					9. Mentre, per semplicità, il nostro obiettivo è ridurre al minimo la diffusione delle informazioni utilizzate per prevedere un risultato, non intendiamo precluderci il suggerimento, offerto da Prasad (2018), per cui in alcuni casi un approccio più equo può essere quello di massimizzare le informazioni disponibili e quindi di “democratizzare” la capacità di manipolare i predittori.



				 					10. Le quattro rimanenti dimensioni proposte da McFarlane – la fonte dell’interruzione, il metodo di espressione, il canale di trasmissione e l’attività umana modificata dall’interruzione – non sono rilevanti ai fini di questo capitolo.



				 					11. Si noti che l’importanza di coinvolgere esperti del settore nel processo non mira solo a migliorare la loro esperienza come destinatari delle decisioni, ma risiede anche nella loro incomparabile conoscenza del settore a cui i ricercatori hanno attinto nel design del sistema, contribuendo a fornire ai ricercatori quella che Pagallo (2015) chiama “comprensione preventiva” del settore.



				 					12. Non vi è alcuna indicazione che questo sia l’uso previsto.





10


			Macchine ultraintelligenti, singolarità e altre distrazioni fantascientifiche

			Sommario Nel presente capitolo esamino alcune conseguenze che derivano dal nostro crescente successo nel design di agenti smart, autonomi e sociali, nonché di strumenti predittivi in grado di anticipare e manipolare le decisioni e le scelte umane. Sostengo che lo sviluppo dell’ia come capacità di agire efficace ma priva d’intelligenza non conduce ad alcuna strampalata elaborazione di scenari di fantascienza (singolarità), che sono nel migliore dei casi fuorvianti e nel peggiore irresponsabili. Al contempo, ritengo anche che negare che l’ia comporti una rivoluzione nel modo in cui creiamo, controlliamo e concepiamo l’agire sia sbagliato. Concludo sottolineando che ciò richiede una capacità di lungimiranza etica e di design per delineare il tipo di infosfera e di società dell’informazione che vorremmo sviluppare.





10.1 Introduzione: l’aggiornata paura ancestrale dei mostri


			Supponiamo che qualcuno entri in una stanza buia in un edificio sconosciuto. Potrebbe farsi prendere dal panico a causa dei possibili mostri in agguato nell’oscurità. Oppure potrebbe semplicemente accendere la luce, per evitare di sbattere contro i mobili. La stanza buia è il futuro dell’ia. Purtroppo, alcune persone credono che, non appena entrati nella stanza, potremmo imbatterci in alcune macchine malvagie e ultraintelligenti. La paura che esista qualche tipo di orco, come un Golem o un mostro di Frankenstein, è antica quanto la memoria umana. La versione computerizzata di tale paura risale almeno agli anni Sessanta, quando Irving John Good,1 un matematico britannico che ha lavorato come crittologo a Bletchley Park con Alan Turing, ha fatto la seguente osservazione:

			Definiamo come ultraintelligente una macchina che può superare di gran lunga tutte le attività intellettuali di qualsiasi uomo per quanto intelligente. Poiché il design di macchine è una di queste attività intellettuali, una macchina ultraintelligente potrebbe disegnare macchine ancora migliori; assisteremmo allora indubbiamente a una “esplosione d’intelligenza”, mentre l’intelligenza umana sarebbe lasciata molto indietro. Per questo, la prima macchina ultraintelligente è l’ultima invenzione che l’uomo abbia bisogno di realizzare, a patto che la macchina sia sufficientemente docile da dirci come tenerla sotto controllo. È curioso che questo punto venga sollevato così di rado al di fuori della fantascienza. A volte, vale la pena di prendere sul serio la fantascienza. (Good, 1965, p. 33)

			Una volta che le macchine ultraintelligenti diventassero realtà, potrebbero non essere affatto docili ma comportarsi come Terminator o meglio Skynet (nel film questa è la rete di difesa di ia che diventa autocosciente e dà inizio a un olocausto nucleare). Potrebbero schiavizzare l’umanità come specie inferiore, ignorarne i diritti e perseguire i propri fini, indipendentemente dagli effetti che ciò ha sulle vite umane. Se ciò sembra troppo incredibile per essere preso sul serio, il rapido avanzamento di mezzo secolo e gli incredibili sviluppi nelle nostre tecnologie digitali hanno portato alcune persone a credere che l’“esplosione d’intelligenza” di Good, talora presentata anche come singolarità, possa costituire un serio rischio, e che, se non stiamo attenti, la fine della nostra specie potrebbe essere vicina. Nelle parole di Stephen Hawking:

			Penso che lo sviluppo dell’intelligenza artificiale completa potrebbe dichiarare la fine della razza umana. (Holley, 2014)

			Bill Gates è parimenti preoccupato. Durante una sessione di domande e risposte “chiedimi qualsiasi cosa” su Reddit, ha scritto:

			Sono tra quelli che si preoccupano per la superintelligenza. Prima di tutto, le macchine faranno molti lavori per noi e non saranno superintelligenti. Ciò dovrebbe essere positivo se lo gestiamo bene. Pochi decenni dopo, tuttavia, l’intelligenza sarà abbastanza forte da destare preoccupazioni. Sono d’accordo con Elon Musk e altri su questo punto e non capisco perché alcune persone non se ne preoccupino.

			La dichiarazione è stata ripubblicata molte volte (vedi per esempio Mack, 2015; Holley, 2015; Rawlinson, 2015; Christian, 2019). E che cosa ha detto Elon Musk, esattamente?

			Penso che dovremmo stare molto attenti all’intelligenza artificiale. Se dovessi scommettere su quale sia la nostra più grande minaccia esistenziale, direi che è probabilmente quella. Perciò, dobbiamo stare molto attenti all’intelligenza artificiale. Un numero crescente di scienziati ritiene che ci dovrebbe essere una supervisione normativa, magari a livello nazionale e internazionale, che si assicuri che non facciamo qualcosa di davvero sciocco. Con l’intelligenza artificiale stiamo evocando il demone. In tutte le storie in cui c’è il ragazzo con il pentagramma e l’acqua santa, è come se fosse senz’altro sicuro di poter controllare il demone. Non ha funzionato. (McFarland, 2014)

			Negli ultimi anni, Musk ha lanciato allarmi sempre più preoccupati. L’hanno seguito autori che hanno reso popolare la paura di una sorta di ultraintelligenza artificiale o superintelligenza.2 Molti non sono d’accordo, o semplicemente non prendono sul serio tali speculazioni. Alcuni li prendono in giro. Nel 2016, l’Information Technology and Innovation Foundation ha attribuito

			il suo annuale Premio Luddista (Luddite Award) a un’ampia coalizione di scienziati e luminari che nel 2015 ha suscitato paura e isteria lanciando allarmi sul fatto che l’intelligenza artificiale potrebbe segnare il destino dell’umanità. […] “È profondamente spiacevole che luminari come Elon Musk e Stephen Hawking abbiano contribuito alla delirante preoccupazione al riguardo di un’incombente apocalisse dell’intelligenza artificiale”, ha affermato il presidente dell’itif Robert D. Atkinson.3

			La realtà è più banale e, in un certo senso, solleva preoccupazioni più realistiche. Le attuali e prevedibili tecnologie smart hanno l’intelligenza di un abaco, ossia zero. E coloro che discutono in termini di crescita lenta ma costante dell’attuale livello d’intelligenza delle tecnologie smart – per cui faremmo bene a preoccuparci ora perché la vera ia arriverà prima o poi – dovrebbero ricordare che non importa quanti zeri aggiungiamo, ciò ci manterrà sempre al punto di partenza. Il problema è sempre la stupidità umana o la natura malvagia. Pochi mesi dopo il Premio Luddista di cui sopra, il 23 marzo 2016, Microsoft ha introdotto Tay su Twitter. Abbiamo già incontrato Tay nell’ottavo capitolo, ma ecco un breve promemoria: Tay era un chatbot basato sull’ia. Ha dovuto essere rimosso solo sedici ore dopo. Tay sarebbe dovuto diventare sempre più intelligente mentre interagiva con gli umani. Invece, è diventato rapidamente un malvagio sostenitore di Hitler, negazionista dell’Olocausto, promotore dell’incesto e assertore del fatto che “Bush ha realizzato l’11 settembre”. Come mai? Perché funzionava come la carta assorbente da cucina, impregnandosi e assumendo la forma dei messaggi ingannevoli e sgradevoli che gli venivano inviati. Microsoft si è scusata (Hunt, 2016). Questo è lo stato dell’ia oggi e nel futuro realisticamente prevedibile: vedi, per esempio, il premio Loebner e il test di Turing già menzionati. Ma non è un buon motivo per stare sereni. Al contrario, dopo tante speculazioni fuorvianti sui rischi inverosimili delle macchine ultraintelligenti, è giunto il momento di accendere la luce, smettere di preoccuparsi di scenari fantascientifici che distraggono e iniziare a concentrarsi sulle reali sfide dell’ia, per evitare di fare errori dolorosi e costosi nel design e nell’uso delle tecnologie smart. È quanto ho cercato di fare nei capitoli precedenti. In questo, esaminerò quelle che considero le ragioni principali per cui tali preoccupazioni, ma anche le opinioni opposte, eccessivamente ottimistiche, sono sbagliate.





10.2 Credenti e miscredenti nella vera ia: un dibattito sulla fede


			La filosofia non coglie bene le sfumature (questa è la confessione di un peccatore). Può apprezzare la precisione e le distinzioni affinate, ma ciò che ama davvero sono le polarizzazioni e le dicotomie. Internalismo o esternalismo, fondazionalismo o coerentismo, trolley a sinistra o a destra, zombie o non zombie, mondi relativi all’osservatore o indipendenti dall’osservatore, possibili o impossibili, fondati o non fondati, […] la filosofia può predicare il vel inclusivo (“ragazze o ragazzi possono giocare”) ma fin troppo spesso indulge nell’aut aut esclusivo (“o ti piace o non ti piace”). L’attuale dibattito sull’ia ne è un esempio calzante. Qui la dicotomia riguarda i credenti e i miscredenti nella vera ia, conosciuta anche come iag (intelligenza artificiale generale), ia forte, piena o universale. Sì, la cosa vera e propria, non Siri nel nostro iPhone, Roomba (un robot aspirapolvere) nel nostro soggiorno o Nest (un termostato intelligente) nella nostra cucina (ammissione: sono felice proprietario di tutti e tre questi dispositivi). Pensiamo, invece, alla falsa Maria in Metropolis (1927), Hal 9000 in 2001: Odissea nello spazio (1968; Good era uno dei consulenti), c-3po in Star Wars (1977), Rachael in Blade Runner (1982), Data in Star Trek: The Next Generation (1987), l’agente Smith in The Matrix (1999), la Samantha priva di corpo in Lei (2013) o Ava in Ex Machina (2014). L’elenco potrebbe continuare, ma il quadro è chiaro. I credenti nella vera ia e nell’“esplosione di intelligenza” di Good appartengono alla Chiesa dei fautori della singolarità. In mancanza di un termine migliore, mi riferirò ai miscredenti come membri della Chiesa degli atei dell’ia. Nel resto di questo capitolo, desidero esaminare le due fedi e capire perché entrambe siano errate, per sgombrare il campo da concezioni potenzialmente fuorvianti. Nel frattempo, ricordiamoci che la buona filosofia sta quasi sempre nel noioso mezzo.





10.3 Gli adepti della singolarità: la fine è vicina, la vera ia sta arrivando


			Gli adepti della singolarità credono in tre dogmi. Primo, la creazione di una qualche forma di ultraintelligenza artificiale è probabile o almeno non impossibile nel (per alcuni di loro prevedibile) futuro. Questa svolta è nota come singolarità tecnologica, da cui il nome. Sia la natura di tale superintelligenza sia l’esatto lasso di tempo del suo arrivo non sono specificati, sebbene gli adepti della singolarità tendano a preferire futuri che sono convenientemente abbastanza vicini da preoccuparsene ma sufficientemente lontani da non essere più là per verificarne la correttezza o no (dirò di più su queste fantasiose scansioni temporali a breve). In secondo luogo, l’umanità corre il grosso rischio di essere dominata da tale ultraintelligenza. In terzo luogo, la generazione attuale ha la responsabilità primaria di assicurare che la singolarità non abbia luogo o, se accade, che sia benigna e vada a vantaggio dell’umanità. Ciò ha tutte le caratteristiche di una visione manichea del mondo: il Bene che combatte il Male, toni apocalittici, l’urgenza del “dobbiamo fare qualcosa ora o sarà troppo tardi”, una prospettiva escatologica della salvezza umana, e un appello alle paure e all’ignoranza. Se proiettiamo tutto questo in un contesto in cui le persone sono giustamente preoccupate per l’impatto delle tecnologie digitali sulle loro vite, specialmente per ciò che concerne il mercato del lavoro, la politica, i crimini e i conflitti informatici, e in cui i mass media ogni giorno danno notizia di nuovi aggeggi e disastri informatici senza precedenti, avremo la ricetta perfetta per un dibattito che provoca distrazione di massa, un oppiaceo digitale per le masse.

			Come tutte le opinioni basate sulla fede, il credo della singolarità è inconfutabile perché alla fine non è limitato dalla ragione e dall’evidenza. È anche poco plausibile, dal momento che non c’è motivo di credere che qualcosa che assomigli a macchine intelligenti (per non parlare di ultraintelligenti) emergerà dalla nostra attuale e prevedibile comprensione dell’informatica e delle tecnologie digitali. Lasciatemi spiegare.

			Il credo della singolarità è, talvolta, presentato in termini condizionali. Ciò è scaltro, perché l’allora segue il se, e non solo nel senso per cui ex falso quod libet: se un qualche tipo di ultraintelligenza dovesse manifestarsi, allora ci troveremmo in guai grossi, e non semplicemente “potremmo” trovarci, come detto in precedenza da Hawking. Corretto. Assolutamente. Ma questo vale anche per il seguente condizionale: se dovessero apparire i Quattro Cavalieri dell’Apocalisse, allora ci troveremmo in guai ancora più grossi.

			Altre volte, il credo della singolarità si basa su un senso molto debole di possibilità: una qualche forma di ultraintelligenza artificiale potrebbe svilupparsi, no? Sì, potrebbe. Ma tale “potrebbe” è una mera possibilità logica, cioè, per quanto ne sappiamo, non c’è contraddizione nell’ipotizzare lo sviluppo dell’ultraintelligenza artificiale. Eppure questo è un trucco, che offusca l’enorme differenza tra “potrei ammalarmi domani” quando già non mi sento troppo bene, e “potrei essere una farfalla che sogna di essere un essere umano”. Non c’è contraddizione nell’ipotizzare che un tuo parente, di cui non hai mai sentito parlare, sia morto lasciandoti dieci milioni di dollari. Ciò potrebbe accadere. Dunque? Le contraddizioni, come gli scapoli felicemente sposati, non sono stati di cose possibili, ma le non-contraddizioni (semplici “potrebbero”), come agenti extraterrestri che vivono in mezzo a noi così ben nascosti che non li abbiamo mai scoperti, possono essere liquidate come qualcosa di assolutamente privo di senso. Russell utilizzava una meravigliosa analogia per chiarire il punto:

			Se dovessi suggerire che tra la Terra e Marte c’è una teiera di porcellana che ruota intorno al sole in un’orbita ellittica, nessuno sarebbe in grado di smentire la mia affermazione purché fossi attento ad aggiungere che la teiera è troppo piccola per essere rilevata anche dai nostri telescopi più potenti. Ma se aggiungessi che, poiché la mia affermazione non può essere confutata, dubitarne costituisce un’intollerabile presunzione da parte della ragione umana, giustamente crederei di stare dicendo delle sciocchezze […]. (Russell, 1952)

			La singolarità è solo un caso di credenza nella teiera, perché il “potrebbe” in “l’ultraintelligenza artificiale potrebbe svilupparsi” non è come il “potrebbe” in “potrebbe verificarsi un terremoto”, ma come il potrebbe in “non è vero che non potrebbe accadere” che tu sia il primo essere umano immortale. Giusto, ma non è un buon motivo per iniziare a comportarsi come se dovessi vivere per sempre.

			A meno che, naturalmente, qualcuno non fornisca prove contrarie, cioè mostri che ci sia qualcosa nella nostra attuale e prevedibile comprensione dell’informatica che dovrebbe portarci a sospettare che l’emergere dell’ultraintelligenza artificiale sia anche solo lontanamente plausibile. È qui che gli adepti della singolarità mescolano fede e fatti, spesso spinti, mi piace credere, da un sincero senso di urgenza apocalittica. Cominciano con il parlare di posti di lavoro persi, sistemi digitali a rischio, droni senza pilota fuori controllo e altri problemi reali e preoccupanti relativi alle tecnologie computazionali che sono prossime a dominare la vita umana, dall’istruzione all’occupazione, dall’intrattenimento ai conflitti. Da tali evidenze, saltano poi a essere seriamente preoccupati perché sono incapaci di controllare la loro prossima auto poiché avrà una mente propria. Come una cattiva ia ultraintelligente potrà mai evolvere autonomamente dalle abilità computazionali necessarie per parcheggiare in un luogo angusto resta poco chiaro. La verità è che arrampicarsi sulla cima di un albero non è un piccolo passo verso la Luna; è la fine del viaggio. Quello a cui siamo destinati ad assistere sono macchine sempre più smart in grado di svolgere un numero crescente di attività, alcune delle quali siamo noi stessi attualmente a svolgere, mentre altre supereranno le nostre capacità. Per la prima volta nella storia umana, l’agire ha divorziato in modo irreversibile e con successo dall’intelligenza, e questo è di per sé alquanto straordinario, senza dover credere alla fantascienza.

			Se tutti gli altri argomenti falliscono, gli adepti della singolarità amano aggiungere un po’ di matematica. Uno dei riferimenti preferiti è la legge di Moore, che è molto nota, ma permettetemi di enunciarla nuovamente per motivi di chiarezza. Si tratta di un’affermazione empirica per cui, nello sviluppo dei computer digitali, il numero di transistor sui circuiti integrati raddoppia circa ogni due anni. Il risultato è stato finora una maggiore potenza di calcolo a un prezzo inferiore. Ma le cose stanno cambiando. Le difficoltà tecniche nella nanotecnologia presentano serie sfide di produzione. Dopotutto, c’è un limite a quanto le cose possono diventare piccole prima di dissolversi. La legge di Moore non vale più (Waldrop, 2016; The Economist, 2016). Altri tipi di soluzioni tecnologiche dovranno essere identificate, tra cui il computer quantistico. Solo perché qualcosa cresce esponenzialmente per un certo tempo, non significa che continuerà a farlo per sempre. Ecco un buon esempio di che cosa succede se non si presta attenzione alle “proiezioni”:

			Nel corso della storia di cui si ha conoscenza, gli umani hanno regnato incontrastati come specie dominante sulla Terra. Ciò potrebbe cambiare presto? I tacchini, creature finora innocue, sono esplose in dimensioni, gonfiandosi da una media di 13,2 libbre (6 chili) nel 1929 a oltre 30 libbre oggi. Partendo dal solido presupposto scientifico che le tendenze attuali persisteranno, The Economist calcola che i tacchini saranno grandi quanto gli umani in soli 150 anni. Entro 6000 anni, i tacchini sovrasteranno l’intero pianeta. Gli scienziati affermano che la rapida crescita dei tacchini è il risultato di innovazioni nell’allevamento del pollame, come l’allevamento selettivo e l’inseminazione artificiale. La natura artificiale della loro crescita e il fatto che la maggior parte abbia perso la capacità di volare suggeriscono che non tutto è perduto. Tuttavia, con quasi 250 milioni di tacchini che continuano a ingozzarsi e a sfilare solo in America, c’è motivo di preoccupazione. Nel giorno del Ringraziamento, c’è una sola linea prudente d’azione prudente: mangiarli prima che ci mangino. (The Economist, 2014b)

			Dal mostro tacchino al mostro di ia il passo è breve, se non fosse per il fatto che una curva di crescita può essere facilmente sigmoide (si veda Figura 10.1), con uno stadio iniziale di crescita approssimativamente esponenziale, seguito da saturazione, crescita più lenta, maturità e, infine, nessuna crescita ulteriore. Ma temo che la rappresentazione delle curve sigmoidi potrebbe risultare blasfema per alcuni adepti della singolarità.

			Pensavo che il credo della singolarità fosse semplicemente qualcosa di divertente. Un po’ come le persone che indossano cappelli di stagnola. Eppure, ho ascoltato in conferenze specialistiche persone sostenere che dovremmo essere preoccupati per la singolarità perché (segue qui qualche ragionamento fallace…), e che è meglio prevenire che curare. Seguendo la stessa logica, dovremmo anche portare sempre con noi un paletto di legno, nel caso in cui incontrassimo un vampiro. Pensavo che alcuni dei loro ragionamenti fallaci non avrebbero ingannato nessuno. Eppure, ricordo ancora con stupore una persona del mio dipartimento sostenere che, poiché nel passato ci eravamo sbagliati su possibilità che si sarebbero in seguito rivelate reali, avremmo dovuto prendere molto sul serio la singolarità. Ricordo di aver cercato di spiegare che questa “equazione” – A (per esempio, volare, l’esempio fornito) era ritenuta impossibile in passato, ma è possibile ora, quindi B (in tal caso, la vera ia), che è ritenuta impossibile ora, sarà o potrebbe essere possibile in futuro – era semplicemente troppo permissiva e quindi inutile, dal momento che si poteva sostituire B con qualsiasi cosa (per esempio, essere immortale). Ho insistito sul fatto che non si trattasse di un’argomentazione logica ma di una mera manovra retorica. Non sono riuscito a convincerlo, naturalmente (“naturalmente” è stata una lezione appresa dopo il tentativo), anche perché chiunque trovi convincente quel modo di ragionare è impermeabile a qualsiasi spiegazione ragionevole che dimostri che esso sia fin dal principio destituito di senso.

			Oggi ritengo che il credo della singolarità non sia né divertente né solo logicamente irritante, ma irresponsabilmente fuorviante. È una preoccupazione tipica del mondo ricco, che probabilmente impensierirà le persone nelle società opulente, che sembrano dimenticare i mali reali che opprimono l’umanità e il nostro pianeta. Purtroppo, la pandemia da Covid-19 ha ricordato ai profeti di sventura che abbiamo problemi tragici che sono seri e urgenti. Il cambiamento climatico – non importa cosa ne possa pensare Musk – è “la nostra più grande minaccia esistenziale”. È immorale speculare su scenari hollywoodiani mentre, nel 2019,

			miliardi di persone in tutto il mondo continuano a soffrire per lo scarso accesso all’acqua, ai servizi sanitari e all’igiene, secondo un nuovo rapporto dell’unicef e dell’Organizzazione mondiale della sanità. Circa 2,2 miliardi di persone in tutto il mondo non dispongono di servizi di acqua potabile gestiti in modo sicuro, 4,2 miliardi di persone non dispongono di servizi sanitari gestiti in modo sicuro e 3 miliardi non dispongono di strutture di base per il lavaggio delle mani.

			Queste sono minacce reali e importanti per l’umanità. E nel caso in cui pensassimo che le previsioni degli esperti siano una guida affidabile, riflettiamoci ancora una volta. Ci sono molte previsioni tecnologiche incredibilmente sbagliate da parte di grandi esperti (alcune esilaranti in Pogue, 2012, e Cracked Readers, 2014). Per esempio, nel 2004 Bill Gates ha dichiarato: “Tra due anni lo spam sarà risolto”. E nel 2011 Stephen Hawking ha dichiarato che “la filosofia è morta” (Warman, 2011), per cui non state leggendo questo libro. Ma la previsione a cui sono piuttosto affezionato è quella di Robert Metcalfe, coinventore di Ethernet e fondatore di 3Com. Nel 1995 ha giurato che si sarebbe rimangiato le proprie parole se la sua previsione che Internet sarebbe diventata presto una supernova e collassata catastroficamente nel 1996 si fosse rivelata sbagliata. Nel 1997, ha liquefatto pubblicamente il suo articolo in un robot da cucina e lo ha doverosamente bevuto. Un uomo di parola. Mi piacerebbe che gli adepti della singolarità fossero audaci e coerenti come lui.





10.4 ia-teismo dell’ia: quello che i computer non possono fare, presumibilmente


			Ho speso più di qualche parola per descrivere il credo della singolarità, non perché possa essere preso sul serio, ma perché i miscredenti dell’ia, gli atei, possono essere meglio compresi come persone che reagiscono in modo eccessivo a tutte queste sciocchezze della singolarità. Profondamente irritati da coloro che adorano gli dei digitali sbagliati e dalle loro profezie irrealizzate sulla singolarità, i miscredenti (gli atei dell’ia) intraprendono la missione di dimostrare una volta per tutte che qualsiasi tipo di fede nella vera ia è sbagliata, del tutto sbagliata. L’ia è solo computer, i computer sono solo macchine di Turing, le macchine di Turing sono solo motori sintattici e i motori sintattici non possono pensare, non possono sapere, non possono essere coscienti. Fine della storia. Questo è il motivo per cui ci sono così tante cose che i computer non possono ancora fare (secondo la formulazione dei titoli di diverse datate pubblicazioni: Wilson, Wilson, 1970; Dreyfus, 1972, 1979, 1992; Harel, 2000; Searle, 2014), anche se ciò che esattamente non possono fare è un bersaglio che può essere opportunamente spostato. È anche il motivo per cui non sono in grado di processare la semantica (di qualsiasi lingua, incluso il cinese), indipendentemente da ciò che la traduzione di Google riesce a ottenere (Preston, Bishop, 2002). Ciò dimostra che non c’è assolutamente nulla di cui discutere, e tantomeno di cui preoccuparsi. Non esiste una vera ia, e per questo non ci sono a fortiori problemi causati da essa. Possiamo rilassarci e goderci tutti questi meravigliosi dispositivi elettrici.

			La fede degli atei è malriposta quanto quella degli adepti della singolarità. Entrambe le Chiese hanno molti seguaci in California, dove i film di fantascienza di Hollywood, le meravigliose università di ricerca come Berkeley e alcune delle aziende digitali più importanti del mondo prosperano fianco a fianco. Ciò può non essere un caso. Quando ci sono molti soldi in gioco, le persone si confondono facilmente. Per esempio, tutti sanno che Google ha acquistato società di tecnologia di ia come se non ci fosse un domani.4 Certamente, devono sapere qualcosa sulle reali possibilità di sviluppare un computer in grado di pensare, che noi, al di fuori del “Circolo”, non conosciamo. Eric Schmidt (all’epoca presidente esecutivo di Google) ha alimentato questa visione, quando ha parlato all’Aspen Institute il 16 luglio 2013:

			Molte persone nel settore dell’ia credono che saremo vicini a [un computer che supererà il test di Turing] entro i prossimi cinque anni [corsivo mio].5

			“Cinque anni.” Turing preferiva “50 anni”. Hawking ha indicato “100 anni”.6 È forte l’impressione che il futuro dell’ia sarebbe stato molto diverso se ci fossimo evoluti con sei dita invece che con cinque. Tuttavia, Kurzweil, che ha reso celebre l’espressione “singolarità tecnologica”, ha resistito alla tentazione di contare in termini di multipli di cinque. Ha previsto che la vera ia sarà disponibile nel 2029.7 Si tratta di un’indicazione molto precisa, per cui, se qualcuno si sta chiedendo quale analisi scientifica si celi dietro la scelta di quella specifica data, la risposta è che si colloca dopo il suo ottantesimo compleanno (è nato nel 1948).

			Come è risaputo, il test di Turing è un modo per verificare se l’ia si stia in qualche modo avvicinando. Ricordiamo in breve in che cosa consista il test. Sono poste domande a due agenti che si trovano in stanze diverse; uno è un agente umano, l’altro artificiale; se non siamo in grado di cogliere la differenza tra i due agenti in base alle loro risposte, allora il robot supera il test. È un test grossolano. Pensiamolo come un esame di guida: se Alice non lo supera, non guida in modo sicuro, ma se lo supera, potrebbe comunque guidare in modo pericoloso. In breve, il test di Turing fornisce una condizione necessaria ma insufficiente per identificare una forma d’intelligenza. È un test, la cui soglia è davvero molto bassa, eppure nessuna ia l’ha mai superata. E, cosa ancora più importante, tutti i programmi continuano a fallire allo stesso modo, utilizzando trucchi sviluppati negli anni Sessanta. Per questo in passato ho fatto una scommessa. Odio le melanzane, ma ho promesso di mangiarne un piatto intero, se un programma software avesse vinto la medaglia d’oro (ovvero avesse superato il test di Turing) di un concorso del premio Loebner prima del 16 luglio 2018, data indicata da Schmidt. Era una scommessa sicura. È il 2021 e non abbiamo ancora alcun segno di un vero vincitore del test di Turing. Non so chi siano le “molte persone” nella citazione di Schmidt, ma so che le ultime persone a cui dovremmo chiedere se qualcosa è possibile sono quelle che hanno consistenti ragioni economiche per rassicurarci che lo sia.

			Finora, ci sono stati solo premi di consolazione assegnati alle versioni meno performanti di una contemporanea eliza. Talora, sono le persone che pongono le domande che falliscono il test di Turing, ponendo domande binarie come “Ti piace il gelato?” o “Credi in Dio?” (si tratta di esempi reali), alle quali qualsiasi risposta non fornirebbe comunque alcuna informazione (Floridi, Taddeo, Turilli, 2009).





10.5 Adepti della singolarità e atei dell’ia: una diatriba inutile


			Sia gli adepti della singolarità sia gli atei dell’ia si sbagliano. Come Turing ha affermato chiaramente nell’articolo in cui ha introdotto il suo test (Turing, 1950), la domanda “È una macchina in grado di pensare?” è “troppo insignificante per meritare una discussione”. Ironia della sorte, o forse preveggenza, quella domanda è in realtà incisa sulla medaglia del premio Loebner. Resta una domanda insignificante, a prescindere dall’una o l’altra Chiesa alla quale si appartenga. Eppure, entrambe le Chiese proseguono questo inutile dibattito, soffocando spesso ogni voce dissenziente della ragione. La vera ia non è logicamente impossibile, ma è assolutamente non plausibile. Le persone confondono “la singolarità non accadrà mai” con “la singolarità è impossibile”. Ripetiamolo ancora una volta, impossibile è un concetto logico e la vera ia è logicamente possibile. Ma è possibile come un calcolo che, per esempio, richiederebbe più tempo della vita dell’universo per essere completato: non accadrà. Ecco perché è deludente quando gli esperti che dovrebbero saperlo bene si nascondono timidamente dietro un “non ancora” o “non per molto tempo”. È un caso pericoloso di fuoco amico. Se fosse vero che Terminator non è ancora possibile e non lo sarà per molto tempo, dovremmo farci prendere dal panico. Subito. Fortunatamente, la vera risposta è “non ora, né mai”, per quanto ne sappiamo. Non abbiamo idea di come potremmo iniziare a progettare una vera ia, non ultimo perché abbiamo una limitatissima comprensione di come funzionano il nostro cervello e la nostra stessa intelligenza. È anche improbabile che la nostra concezione dell’intelligenza rimanga incontrastata, in termini di fenomeno unificato. Tutto ciò significa che non dobbiamo perdere il sonno per la possibile comparsa di qualche forma di ultraintelligenza. Come ho sostenuto nei capitoli precedenti, ciò che conta davvero è che la presenza crescente di tecnologie sempre più smart nelle nostre vite sta avendo un enorme impatto sul modo in cui concepiamo noi stessi, il mondo e le interazioni che intratteniamo tra noi e con il mondo. Il punto non è che le nostre macchine siano coscienti, intelligenti o capaci di conoscere qualcosa come noi. Non lo sono. Un sacco di macchine possono fare cose incredibili, come giocare a dama, scacchi e Go o al quiz show Jeopardy! meglio di noi. Eppure, sono tutte versioni di una macchina di Turing, un modello astratto che fissa i limiti di ciò che può essere realizzato da un computer tramite la sua logica matematica. Anche i computer quantistici sono vincolati dagli stessi limiti, i limiti di ciò che può essere calcolato (le cosiddette funzioni computabili). Nessuno sembra capace di spiegare come un ente cosciente, intelligente ed empatico possa emergere da una macchina di Turing. Il punto è che le nostre tecnologie smart, anche grazie all’enorme quantità di dati disponibili e a programmi molto sofisticati, sono sempre più capaci di svolgere un numero crescente di compiti meglio di noi, compresa la previsione dei nostri stessi comportamenti, senza dover essere affatto intelligenti. Per questo, non siamo gli unici agenti in grado di svolgere compiti con successo, tutt’altro. È quello che ho definito Quarta rivoluzione nella comprensione di noi stessi (Floridi, 2014a). Non siamo al centro dell’universo (Copernico), del regno biologico (Darwin) o del regno della razionalità (Freud). Dopo Turing, non siamo più al centro dell’infosfera né del mondo dell’elaborazione delle informazioni e dell’agire smart. Ironia della sorte, la bbc ha realizzato un breve video di due minuti,8 per introdurre quest’idea di una quarta rivoluzione, che vale la pena di guardare, ma ha commesso un errore alla fine, equiparando l’essere “migliore nel portare a termine i compiti” con l’essere “il più bravo a pensare”. Non ho mai sostenuto che le tecnologie digitali pensino meglio di noi, ma che possano fare sempre più cose meglio di come le facciamo noi senza pensare, limitandosi a elaborare quantità crescenti di dati in modo sempre più efficiente ed efficace. E se quest’ultima è la definizione di pensare usata per primeggiare nella discussione, allora stiamo avendo un dibattito linguistico.

			Condividiamo l’infosfera con le tecnologie digitali. Queste non sono la progenie di qualche superintelligenza fantascientifica, ma artefatti ordinari che ci sopravanzano in un numero crescente di compiti, nonostante non siano più intelligenti di un tostapane. Le loro capacità sono umilianti e ci fanno riconsiderare la nostra eccezionalità umana e il nostro ruolo speciale nell’universo, che rimane unico. Pensavamo di essere intelligenti perché sapevamo giocare a scacchi. Ora un telefono gioca meglio di un maestro di scacchi. Pensavamo di essere liberi perché potevamo comprare quello che volevamo. Ora i nostri modelli di spesa sono previsti, a volte addirittura anticipati, da dispositivi stupidi come una zucchina. Che cosa significa tutto questo per la comprensione che abbiamo di noi stessi? Questa è una domanda che vale la pena di indagare dal punto di vista filosofico.

			Il successo delle nostre tecnologie dipende in gran parte dal fatto che, mentre speculavamo sulla possibilità dell’ultraintelligenza, abbiamo sempre più avvolto il mondo per mezzo di così tanti dispositivi, sensori, applicazioni e dati da diventare un ambiente adattato alle ict, dove le tecnologie possono sostituirci senza disporre di alcuna comprensione, stato mentale, intenzione, interpretazione, stato emotivo, abilità semantiche, coscienza, autocoscienza o intelligenza flessibile. La memoria (come quella presente in algoritmi e immensi set di dati) supera l’intelligenza quando si tratta di far atterrare un aereo, individuare il percorso più veloce da casa all’ufficio o scoprire il prezzo migliore per il nostro prossimo frigorifero. Le tecnologie digitali possono fare sempre più cose meglio di noi, elaborando quantità crescenti di dati e migliorando le loro prestazioni, analizzando il proprio output come input per le operazioni successive. AlphaGo, il programma computerizzato sviluppato da Google DeepMind, ha sconfitto al gioco da tavolo Go il miglior giocatore del mondo, perché ha potuto usufruire di un database di circa 30 milioni di mosse e giocare migliaia di partite contro se stesso, “apprendendo” ogni volta un po’ di più su come migliorare le proprie prestazioni (Silver, Huang, Maddison et al., 2016). Nel terzo capitolo, abbiamo osservato che AlphaZero ha imparato a giocare meglio di qualsiasi giocatore umano o altro software giocando contro se stesso, basandosi solo sulle regole del gioco. È come un sistema a due coltelli che può affilarsi da solo. Qual è la differenza? La stessa che intercorre tra noi e la lavastoviglie quando laviamo i piatti. Qual è la conseguenza? Che qualsiasi visione apocalittica dell’ia può essere ignorata. Il vero rischio non sta nella comparsa di qualche forma di ultraintelligenza, ma nel fatto che possiamo utilizzare male le nostre tecnologie digitali, a danno di una grande percentuale dell’umanità e dell’intero pianeta.





10.6 Conclusione: il problema non è hal ma l’umanità nel suo complesso


			Noi siamo e rimarremo, in qualsiasi prevedibile futuro, il problema, non la nostra tecnologia. Questo è il motivo per cui dovremmo accendere la luce nella stanza buia e guardare attentamente dove stiamo andando. Non ci sono mostri ma molti ostacoli da evitare, rimuovere o negoziare. Dovremmo preoccuparci della vera stupidità umana, non dell’intelligenza artificiale immaginaria, e concentrarci sulle sfide reali che l’ia solleva. Per concludere, elenco cinque di queste sfide, tutte parimenti importanti. Sono un modo per richiamare i temi trattati nei capitoli precedenti.

			Innanzitutto, dovremmo rendere l’ia adatta all’ambiente. Abbiamo bisogno delle tecnologie più smart che possiamo costruire per affrontare i mali molto concreti che opprimono l’umanità e il nostro pianeta, dai disastri ambientali e sanitari alle crisi finanziarie, dalla criminalità al terrorismo e alla guerra, alla carestia, alla povertà, all’ignoranza, alla disuguaglianza e agli standard di vita terribili. In secondo luogo, dovremmo rendere l’ia adatta all’uomo. L’ia dovrebbe essere usata per trattare le persone sempre come fini, mai come semplici mezzi, per parafrasare Kant. Terzo, dovremmo mettere la stupidità dell’ia al servizio dell’intelligenza umana. Abbiamo visto che milioni di posti di lavoro verranno trasformati, eliminati e creati; i benefici di tale trasformazione dovrebbero essere condivisi da tutti mentre i costi dovrebbero essere sostenuti dalla società. Quarto, dovremmo mettere il potere predittivo dell’ia al servizio della libertà e dell’autonomia. La commercializzazione di prodotti, l’influenza sui comportamenti, le spinte gentili date alle persone o la lotta alla criminalità e al terrorismo non dovrebbero mai minare la dignità umana. E, infine, dovremmo fare in modo che l’ia ci renda più umani. Il rischio grave è che possiamo usare male, troppo o troppo poco le nostre tecnologie smart, a danno della maggior parte dell’umanità e dell’intero pianeta.

			Gli adepti della singolarità e gli atei dell’ia proseguiranno per ora le loro diatribe sulla possibilità o l’impossibilità di una vera ia. Dobbiamo essere tolleranti. Ma non dobbiamo impegnarci in esse. Come suggerisce Virgilio a Dante nel terzo canto dell’Inferno: “Non ragioniam di lor, ma guarda e passa”. Perché il mondo ha bisogno di una buona filosofia e noi dobbiamo occuparci di problemi più urgenti.9 Permettetemi di ricordare la citazione di Winston Churchill adoperata nella prefazione: “Prima siamo noi a dare forma agli edifici; poi sono questi a dare forma a noi”. Ciò vale anche per l’infosfera e le tecnologie smart che la abitano. Faremmo meglio a comprenderle bene al più presto. Il prossimo capitolo discute alcuni suggerimenti su come possiamo procedere.



* * *





			 				 					1. https://www.theguardian.com/science/2009/apr/29/jack-good-codebreaker-obituary.



				 					2. Si veda per esempio Bostrom (2014) e Russell (2019). Consiglio la seguente recensione per comprendere i limiti dei libri precedenti: Leslie (2019).



				 					3. https://itif.org/publications/2016/01/19/artificial-intelligence-alarmists-win-itif%E2%80%99s-annual-luddite-award.



				 					4. Avvertenza: sono stato membro dell’Advisory Council di Google sul diritto all’oblio (Herritt, 2014) e dello sfortunato Advanced Technology External Advisory Council, nel 2019 (https://www.vox.com/future-perfect/2019/4/4/18295933/google-cancels-ai-ethics-bordo).



				 					5. https://www.youtube.com/watch?v=3Ox4EMFMy48.



				 					6. https://www.computerworld.com/article/2922442/stephen-hawking-fears-robots-could-take-over-in-100-years.html.



				 					7. https://www.theguardian.com/technology/2014/feb/22/computers-cleverer-than-humans-15-years.



				 					8. http://www.bbc.co.uk/programmes/p02hvcjm.



				 					9. Per ulteriori informazioni di base vedi Floridi (2009, 2011a, 2008a). Sul dibattito sulla natura tautologica e sull’informatività della logica vedi D’Agostino e Floridi (2009).





11


			La società per la buona ia

			Sommario In precedenza, nei capitoli 4-6, ho esaminato i concetti fondamentali che possono fondare una “società per la buona ia”. Poi, nei capitoli 7-9, ho discusso le sfide, le cattive e le buone pratiche che caratterizzano l’uso dei sistemi di ia. Nel decimo capitolo, ho sostenuto che tale sostituzione è una questione che coinvolge l’etica, la cultura e la politica, non uno scenario di fantascienza. In questo capitolo, e nei due successivi, mi concentro su alcune raccomandazioni costruttive e concrete, per valutare, sviluppare, incentivare e sostenere una buona ia, che in taluni casi possono essere attuate direttamente da soggetti politici nazionali o sovranazionali, mentre in altri casi possono essere guidate da altri soggetti interessati, dalla società civile agli attori privati e alle organizzazioni di settore. La speranza è che, se adottate, tali raccomandazioni possano gettare una solida base per l’istituzione di una società per la buona ia.





11.1 Introduzione: quattro modi per realizzare una società per la buona ia


			L’ia non è un altro dispositivo che deve essere regolamentato una volta che sia stato sviluppato. Abbiamo commesso l’errore di pensare in questi termini quando il web ha iniziato a crescere negli anni Novanta e oggi il pasticcio è evidente. Il web era sin dal principio destinato a essere un nuovo ambiente, parte della nostra infosfera, e non semplicemente un altro tipo di mezzo di comunicazione di massa, per cui avremmo dovuto regolarlo di conseguenza (le regole per uno spazio pubblico sono diverse da quelle per la comunicazione privata: Floridi, 2014a). Non dovremmo ripetere lo stesso errore. L’ia non è solo un servizio commerciale. È una forza potente, una nuova forma dell’agire smart nell’infosfera, che sta già rimodellando le nostre vite, interazioni e ambienti. Questa nuova forma dell’agire deve essere orientata verso il bene della società, di tutti i suoi membri e degli ambienti che condividiamo. Le forze di mercato saranno irrilevanti o insufficienti. Abbiamo bisogno di un approccio normativo. In questo capitolo, contribuisco all’attuale sforzo internazionale e collaborativo per lo sviluppo di una società per la buona ia proponendo 20 raccomandazioni.1 Se adottate, possono aiutare tutte le parti interessate a cogliere le opportunità offerte dall’ia, a evitare o quantomeno minimizzare e controbilanciare i rischi, a rispettare i principi discussi nel quarto capitolo e dunque a sviluppare una società per la buona ia.

			Come detto nei capitoli precedenti, non si tratta più di discutere se l’ia avrà o no un impatto importante su individui, società e ambienti. Il dibattito attuale verte invece su quanto questo impatto sarà positivo o negativo, per chi, in che modo, in quali luoghi e in quale orizzonte temporale. In altre parole, nel quarto capitolo ho scritto che le domande chiave ora sono chi, come, dove e quando sentirà l’impatto positivo o negativo dell’ia. Per inquadrare queste domande in modo più sostanziale e pratico, introduco qui quelle che possono essere considerate le quattro principali opportunità che l’ia offre alla società. Sono quattro perché affrontano i quattro punti fondamentali della nostra antropologia filosofica, cioè della nostra comprensione della dignità e del progresso dell’umanità:

			a)	la realizzazione autonoma di noi stessi, ovvero chi possiamo diventare;

			b)	l’agire umano, ovvero cosa possiamo fare;

			c)	le capacità individuali e sociali, ovvero cosa possiamo conseguire;

			d) 	la coesione sociale, ovvero come possiamo interagire gli uni con gli altri e con il mondo.

			In ciascun caso, l’ia può essere utilizzata per favorire la natura umana e le sue potenzialità, generando in tal modo opportunità; sottoutilizzata, creando in tal modo costi opportunità; sovrautilizzata o utilizzata male, creando in tal modo rischi. La Figura 11.1 offre una rapida panoramica, mentre i paragrafi seguenti offrono una spiegazione più dettagliata.



			Come indica la terminologia, il presupposto è che l’utilizzo dell’ia sia sinonimo di buona innovazione e applicazioni positive di questa tecnologia. Tuttavia, la paura, l’ignoranza, le preoccupazioni fuori luogo o le reazioni eccessive possono portare una società a sottoutilizzare le tecnologie di ia, cioè a impiegarle al di sotto del loro pieno potenziale, a causa di quelle che potrebbero essere ampiamente descritte come le ragioni sbagliate. Ciò può creare rilevanti costi opportunità. Potrebbe includere, per esempio, una regolamentazione farraginosa o mal concepita, investimenti insufficienti o una reazione pubblica negativa simile a quella che ha investito le colture geneticamente modificate (Imperial College London, 2017) e l’energia nucleare. Di conseguenza, i vantaggi offerti dalle tecnologie di ia potrebbero non essere pienamente realizzati dalla società. Questi pericoli derivano in gran parte da conseguenze non volute e sono di regola legati a buone intenzioni tradite. E, naturalmente, si deve anche tenere in considerazione i rischi associati a un involontario uso eccessivo o a un intenzionale cattivo uso delle tecnologie di ia, radicato, per esempio, in incentivi orientati male, avidità, geopolitica contraddittoria o intenti malevoli. Come rilevato nei capitoli settimo e ottavo, tutto – dalle truffe via e-mail alla cyberguerra su ampia scala – può essere accelerato o intensificato dall’uso malevolo delle tecnologie di ia (Taddeo, 2017b). E nuovi mali possono essere creati. La possibilità di progresso sociale rappresentata dalle suddette opportunità deve essere soppesata di contro al rischio che una manipolazione malevola sia resa possibile o potenziata dall’ia. Tuttavia, c’è il forte rischio che l’ia possa essere sottoutilizzata per paura di un uso eccessivo o improprio dovuto alla mancanza di un quadro etico e giuridico chiaro, come vedremo nel resto di questo capitolo.





11.2 Chi possiamo diventare: rendere possibile l’umana realizzazione di sé, senza svalutare le capacità umane


			L’ia può rendere possibile la realizzazione di sé, ossia la capacità delle persone di prosperare in termini di caratteristiche, interessi, capacità o abilità potenziali, aspirazioni e progetti di vita. Proprio come le invenzioni, per esempio la lavatrice, hanno sollevato le persone (in particolare le donne) dalla fatica di quel lavoro domestico, l’automazione “smart” di altri aspetti mondani della vita può restituire ancora più tempo per attività culturali, intellettuali e sociali e per un lavoro più interessante e gratificante. Più ia può facilmente voler dire più vita umana trascorsa in modo più intelligente. Il rischio in questo caso non risiede tanto nell’obsolescenza di alcune vecchie competenze e nell’emergerne di nuove, quanto piuttosto nel ritmo con cui ciò sta accadendo e nell’ineguale distribuzione dei costi e dei benefici che ne derivano. Una svalutazione molto rapida delle vecchie competenze e quindi un celere stravolgimento del mercato del lavoro e della natura dell’occupazione possono cogliersi a livello sia dell’individuo sia della società. A livello individuale, i lavori sono spesso intimamente legati all’identità personale, all’autostima e al ruolo o alla posizione sociale, tutti fattori che possono essere influenzati negativamente dalla sovrabbondanza, anche mettendo da parte il potenziale di gravi danni economici. Inoltre, a livello della società, la perdita di competenze in ambiti sensibili e che richiedono elevate capacità, come la diagnostica sanitaria o l’aviazione, può creare pericolose fragilità in caso di malfunzionamento dell’ia o di attacchi avversi. Promuovere lo sviluppo dell’ia a sostegno di nuove capacità e competenze, anticipando e mitigando al contempo il suo impatto su quelle tradizionali, richiederà sia uno studio approfondito sia idee potenzialmente radicali, come la proposta di qualche forma di “reddito di base universale”, di cui popolarità e uso sperimentale sono in crescita. Alla fine, abbiamo bisogno di una solidarietà intergenerazionale tra le persone svantaggiate di oggi e quelle avvantaggiate di domani, per garantire che la transizione dirompente tra il presente e il futuro sia la più equa possibile, per tutti. In altre parole, adottando una terminologia che purtroppo abbiamo dovuto tristemente apprezzare durante la pandemia da Covid-19, dobbiamo far diminuire la curva dell’impatto sociale delle tecnologie digitali, inclusa l’ia.





11.3 Cosa possiamo fare: migliorare l’agire umano, senza rimuovere la responsabilità umana


			L’ia sta fornendo una riserva crescente di capacità di “agire smart”. Messa al servizio dell’intelligenza umana, tale risorsa può potenziare enormemente l’agire umano. Possiamo fare di più, meglio e più velocemente, grazie al sostegno fornito dall’ia. Nel senso di “intelligenza aumentata”, l’ia potrebbe essere paragonata all’impatto avuto dal motore sulle nostre vite. Maggiore è il numero di persone che godranno delle opportunità e dei benefici di una tale riserva di agire smart “a disposizione”, migliori saranno le nostre società. La responsabilità è dunque essenziale, in considerazione del tipo di ia che sviluppiamo, di come la utilizziamo e se ne condividiamo vantaggi e benefici con tutti. Naturalmente, il rischio corrispondente risiede nell’assenza di tale responsabilità. Ciò può verificarsi non solo perché disponiamo del quadro di riferimento sociopolitico errato, ma anche a causa di una mentalità da “scatola nera” (black-box), per la quale i sistemi di ia che presiedono a un processo decisionale sono concepiti come qualcosa che si colloca al di là della comprensione umana, e quindi del controllo. Queste preoccupazioni si applicano non solo a casi di alto profilo, come i decessi causati dai veicoli autonomi, ma anche a usi più comuni ma parimenti rilevanti, come le decisioni automatizzate sulla libertà vigilata o sull’affidabilità creditizia. Tuttavia, il rapporto tra grado e qualità dell’agire di cui godono le persone e la quantità di agire che deleghiamo ai sistemi autonomi non è a somma zero, né pragmaticamente né eticamente. Infatti, se viene sviluppata in modo equilibrato, l’ia offre l’opportunità di migliorare e moltiplicare le possibilità dell’agire umano. Consideriamo gli esempi di “moralità distribuita” nei sistemi da umano a umano come il prestito tra pari (peer-to-peer) (Floridi, 2013). L’agire umano può in definitiva essere sostenuto, migliorato e ampliato dall’incorporazione di “quadri di facilitazione”, progettati per migliorare la probabilità di risultati moralmente buoni, nell’insieme di funzioni che deleghiamo ai sistemi di ia. I sistemi di ia potrebbero, se progettati in modo efficace, amplificare e rafforzare i sistemi morali condivisi.





11.4 Cosa possiamo conseguire: incrementare le capacità della società, senza ridurre il controllo umano


			L’ia offre molte opportunità per migliorare e incrementare le capacità degli individui e della società più in generale. Che sia per la prevenzione e la cura delle malattie o per l’ottimizzazione di trasporti e logistica, l’uso di tecnologie di ia presenta innumerevoli possibilità per reinventare la società, migliorando radicalmente ciò di cui gli esseri umani sono collettivamente capaci. Una migliore collaborazione e quindi obiettivi più ambiziosi possono essere sostenuti da una maggiore ia. L’intelligenza umana potenziata dall’ia può individuare nuove soluzioni a problemi vecchi e nuovi, da una distribuzione più equa o più efficiente delle risorse a un approccio più sostenibile al consumo. Proprio perché tali tecnologie hanno il potenziale per essere così potenti e dirompenti, introducono anche rischi della stessa grandezza. Ci saranno sempre più contesti in cui potremmo non doverci trovare più “nel o sul processo” (vale a dire, essere parte del processo stesso o almeno in condizione di controllarlo), se possiamo delegare i nostri compiti all’ia. Tuttavia, se ci affidiamo all’uso delle tecnologie di ia per incrementare le nostre capacità in modo sbagliato, possiamo delegare compiti importanti e, soprattutto, decisioni cruciali a sistemi autonomi che dovrebbero rimanere almeno in parte soggetti a supervisione, scelta e correzione umane. Ciò a sua volta può ridurre la nostra capacità di monitorare le prestazioni di tali sistemi (non essendo più “sul processo”) e di prevenire o correggere gli errori o i danni che si verificano (“dopo il processo”). È anche possibile che questi potenziali danni si accumulino e diventino radicati, poiché un numero crescente di funzioni è delegato a sistemi artificiali. È dunque tassativo trovare un equilibrio tra il perseguimento delle ambiziose opportunità offerte dall’ia per migliorare la vita umana e ciò che possiamo conseguire, da un lato, e, dall’altro, assicurarci di mantenere il controllo su questi importanti sviluppi e sui loro effetti.





11.5 Come possiamo interagire: coltivare la coesione sociale, senza erodere l’autodeterminazione umana


			Dal cambiamento climatico e dalla resistenza antimicrobica alla diffusione del nucleare e al fondamentalismo, i problemi globali sono caratterizzati da livelli sempre più elevati di complessità di coordinamento, il che significa che possono essere affrontati con successo soltanto se tutte le parti interessate progettano insieme e condividono le soluzioni e cooperano per realizzarle. L’ia, con le sue soluzioni ad alta intensità di dati e basate su algoritmi, può contribuire fortemente ad affrontare tale complessità di coordinamento, supportando una maggiore coesione e collaborazione della società. Per esempio, gli sforzi per affrontare il cambiamento climatico hanno messo in luce l’esigenza di rispondere a tale sfida creando una risposta coesa, sia all’interno delle società sia tra di esse. La portata di questa sfida è tale che potremmo presto dover decidere come trovare un equilibrio tra l’ingegnerizzazione diretta del clima e il design di strutture sociali volte a incoraggiare una drastica riduzione delle emissioni nocive. Quest’ultima opzione potrebbe essere sostenuta da un sistema algoritmico diretto ad alimentare la coesione sociale. Un tale sistema non dovrebbe essere imposto dall’esterno; dovrebbe essere il risultato di una scelta autoimposta, non dissimile dalla scelta di non acquistare cioccolata se abbiamo scelto di stare a dieta, o di programmare la sveglia per svegliarci presto. “L’autonoma spinta gentile” (self-nudging) per comportarsi in modi socialmente preferibili è la migliore forma di condizionamento e l’unica che preserva l’autonomia (Floridi, 2015b, 2016f). È il risultato di decisioni e scelte umane, ma può fare affidamento su soluzioni di ia per essere attuata e resa più agevole. Tuttavia, il rischio è che i sistemi di ia possano erodere l’autodeterminazione umana, poiché possono portare a cambiamenti non pianificati e voluti nei comportamenti umani per adattarsi alle routine che agevolano il funzionamento dell’automazione e rendono la vita delle persone più facile. Il potere predittivo e l’incessante spinta gentile dell’ia, anche se non intenzionali, dovrebbero essere posti al servizio dell’autodeterminazione umana e favorire la coesione sociale, senza minare la dignità o il progresso umani (Milano, Taddeo, Floridi, 2019, 2020).





11.6 Raccomandazioni per una società della buona ia


			Considerate unitariamente, le quattro opportunità descritte sopra e le loro corrispondenti sfide dipingono un quadro misto sull’impatto dell’ia sulla società, le persone e gli ambienti che queste condividono. Accettare la presenza di trade-off, cogliere le opportunità mentre si lavora per anticipare, evitare o ridurre con decisione al minimo i rischi migliorerà la prospettiva che le tecnologie dell’ia promuovano la dignità e il progresso umani. Garantire risultati socialmente preferibili (equi) dell’ia si fonda sulla capacità di risolvere la tensione tra l’attuazione dei benefici e la mitigazione dei potenziali danni dell’ia: in breve si tratta di evitare contemporaneamente l’uso improprio e il sottoutilizzo di queste tecnologie. In tale contesto, il valore di un approccio etico alle tecnologie di ia diventa più evidente. Nel sesto capitolo ho sostenuto che il rispetto della legge è semplicemente necessario (il minimo richiesto), ma significativamente insufficiente (non il massimo di quanto si possa fare). Con un’analogia, è la differenza tra giocare secondo le regole e giocare bene, in modo che si possa vincere la partita. Abbiamo bisogno di una strategia etica, di un’etica soft per la società digitale che vogliamo costruire. In linea con questa distinzione, seguono venti raccomandazioni per una società della buona ia. Ci sono quattro tipi di punti d’intervento: valutare, sviluppare, incentivare e sostenere. Alcune raccomandazioni possono essere attuate direttamente, per esempio dai responsabili politici nazionali o europei, in collaborazione con le parti interessate, dove necessario. Nel caso di altre raccomandazioni, i responsabili politici possono svolgere un ruolo di agevolazione per gli sforzi intrapresi o condotti da terzi. Il presupposto è che, al fine di creare una società della buona ia, i principi etici identificati nel quarto capitolo dovrebbero essere incorporati in modo predefinito nelle pratiche dell’ia. In particolare, abbiamo visto nel nono capitolo che l’ia può e deve essere disegnata e sviluppata in modo da ridurre le disuguaglianze, incrementare il potenziamento sociale, nel rispetto della dignità e dell’autonomia umane, e accrescere i benefici condivisi da tutti, in modo equo. È particolarmente importante che l’ia sia spiegabile, poiché la spiegabilità è uno strumento fondamentale per costruire la fiducia del pubblico nella tecnologia e la sua comprensione. La creazione di una società della buona ia richiede un approccio che tenga conto di tutte le parti interessate (multistakeholder), che è il modo più efficace per assicurare che l’ia soddisfi le esigenze della società, consentendo a sviluppatori, utenti e legislatori di essere tutti coinvolti e di collaborare sin dal principio. Inevitabilmente, diverse cornici culturali informano l’atteggiamento nei confronti delle nuove tecnologie.

			Un ultimo commento è utile prima di presentare le raccomandazioni. Il seguente approccio europeo vuole essere complementare con altri approcci e non dovrebbe essere erroneamente considerato una sorta di adesione a una prospettiva eurocentrica. Allo stesso modo in cui ho avuto bisogno di fondare le considerazioni etiche nell’ottavo capitolo in un quadro giuridico specifico, in quel caso il diritto penale inglese, senza perdere in generalità, così la scelta di contestualizzare le seguenti raccomandazioni nel quadro del contesto europeo ha solo lo scopo di fornire loro un valore concreto e attuabile. Non c’è nulla nelle raccomandazioni che le renda relative al contesto europeo in modo esclusivo; per cui restano universalizzabili. Perché non importa dove viviamo nel mondo, dovremmo tutti impegnarci nello sviluppo delle tecnologie di ia in modo da assicurare la fiducia delle persone, servire l’interesse pubblico, rafforzare la responsabilità sociale condivisa e sostenere l’ambiente.

			1. Valutare la capacità delle istituzioni esistenti, come i tribunali civili nazionali, di riparare agli errori commessi o ai danni procurati dai sistemi di ia. Tale valutazione dovrebbe considerare la presenza di basi sostenibili e concordate a maggioranza per la responsabilità dalla fase di design in avanti, al fine di ridurre negligenze e conflitti (vedi anche la raccomandazione 5).2

			2. Valutare quali compiti e funzioni decisionali non dovrebbero essere delegati ai sistemi di ia, tramite l’utilizzo di meccanismi partecipativi per garantire di restare in linea con i valori della società e la comprensione dell’opinione pubblica. Tale valutazione dovrebbe tenere conto della normativa vigente ed essere sostenuta da un dialogo continuo fra tutte le parti interessate (compresi governo, industria e società civile) per esaminare in che modo l’ia ha un impatto sulle opinioni della società (in accordo con la raccomandazione 17).

			3. Valutare se le normative vigenti sono sufficientemente fondate sull’etica per offrire un quadro normativo in grado di stare al passo con gli sviluppi tecnologici. Ciò può comprendere un quadro di principi chiave applicabili a problemi urgenti e/o imprevisti.

			4. Sviluppare un quadro per migliorare la spiegabilità dei sistemi di ia che assumono decisioni socialmente rilevanti. Centrale in questo quadro è la capacità degli individui di ottenere una spiegazione fattuale, diretta e chiara del processo decisionale, soprattutto in caso di conseguenze indesiderate. Ciò richiede verosimilmente lo sviluppo di strutture specifiche per diversi settori, con il coinvolgimento delle associazioni professionali in questo processo, unitamente a esperti nei campi della scienza, del commercio, del diritto e dell’etica.

			5. Sviluppare procedure giuridiche adeguate e migliorare l’infrastruttura digitale del sistema giudiziario per consentire la verifica delle decisioni algoritmiche nei tribunali. Ciò include presumibilmente la creazione di un quadro per la spiegabilità dell’ia specifico per il sistema giuridico, come indicato nella raccomandazione 4. Esempi di procedure adeguate possono includere la divulgazione ammissibile di informazioni commerciali sensibili nelle controversie in materia di proprietà intellettuale e – dove la divulgazione comporti rischi inaccettabili, per esempio per la sicurezza nazionale – la configurazione dei sistemi di ia per adottare soluzioni tecniche per impostazione predefinita, come prove a conoscenza zero, al fine di valutare la loro affidabilità.

			6. Sviluppare meccanismi di audit per i sistemi di ia al fine di identificare le conseguenze indesiderate, come pregiudizi ingiusti, e (per esempio in collaborazione con il settore assicurativo) un meccanismo di solidarietà per affrontare i rischi gravi nei settori ad alta intensità di ia. Tali rischi potrebbero essere mitigati dall’adozione fin dal principio di meccanismi volti a coinvolgere tutte le parti interessate. L’esperienza predigitale indica che, in alcuni casi, potrebbero essere necessari un paio di decenni prima che la società si metta al pari della tecnologia, bilanciando in modo idoneo diritti e protezione per ripristinare la fiducia. Quanto prima saranno coinvolti utenti e governi – come ormai reso possibile dalle tecnologie digitali – tanto più breve sarà questo ritardo.

			7. Sviluppare un sistema di riparazione o un meccanismo per rimediare o compensare un errore o un torto causati dall’ia. Per promuovere la fiducia del pubblico nell’ia, la società ha bisogno di un meccanismo di riparazione ampiamente accessibile e affidabile per i danni inflitti, i costi sostenuti o altri torti causati dalla tecnologia. Tale meccanismo richiederà necessariamente di adottare un criterio chiaro ed esaustivo di attribuzione della responsabilità a esseri umani e/o organizzazioni. Si potrebbero trarre indicazioni dall’industria aerospaziale, per esempio, che dispone di un sistema collaudato per gestire le conseguenze non volute in modo completo e serio. Lo sviluppo di questo processo deve derivare dalla valutazione della capacità esistente delineata nella raccomandazione 1. Se venisse identificata una mancanza di capacità, dovrebbero essere sviluppate soluzioni istituzionali ulteriori a livello nazionale e/o della ue, per consentire alle persone di chiedere un risarcimento. Tali soluzioni possono includere:

			–	un “difensore civico dell’ia” per garantire il controllo sugli usi presumibilmente sleali o iniqui dell’ia;

			–	un processo guidato per la proposizione di un reclamo simile a una richiesta di libertà di informazione;

			–	lo sviluppo di meccanismi di assicurazione della responsabilità civile, che sarebbero richiesti come complemento obbligatorio di classi specifiche di offerte di ia nella ue e in altri mercati. Ciò garantirebbe che l’affidabilità relativa degli artefatti basati sull’ia, in particolare nella robotica, si rifletta nei prezzi assicurativi e pertanto nei prezzi di mercato dei prodotti concorrenti.3

			Qualunque siano le soluzioni sviluppate, è probabile che queste trovino fondamento nel quadro di riferimento per l’intelligibilità proposto nella raccomandazione 4.

			8. Sviluppare metriche condivise per misurare l’affidabilità dei prodotti e servizi di ia, che possano essere attuate da una nuova organizzazione o da un’adeguata organizzazione esistente. Tali metriche servirebbero come base per un sistema che consente una valutazione comparativa guidata dalle indicazioni degli utenti di tutte le offerte di ia commercializzate. In tal modo, oltre al prezzo di un prodotto, è possibile sviluppare e fare riferimento a un indice di affidabilità dell’ia. Questo “indice di comparazione della fiducia” per l’ia migliorerebbe la comprensione del pubblico e creerebbe competitività nello sviluppo di un’ia più sicura e più socialmente vantaggiosa (per esempio, “Iwantgreatai.org”). Sul lungo termine tale sistema potrebbe costituire la base per un più ampio sistema di certificazione dei prodotti e servizi meritevoli, amministrato dall’organizzazione indicata in questo contesto e/o dall’agenzia di supervisione proposta nella raccomandazione 9. L’organizzazione potrebbe anche promuovere lo sviluppo di codici di condotta (vedi raccomandazione 18). Inoltre, coloro che possiedono o forniscono input ai sistemi di ia e ne traggono profitto potrebbero essere incaricati di finanziare e/o contribuire allo sviluppo di programmi di alfabetizzazione dei consumatori al riguardo dell’ia, nel proprio interesse.

			9. Sviluppare una nuova agenzia di supervisione della ue responsabile della protezione del benessere pubblico tramite la valutazione scientifica e la supervisione di prodotti, software, sistemi o servizi di ia. Tale agenzia potrebbe essere analoga, per esempio, all’agenzia europea per i medicinali. Allo stesso modo, dovrebbe essere sviluppato un sistema di monitoraggio “successivo all’immissione” nel mercato dell’ia, simile per esempio a quello disponibile per i farmaci, con obblighi di segnalazione per alcune parti interessate e meccanismi agevolati di segnalazione per altri utenti.

			10. Sviluppare un osservatorio europeo per l’ia. La missione dell’osservatorio sarebbe quella di monitorare gli sviluppi, fornire un forum per alimentare il dibattito e il consenso, fornire un archivio per la letteratura e il software sull’ia (compresi concetti e collegamenti alla letteratura disponibile) e formulare progressive raccomandazioni e linee guida per l’azione.

			11. Sviluppare strumenti legali e modelli contrattuali per gettare le basi per una collaborazione agile e gratificante umano-macchina nell’ambiente di lavoro. Plasmare la narrativa sul “futuro del lavoro” è strumentale per conquistare “cuori e menti”. In linea con “un’Europa che protegge”, l’idea di “innovazione inclusiva” e il tentativo di agevolare la transizione verso nuovi tipi di posti di lavoro, potrebbe essere istituito un Fondo europeo di adeguamento all’ia sulla falsariga del Fondo europeo di adeguamento alla globalizzazione.

			12. Incentivare finanziariamente, a livello europeo, lo sviluppo e l’uso di tecnologie di ia nella ue che siano socialmente preferibili (non semplicemente accettabili) e rispettose dell’ambiente (non solo sostenibili ma anche favorevoli all’ambiente). Ciò comprende l’elaborazione di metodologie che possono contribuire a valutare se i progetti di ia siano socialmente preferibili e rispettosi dell’ambiente. In tal senso, l’adozione di un “approccio basato su sfide” (vedi le sfide del darpa) può incoraggiare la creatività e promuovere la concorrenza nello sviluppo di soluzioni specifiche di ia che siano eticamente corrette e nell’interesse del bene comune.

			13. Incentivare finanziariamente uno sforzo di ricerca europeo sostenuto, esteso e coerente, ritagliato sulle caratteristiche specifiche dell’ia come settore scientifico di indagine. Ciò dovrebbe tradursi nella missione chiaramente orientata a far progredire l’ia per il bene sociale, in modo da fungere da contrappeso unitario alle tendenze dell’ia meno attente alle opportunità sociali.

			14. Incentivare finanziariamente la cooperazione e il dibattito interdisciplinari e intersettoriali concernenti i luoghi di intersezione fra tecnologia, questioni sociali, studi giuridici ed etica. I dibattiti sulle sfide tecnologiche possono essere in ritardo rispetto all’effettivo progresso tecnico, ma se sono strategicamente informati da un gruppo diversificato e rappresentativo dei vari interessi, possono guidare e promuovere l’innovazione tecnologica nella giusta direzione. L’etica dovrebbe aiutare a cogliere le opportunità e ad affrontare le sfide, e non solo a descriverle. È cruciale a tal proposito che la diversità permei il design e lo sviluppo dell’ia, in termini di genere, classe, etnia, disciplina e altre dimensioni rilevanti, al fine di incrementare l’inclusione, la tolleranza e la ricchezza di idee e prospettive.

			15. Incentivare finanziariamente l’inclusione di considerazioni etiche, giuridiche e sociali nei progetti di ricerca sull’ia. Parallelamente, incentivare revisioni periodiche della normativa vigente per verificare in che misura promuova l’innovazione socialmente positiva. Considerate unitariamente, queste due misure contribuiranno a garantire che la tecnologia dell’ia abbia al centro l’etica e che le scelte politiche siano orientate all’innovazione.

			16. Incentivare finanziariamente lo sviluppo e l’uso di zone speciali legalmente deregolamentate all’interno della ue per la verifica empirica e lo sviluppo dei sistemi di ia. Queste zone possono assumere la forma di un “laboratorio vivente” (o Tokku), basandosi sull’esperienza delle esistenti “autostrade di prova” (o Teststrecken). Oltre ad allineare più strettamente l’innovazione con il livello di rischio prescelto dalla società, esperimenti realizzati tramite sandbox, come quelli appena visti, contribuiscono a dare una formazione pratica e a promuovere la responsabilità e l’accettabilità sin dalla fase iniziale. La “protezione by design” è tipica di questo quadro di riferimento.

			17. Incentivare finanziariamente la ricerca sulla percezione e la comprensione del pubblico dell’ia e delle sue applicazioni e l’attuazione di meccanismi strutturati di consultazione pubblica per progettare politiche e regole relative all’ia. Ciò può includere la sollecitazione diretta dell’opinione pubblica tramite metodi di ricerca tradizionali, come sondaggi di opinione e focus group, nonché approcci più sperimentali, come esempi simulati delle questioni etiche introdotte dai sistemi di ia o esperimenti nei laboratori di scienze sociali. Questa agenda di ricerca non dovrebbe servire solo a misurare l’opinione pubblica, ma dovrebbe portare anche, di conseguenza, alla co-creazione di politiche, standard, migliori pratiche e regole.

			18. Sostenere lo sviluppo di codici di condotta di autoregolamentazione per le professioni collegate a dati e ia, con la previsione di specifici doveri etici. Sulla falsariga di altre professioni socialmente sensibili, come medici o avvocati, l’adozione di una relativa certificazione di “ia etica” attraverso attestati di affidabilità può indurre le persone a comprendere i meriti dell’ia etica e perciò a farne richiesta ai fornitori. Le attuali tecniche di manipolazione dell’attenzione possono essere limitate attraverso questi strumenti di autoregolazione.

			19. Sostenere la capacità dei consigli di amministrazione delle società di assumersi la responsabilità delle implicazioni etiche delle società tecnologiche di ia. Per esempio, ciò può comprendere una migliore formazione per i consigli esistenti e il potenziale sviluppo di un comitato etico con poteri di controllo interno. Ciò potrebbe essere sviluppato all’interno della struttura esistente dei sistemi di consiglio sia a un livello sia a due livelli e/o unitamente allo sviluppo di una forma obbligatoria di “comitato di revisione etica aziendale” che deve essere adottata dalle organizzazioni che sviluppano o si avvalgono di sistemi di ia, per valutare i progetti iniziali e la loro attuazione rispetto ai principi fondamentali.

			20. Sostenere la creazione di piani di studio e attività di sensibilizzazione pubblica sull’impatto sociale, giuridico ed etico dell’ia. Ciò può includere:

			–	curricula per le scuole, favorendo l’inclusione dell’informatica tra le discipline di base da insegnare;

			–	iniziative e programmi di qualificazione nelle aziende che si occupano di tecnologie di ia, per formare i dipendenti sull’impatto sociale, giuridico ed etico del lavoro legato all’ia;

			–	una raccomandazione a livello europeo per includere l’etica e i diritti umani nella formazione degli scienziati dei dati e dell’ia e in altri curricula scientifici e ingegneristici che si occupano di sistemi computazionali e di ia;

			–	lo sviluppo di programmi simili per il pubblico in generale, con un’attenzione particolare per coloro che sono coinvolti in ogni fase della gestione della tecnologia, inclusi funzionari pubblici, politici e giornalisti;

			–	impegno a promuovere iniziative più ampie come gli eventi ai for Good di itu e le ong che lavorano sugli obiettivi di sviluppo sostenibile delle Nazioni Unite.





11.7 Conclusione: la necessità di politiche concrete e costruttive


			L’umanità deve affrontare l’emergere di una tecnologia che porta con sé molte promesse entusiasmanti per tanti aspetti della vita umana, e tuttavia sembra sollevare anche grandi minacce. Questo capitolo, e in particolare le raccomandazioni formulate nel precedente paragrafo, cercano di spingere il timone nella direzione di risultati eticamente, socialmente ed ecologicamente preferibili derivanti da sviluppo, design e implementazione delle tecnologie di ia. Sulla base dei cinque principi etici per l’ia, espressi nel quarto capitolo, e dell’identificazione dei rischi e delle principali opportunità dell’ia per la società, esaminati nei capitoli 6-9, le raccomandazioni sono formulate con spirito di collaborazione e nell’interesse della creazione di risposte concrete e costruttive alle sfide sociali più urgenti sollevate dall’ia.

			Con il rapido ritmo del cambiamento tecnologico, si è tentati di ritenere il processo politico nelle odierne democrazie liberali antiquato, sorpassato e non più all’altezza del compito di preservare i valori e di promuovere gli interessi della società e dei suoi membri. Non sono d’accordo. Le raccomandazioni offerte qui, compresa la creazione di centri, agenzie, curricula e altre infrastrutture, sostengono la causa di un programma ambizioso, inclusivo, equo e sostenibile di elaborazione delle politiche e di innovazione tecnologica, che contribuirà ad assicurare i benefici e a mitigare i rischi dell’ia, per tutti e per il mondo che condividiamo.



* * *





			 				 					1. Sono il risultato di un progetto che ho ideato e presieduto nel 2017, denominato ai4People. Per saperne di più vedi Floridi, Cowls, Beltrametti et al. (2018).



				 					2. La determinazione della responsabilità può utilmente prendere in prestito dai giuristi dell’antica Roma questa formula: cuius commoda eius et incommoda (“chi trae un vantaggio da una situazione deve anche sopportarne gli inconvenienti”). Un buon principio vecchio di 2200 anni che ha una tradizione e un’elaborazione ben radicate potrebbe impostare correttamente il livello di astrazione di partenza in questo ambito.



				 					3. Naturalmente, nella misura in cui i sistemi di ia sono “prodotti”, il diritto generale sulla responsabilità civile si applica allo stesso modo all’ia come si applica in qualsiasi caso che coinvolga prodotti o servizi difettosi che danneggiano gli utenti o non funzionano come dichiarato o previsto.





12


			Il gambetto: l’impatto dell’ia sul cambiamento climatico

			Sommario Abbiamo visto nel quarto capitolo che i principi etici forniscono un quadro di valori di riferimento. Uno di questi principi, la beneficenza, include il sostegno al pianeta. In questo capitolo e nel tredicesimo, esplorerò ulteriormente tale requisito analizzando l’impatto ambientale positivo e negativo dell’ia. L’obiettivo è fornire raccomandazioni politiche per intraprendere un percorso di sviluppo dell’ia più verde e più rispettoso del clima, in particolare in linea con i valori e la normativa dell’Unione Europea. Il tempismo è fondamentale perché l’ia è già utilizzata oggi per modellare gli eventi relativi al cambiamento climatico e per contribuire a sostenere gli sforzi nella lotta al riscaldamento globale, in quanto può essere una grande forza positiva per una società equa e sostenibile.





12.1 Introduzione: il potere duplice dell’ia


			Abbiamo osservato nei capitoli precedenti che l’ia costituisce una nuova forma dell’agire, che può essere sfruttata per risolvere problemi ed eseguire compiti con straordinario successo. Tuttavia, è altrettanto chiaro che questa nuova e potente forma dell’agire deve essere sviluppata e governata eticamente (Floridi, 2013), al fine di evitare, ridurre al minimo e correggere ogni possibile impatto negativo e di assicurare che sia benefica per l’umanità e per il pianeta. Ciò vale anche o forse in particolare quando si tratta di cambiamento climatico, la più grande minaccia che l’umanità deve affrontare nel nostro secolo (Cowls, Tsamados, Taddeo et al., 2021a). Da un lato, l’ia può essere uno strumento estremamente potente nella lotta al cambiamento climatico, e naturalmente abbiamo bisogno di ogni possibile contributo (Ramchurn, Vytelingum, Rogers et al., 2012; Rolnick, Donti, Kaack et al., 2019). D’altro lato, ci sono insidie significative, sia etiche sia ambientali, che occorre evitare (Mendelsohn, Dinar, Williams, 2006; Anthony, Kanding, Selvan, 2020; Cowls, Tsamados, Taddeo et al., 2021a; Malmodin, Lundén, 2018). Comprendere queste insidie è pertanto fondamentale e, in tale prospettiva, misure politiche concrete possono essere d’aiuto, come vedremo in questo capitolo. Più specificamente, l’ia presenta due opportunità cruciali.

			La prima è scientifica. L’ia può contribuire a migliorare ed espandere la nostra attuale comprensione del cambiamento climatico rendendo possibile l’elaborazione di enormi volumi di dati per studiare le tendenze climatiche esistenti e prevedere gli sviluppi futuri, nonché l’impatto e il successo (o l’insuccesso) delle politiche. Sono state impiegate tecniche di ia per prevedere le variazioni della temperatura media globale (Ise, Oba, 2019; Cifuentes, Marulanda, Bello et al., 2020); per predire fenomeni climatici e oceanici come El Niño (Ham, Kim, Luo, 2019), sistemi nuvolosi (Rasp, Pritchard, Gentine, 2018) e onde di instabilità tropicale (Zheng, Li, Zhang et al., 2020); per comprendere meglio gli aspetti del sistema meteorologico, come le precipitazioni, in generale (Sønderby, Espeholt, Heek et al., 2020; Larraondo, Renzullo, Van Dijk et al., 2020) e in luoghi specifici quali la Malesia (Ridwan, Sapitang, Aziz et al., 2020), e i loro effetti a catena, come la domanda di risorse idriche (Shrestha, Manandhar, Shrestha, 2020; Xenochristou, Hutton, Hofman et al., 2020). Gli strumenti di ia possono anche contribuire ad anticipare gli eventi meteorologici estremi che sono più comuni a causa del cambiamento climatico globale, per esempio i danni causati dalle forti piogge (Choi, Kim, Kim et al., 2018), gli incendi boschivi (Jaafari, Zenner, Panahi et al., 2019) e altre conseguenze che ne derivano, come i modelli di migrazione umana (Robinson, Dilkina, 2018). In molti casi, le tecniche di ia possono aiutare a migliorare o accelerare i sistemi di previsione e predizione esistenti, per esempio qualificando automaticamente i dati dei modelli climatici (Chattopadhyay, Hassanzadeh, Pasha, 2020), migliorando le approssimazioni per la simulazione dell’atmosfera (Gagne, Christensen, Subramanian et al., 2020) e separando i segnali dal rumore nelle osservazioni climatiche (Barnes, Hurrel, Ebert-Uphoff et al., 2019). Tutto ciò fa parte di una tendenza ancora più generale: non esiste in pratica alcuna analisi scientifica o politica basata su evidenze che non sia alimentata da tecnologie digitali avanzate. In questo quadro, l’ia sta diventando parte integrante degli strumenti necessari per far progredire la nostra comprensione scientifica in molti ambiti.

			La seconda opportunità è pragmatica. L’ia può aiutare a fornire soluzioni più ecologiche ed efficaci, come il miglioramento e l’ottimizzazione della generazione e dell’uso dell’energia, contribuendo in modo proattivo alla lotta contro il riscaldamento globale. Combattere efficacemente il cambiamento climatico richiede un’ampia gamma di risposte, per mitigare gli effetti esistenti del cambiamento climatico e ridurre le emissioni attraverso la decarbonizzazione per prevenire un ulteriore riscaldamento. Per esempio, un rapporto Microsoft/PwC del 2018 ha stimato che l’utilizzo dell’ia per le applicazioni ambientali potrebbe far crescere il pil globale tra il 3,1 e il 4,4%, riducendo al contempo le emissioni di gas serra dall’1,5 al 4% entro il 2030 rispetto a uno scenario privo di interventi economici (business as usual) (Microsoft, 2018, p. 8). Una serie di tecniche basate sull’ia svolge già un ruolo chiave in molte di queste risposte (Inderwildi, Zhang, Wang et al., 2020; Sayed-Mouchaweh, 2020). Ciò include, per esempio, l’efficienza energetica nell’industria, in particolare nel settore petrolchimico (Narciso, Martins, 2020). Vi sono anche studi che hanno utilizzato l’ia per comprendere l’inquinamento industriale in Cina (Zhou, Xiao, Chen et al., 2016), l’impronta ecologica del calcestruzzo impiegato nelle costruzioni (Thilakarathna, Seo, Kristombu Baduge et al., 2020) e persino l’efficienza energetica nel trasporto marittimo (Perera, Mo, Soares, 2016). Un altro lavoro ha indagato l’uso dell’ia nella gestione della rete elettrica (Di Piazza, Di Piazza, La Tona et al., 2020), per prevedere il consumo energetico degli edifici (Fathi, Srinivasan, Fenner et al., 2020) e valutare la sostenibilità del consumo alimentare (Abdella, Kucukvar, Onat et al., 2020). L’ia può anche aiutare a prevedere le emissioni di carbonio sulla base delle tendenze attuali (Mardani, Liao, Nilashi et al., 2020; Wei, Yuwei, Chongchong, 2018) e l’impatto di politiche interventiste come la tassa sul carbonio (Abrell, Kosch, Rausch, 2019) e i sistemi di commercio del carbonio (Lu, Ma, Huang et al., 2020). L’ia potrebbe anche essere usata per monitorare la rimozione attiva del carbonio dall’atmosfera tramite il suo sequestro (Menad, Hemmati-Sarapardeh, Varamesh et al., 2019).

			Al di là di questi dati indicativi, il crescente utilizzo dell’ia per combattere i cambiamenti climatici può essere osservato anche dal punto di vista privilegiato delle principali istituzioni e delle iniziative su larga scala. L’European Lab for Learning & Intelligent Systems (ellis) ha un programma di apprendimento automatico per le scienze della Terra e del clima che mira a “modellare e comprendere il sistema terrestre con l’apprendimento automatico e la comprensione dei processi”.1 L’Agenzia spaziale europea ha anche istituito la sfida del gemello digitale della Terra (Digital Twin Earth Challenge), per fornire “previsioni sull’impatto del cambiamento climatico e risposte alle sfide della società”.2 Diverse università europee hanno lanciato iniziative e programmi di formazione dedicati a incoraggiare il potere dell’ia per il clima.3, 4, 5 Nel 2020, una ricerca su Cordis (il database europeo per la ricerca finanziata) per i progetti in corso che concernono il cambiamento climatico e l’ia ha prodotto un totale di 122 risultati (Cowls, Tsamados, Taddeo et al., 2021a).6 L’analisi di questi 122 progetti ha mostrato che essi sono ampiamente rappresentativi dal punto di vista sia geografico sia disciplinare. I progetti erano ben distribuiti in tutto il continente, anche se con una chiara inclinazione verso l’Europa occidentale in ragione del contesto in cui sono stati coordinati. Una grande maggioranza di progetti riguardava le scienze naturali e/o l’ingegneria e la tecnologia, ma un numero considerevole era anche radicato nelle scienze sociali. L’ampiezza dei temi toccati da questi progetti era estesa e abbracciava ambiti diversi come viticoltura, micologia e astronomia galattica.

			Risultano inoltre numerose iniziative private e senza scopo di lucro che si avvalgono dell’ia per combattere i cambiamenti climatici in tutto il mondo. L’ia per la Terra (ai for Earth) di Microsoft è un’iniziativa quinquennale da 50 milioni di dollari lanciata nel 2017, progettata per sostenere organizzazioni e ricercatori che adoperano l’ia e altre tecniche computazionali per affrontare vari aspetti della crisi climatica. Attualmente annovera 16 organizzazioni partner,7 ha rilasciato rilevanti strumenti open source8 e ha fornito sovvenzioni sotto forma di crediti di cloud computing per progetti che utilizzano l’ia per una varietà di scopi, dal monitoraggio dei cambiamenti climatici nell’Antartico alla protezione delle popolazioni di uccelli dopo gli uragani. Il programma di ia per il bene sociale (ai for Social Good) di Google sostiene 20 organizzazioni che adoperano l’ia per perseguire vari obiettivi socialmente vantaggiosi con finanziamenti e crediti di cloud computing, inclusi progetti che cercano di ridurre al minimo i danni alle colture in India, gestire meglio i rifiuti in Indonesia, proteggere le foreste pluviali negli Stati Uniti e migliorare la qualità dell’aria in Uganda.9 Nel frattempo, il programma di ia per il clima (ai for Climate) della società di sviluppo dell’ia Elementai10 fornisce competenze e opportunità di partenariato per migliorare l’efficienza energetica delle attività produttive e commerciali. Infine, come vedremo nel tredicesimo capitolo, esistono numerosi progetti di ia che sostengono gli obiettivi di sviluppo sostenibile delle Nazioni Unite.

			Tuttavia, lo sviluppo dell’ia per combattere il cambiamento climatico solleva anche due sfide principali. Una riguarda l’ia in generale: è la possibile esacerbazione di alcuni problemi sociali ed etici già associati all’ia, come pregiudizi ingiusti, discriminazione o opacità nel processo decisionale. Li ho esaminati nei capitoli settimo e ottavo. L’altra è peculiare del cambiamento climatico: si tratta di una sfida meno compresa che costituisce uno dei temi principali di questo capitolo. Riguarda il contributo al cambiamento climatico globale dovuto ai gas serra emessi dai dati di addestramento e dai sistemi di intelligenza artificiale ad alta intensità di calcolo. L’attuale mancanza di informazioni sulla fonte e sulla quantità di energia impiegata nella ricerca, nello sviluppo e nell’implementazione di modelli di ia rende difficile definire esattamente l’impronta ecologica dell’ia. Per affrontare questa sfida, al Digital Ethics Lab abbiamo studiato in modo specifico l’impronta ecologica della ricerca sull’ia e i fattori che influenzano le emissioni di gas serra dell’ia nella fase di ricerca e sviluppo (Cowls, Tsamados, Taddeo et al., 2021a). Abbiamo anche esaminato la mancanza di prove scientifiche relative al trade-off tra le emissioni necessarie per la ricerca, lo sviluppo e l’impiego dell’ia e il guadagno in termini di efficienza energetica e di risorse fornito dall’ia. Abbiamo concluso che sfruttare le opportunità offerte dall’ia per il cambiamento climatico globale è sia fattibile sia auspicabile, ma comporta un sacrificio (in termini di rischi etici e di incremento potenziale dell’impronta ecologica) in vista di un guadagno molto rilevante (una risposta più efficace ai cambiamenti climatici). Si tratta, in altre parole, di un gambetto ecologico, come ho evidenziato in Floridi (2014a), che richiede una governance reattiva ed efficace per diventare una strategia vincente. Per questo motivo, nel suddetto lavoro abbiamo offerto alcune raccomandazioni per i decisori politici e per i ricercatori e sviluppatori di ia, che sono state elaborate in modo da identificare e sfruttare le opportunità dell’ia per contrastare il cambiamento climatico, riducendo al contempo l’impatto del suo sviluppo sull’ambiente. Il presente capitolo si basa su tale lavoro e ne costituisce un aggiornamento.





12.2 L’ia e le “transizioni gemelle” dell’Unione Europea


			Nel 2020-2021, la presidentessa della Commissione europea Ursula von der Leyen ha abbracciato le “transizioni gemelle” digitale ed ecologica che plasmeranno il nostro futuro. Documenti politici di alto profilo hanno messo in evidenza come le iniziative digitali della ue come “Un’Europa adatta per l’era digitale”, inclusa l’ia, e obiettivi ecologici come il “Green Deal europeo” possano convergere, suggerendo un utile punto di partenza per l’elaborazione delle politiche. Il bisogno d’impulso finanziario derivante dalla pandemia da coronavirus ha anche aperto strade per riadattare l’assistenza pubblica, al fine di promuovere queste “transizioni gemelle”. La Commissione ha concepito le due transizioni come profondamente interconnesse, a tal punto che la proposta del “Green Deal europeo” era ricca di riferimenti al ruolo degli strumenti digitali, con la tabella di marcia pubblicata nel dicembre 2019 che rilevava:

			Le tecnologie digitali sono un fattore determinante per raggiungere gli obiettivi di sostenibilità del Green Deal in molti settori […] [e le tecnologie] come l’intelligenza artificiale […] possono accelerare e massimizzare l’impatto delle politiche per affrontare il cambiamento climatico e proteggere l’ambiente

			in particolare nei settori delle reti energetiche, prodotti di consumo, monitoraggio dell’inquinamento, mobilità, alimentazione e agricoltura. L’approccio della Commissione è consonante con l’idea, che articolerò nel quattordicesimo capitolo, di un nuovo matrimonio tra il verde di tutti i nostri habitat e il blu di tutte le nostre tecnologie digitali (Floridi, Nobre, 2020). L’impiego dell’ia per contrastare il cambiamento climatico è un esempio lampante di tale matrimonio tra il verde e il blu (Floridi, 2019b). Infine, molti dei documenti prodotti dalla Commissione per dare corpo alla sua visione digitale e ambientale hanno fatto riferimento all’ia come strumento chiave, in particolare in relazione a “Destinazione Terra”, un piano ambizioso per creare un modello digitale della Terra e simulare l’attività umana come mezzo per testare la potenziale efficacia delle politiche ambientali europee. L’ia è esplicitamente menzionata come una componente chiave dell’iniziativa “Destinazione Terra”. Infatti, nel 2021, una ricerca su Cordis, il database europeo per la ricerca finanziata, ha rilevato l’esistenza di 122 progetti in tutto il continente che usano l’ia per affrontare gli aspetti del cambiamento climatico in una varietà di aree diverse. Anche la proposta di normativa della ue sull’ia ha individuato nell’ia un potenziale grande strumento a sostegno delle politiche sostenibili (European Commission, 2021). In sintesi, l’attenzione europea sulla “buona ia” è in linea sia con i valori e i finanziamenti della ue sia con gli sforzi scientifici in tutto il mondo. Tuttavia, nonostante tutti i documenti prodotti dalla Commissione europea in cui l’ia è stata identificata come strumento chiave e malgrado tutte le opportunità evidenziate, vi è la tendenza a trascurare le sfide che devono essere affrontate per garantire che gli strumenti di ia siano adottati con successo e in modo sostenibile.





12.3 ia e cambiamento climatico: sfide etiche


			Dalle preoccupazioni etiche e di privacy alla notevole quantità di energia che occorre per l’addestramento e l’implementazione di strumenti di ia, la comprensione e l’adozione di misure per affrontare tali sfide è fondamentale per assicurare la sostenibilità dell’utilizzo dell’ia nella lotta contro il riscaldamento globale.

			Una difficoltà intrinseca nell’affrontare qualsiasi sforzo per chiarire il ruolo che l’ia può svolgere nella lotta ai cambiamenti climatici consiste nel determinare dove la tecnologia viene già utilizzata in modo equo e sostenibile. Sono stati compiuti numerosi sforzi per creare una panoramica accurata di come l’ia viene sfruttata in tutto il mondo nei progetti climatici, ma il rapido ritmo dello sviluppo della tecnologia ha inevitabilmente limitato l’accuratezza di ogni singola ricerca. Alcuni approcci si sono concentrati sugli obiettivi di sviluppo sostenibile (oss) delle Nazioni Unite, in particolare quelli che si occupano più specificamente di questioni legate al clima, come punto di partenza per identificare progetti di ia. Analizzerò questo approccio nel tredicesimo capitolo. Qui è sufficiente anticipare che l’iniziativa di ricerca dell’Università di Oxford sull’ia per gli oss11 – un progetto triennale che ho diretto in collaborazione con la Saïd Business School, che fornisce un database di progetti di ia che sostengono gli oss – include decine di progetti relativi all’obiettivo 13, che si concentra in particolare sul cambiamento climatico. Altri database che utilizzano gli oss come punto di partenza contengono, tuttavia, un numero molto inferiore di progetti, a testimonianza del lavoro che deve ancora essere svolto per migliorare la comprensione delle numerose aree legate al clima in cui è stata impiegata l’ia.

			In questo contesto, è importante ricordare che una sfida chiave riguarda i rischi etici legati all’ia più in generale. Anche se abbiamo osservato nei capitoli settimo e ottavo che tali rischi sono molto più rilevanti in aree come l’assistenza sanitaria e la giustizia penale, dove la privacy dei dati è cruciale e le decisioni hanno un impatto molto maggiore sugli individui, è comunque vitale ridurre al minimo i rischi che possono sorgere quando si applicano soluzioni di ia ai cambiamenti climatici. Poiché i modelli di ia vengono “addestrati” utilizzando set di dati esistenti, è possibile introdurre pregiudizi in tali modelli a causa dei set di dati scelti per l’addestramento. Immaginiamo, per esempio, di impiegare l’ia per stabilire dove installare stazioni di ricarica per veicoli elettrici. L’utilizzo dei modelli guida esistenti delle auto elettriche potrebbe distorcere i dati a favore di una fascia demografica più ricca a causa della prevalenza relativamente più elevata nell’uso di veicoli elettrici tra le fasce di reddito più alte. Ciò creerebbe, a sua volta, un ulteriore ostacolo all’incremento nell’utilizzo di auto elettriche nelle aree meno abbienti. Un’altra potenziale insidia etica concerne l’erosione dell’autonomia umana. Affrontare il cambiamento climatico richiede un’azione coordinata su larga scala, che comprende cambiamenti sistemici del comportamento individuale. Occorre trovare un attento equilibrio tra la protezione dell’autonomia individuale e l’attuazione su larga scala di politiche e pratiche rispettose del clima. Vi è infine il problema della tutela della privacy individuale e di gruppo (Floridi, 2014c). Per esempio, nei sistemi di controllo progettati per ridurre l’impronta ecologica in una serie di contesti, come lo stoccaggio di energia (Dobbe, Sondermeijer, Fridovich-Keil et al., 2019), il riscaldamento e il raffreddamento industriale (Aftab, Chen, Chau et al., 2017) e l’agricoltura di precisione (Liakos, Busato, Moshou et al., 2018), l’efficacia dei sistemi di ia dipende da dati granulari sulla domanda di energia, spesso disponibili in tempo reale. Mentre molte soluzioni di ia implementate nella lotta contro il cambiamento climatico si basano su dati non personali, come quelli meteorologici e geografici, alcune strategie per limitare le emissioni possono necessitare di dati relativi al comportamento umano. Questa tensione è stata evidenziata nell’analisi12 di 13 paesi dell’Unione Europea prodotta dal Vodafone Institute for Society and Communications, che mostra come, se gli europei sono ampiamente disposti a condividere i propri dati per contribuire alla protezione dell’ambiente, una netta maggioranza (53%) lo farebbe solo a condizioni rigorose di protezione dei dati.





12.4 ia e cambiamento climatico: l’impronta ecologica


			Forse la più grande sfida legata all’uso dell’ia nell’affrontare il cambiamento climatico è valutare e rendere conto dell’impronta ecologica della tecnologia stessa. Dopotutto, sarebbe di scarso aiuto se le soluzioni di ia contribuissero ad alleviare un aspetto del cambiamento climatico esacerbandone un altro. L’ia (nel senso sia di modelli di addestramento sia di utilizzi) può consumare grandi quantità di energia e generare emissioni di gas serra (García-Martín, Faviola Rodrigues, Riley et al., 2019; Cai, Gan, Wang et al., 2020). Una quota significativa dell’impronta ecologica generata dall’ia è legata alla potenza di calcolo necessaria per addestrare i sistemi di ml. Tali sistemi vengono addestrati con grandi quantità di dati, il che richiede data center altrettanto potenti. E tali centri hanno bisogno di energia per funzionare. Rammento di aver ricordato alle persone in passato che il calore del laptop sulle gambe era un chiaro segno d’impatto ambientale. Oggi, in seguito all’avvento dell’apprendimento profondo (deep learning), un tipo di ml basato su algoritmi che apprendono da enormi quantità di dati, la potenza di calcolo richiesta per l’addestramento di tale modello è raddoppiata ogni 3,4 mesi, con il conseguente aumento della domanda di energia. L’aumento del consumo di energia associato all’addestramento di modelli più ampi e all’adozione diffusa dell’ia è stato in parte mitigato dai miglioramenti nell’efficienza dell’hardware. Tuttavia, a seconda di dove e come l’energia è ottenuta, immagazzinata e trasmessa, l’incremento della ricerca di ia ad alta intensità di calcolo può generare significative conseguenze ambientali negative.

			Molti fattori contribuiscono all’impronta ecologica dell’ia. Uno di questi è la potenza di calcolo necessaria per addestrare i modelli di ml. Dal 2012, con l’emergere del deep learning come tecnica centrale dei sistemi di ia, la potenza di calcolo richiesta per addestrare tale modello ha conosciuto un incremento esponenziale (Amodei, Hernandez, 2018), con il conseguente aumento della domanda di energia e quindi delle emissioni di carbonio. Tale incremento nel consumo di energia associato all’addestramento di modelli più ampi e all’adozione diffusa dell’ia è stato in parte mitigato dai miglioramenti nell’efficienza dell’hardware, ma la sua impronta ecologica rimane relativa a dove e come l’elettricità è acquisita, immagazzinata e trasmessa. Per esempio, consideriamo gpt-3, il modello linguistico autoregressivo di terza generazione che utilizza il deep learning per produrre testi simili a quelli umani. Il suo sviluppo è problematico dal punto di vista ambientale. Secondo la documentazione pubblicata a maggio 2020, gpt-3 richiedeva una quantità di potenza di calcolo di diversi ordini di grandezza superiore rispetto al suo predecessore gpt-2, pubblicato solo un anno prima.

			A causa della mancanza di informazioni relative alle condizioni di addestramento e al processo di sviluppo di gpt-3, un certo numero di ricercatori, avvalendosi di metodologie diverse, ha tentato di stimare il costo di una singola sessione di addestramento (Anthony, Kanding, Selvan, 2020). Diversi fattori devono essere monitorati per determinare l’impronta ecologica di un sistema di ia, compreso il tipo di hardware utilizzato, la durata di una sessione di addestramento, il numero di reti neurali addestrate, l’ora del giorno, l’impiego di memoria e le risorse di energia adoperate dalla rete energetica che fornisce l’elettricità (Henderson, Hu, Romoff et al., 2020). L’assenza di alcuni di questi dati può distorcere le valutazioni dell’impronta ecologica. Pertanto, è possibile stimare il costo di una singola sessione di addestramento per gpt-3, ma la pubblicazione non rivela quanti modelli sono stati addestrati per ottenere risultati pubblicabili. Si consideri che è comune per i ricercatori di ia addestrare in primo luogo migliaia di modelli (Schwartz, Dodge, Smith et al., 2020). Nonostante tutte queste limitazioni, usufruendo delle informazioni relative alla quantità di potenza di calcolo e al tipo di hardware usato dai ricercatori di Openai per addestrare gpt-3 (Brown, Mann, Ryder et al., 2020), facendo ipotesi sul resto delle condizioni di addestramento del modello (vedi Cowls, Tsamados, Taddeo et al., 2021a) per ulteriori informazioni) e utilizzando (Lacoste, Luccioni, Schmidt et al., 2019) il calcolatore dell’impatto del carbonio, abbiamo stimato che una singola sessione di addestramento di gpt-3 avrebbe prodotto 223.920 chilogrammi di co2 (o equivalente – co2eq). Se il fornitore di servizi cloud fosse stato Amazon Web Services, la sessione di addestramento avrebbe prodotto 279.900 chili di co2eq. Ciò non considera le tecniche di contabilizzazione e compensazione dei fornitori per ottenere emissioni “pari a zero”. In confronto, una tipica autovettura negli Stati Uniti emette circa 4600 chili di co2eq all’anno, il che significa che una singola sessione di addestramento che utilizza Microsoft Azure emetterebbe tanto quanto 49 autovetture in un anno (epa, us, 2016). Inoltre, la geografia è rilevante. Per esempio, costa dieci volte di più in termini di co2eq addestrare un modello utilizzando le reti energetiche in Sudafrica piuttosto che in Francia.

			La crescente disponibilità di enormi quantità di dati è stata uno dei principali fattori che hanno alimentato l’ascesa dell’ia. A ciò hanno contribuito anche i nuovi metodi elaborati per sfruttare la legge di Moore, secondo la quale le prestazioni dei microchip raddoppiano ogni due anni. L’introduzione di chip con più core di processore ha accelerato enormemente tale sviluppo (Thompson, Greenewald, Lee et al., 2020). Questa innovazione ha consentito lo sviluppo di sistemi di ia sempre più complessi, ma ha anche ampliato significativamente la quantità di energia necessaria per ricercarli, addestrarli e attivarli. La tendenza è ben nota e i principali operatori di data center, come i fornitori di servizi cloud Microsoft Azure e Google Cloud, hanno adottato misure significative per ridurre la propria impronta ecologica, investendo in infrastrutture efficienti dal punto di vista energetico, passando alle energie rinnovabili, riciclando il calore di scarto e con altre misure simili. Entrambi i fornitori hanno sfruttato l’ia per ridurre il consumo energetico dei propri data center, in alcuni casi fino al 40% (Evans, Gao, 2016; Microsoft, 2018). La domanda di data center, che sono fondamentali per il settore ict e il funzionamento dell’ia nei contesti di ricerca e produzione, è cresciuta notevolmente negli ultimi anni. Da un lato, il consumo energetico dei data center è rimasto relativamente stabile (Avgerinou, Bertoldi, Castellazzi, 2017; Shehabi, Smith, Masanet et al., 2018; Jones, 2018; Masanet, Shehabi, Lei et al., 2020). L’Agenzia internazionale per l’energia afferma che, se le attuali tendenze di efficienza nell’hardware e nell’infrastruttura dei data center possono essere conservate, la domanda globale di energia dei data center, attualmente l’1% della domanda globale di elettricità, può rimanere pressoché inalterata fino al 2022, nonostante un aumento del 60% della domanda di servizi. Dall’altro lato, anche nell’Unione Europea, dove il cloud computing ad alta efficienza energetica è diventato una questione primaria nell’agenda politica, la Commissione europea stima che vi sarà un aumento del 28% del consumo energetico dei data center entro il 2030 (European Commission, 2020). È fonte d’incertezza la mancanza di trasparenza riguardo ai dati necessari per calcolare le emissioni di gas serra dei data center in sede e dei fornitori di cloud. Inoltre, il calcolo dell’impronta ecologica dell’ia non coinvolge solo i data center. Non è chiaro, pertanto, se l’aumento dell’efficienza energetica nei data center compenserà la domanda in rapida crescita di potenza di calcolo, né è chiaro se tali incrementi di efficienza saranno realizzati allo stesso modo in tutto il mondo.

			Per tutti questi motivi, è cruciale valutare l’impronta ecologica di varie soluzioni di ia utilizzate in diversi aspetti della comprensione del cambiamento climatico o nello sviluppo di strategie per affrontarne aspetti specifici. Ma anche questo è problematico. Solo ora iniziano a comparire tecniche di facile utilizzo per monitorare e controllare le emissioni di carbonio prodotte da ricerca e sviluppo dell’ia. Tuttavia, alcuni approcci sembrano promettenti. L’obiettivo è tenere traccia di diversi fattori durante le fasi di addestramento del modello per contribuire a valutare e controllare le emissioni. Tali fattori includono il tipo di hardware utilizzato, la durata dell’addestramento, le risorse energetiche usate dalla rete che fornisce l’elettricità, l’impiego della memoria e altri fattori.

			Tuttavia, è difficile superare persino ostacoli meno elevati nella riduzione dell’impronta ecologica dell’ia poiché manca un’adozione diffusa di tali approcci e un numero sufficiente di informazioni in molte pubblicazioni di ricerca sull’ia. Ciò può anche portare a inutili emissioni di carbonio quando altri ricercatori cercano di riprodurre i risultati degli studi sull’ia. Alcuni documenti non forniscono il loro codice, mentre altri forniscono informazioni insufficienti sulle condizioni di addestramento dei loro modelli. Inoltre, non è sufficientemente documentato il numero di esperimenti condotti dai ricercatori prima di ottenere risultati pubblicabili. Alcuni esperimenti richiedono l’addestramento di migliaia di modelli durante le fasi di ricerca e sviluppo solo per ottenere modesti miglioramenti delle prestazioni. Enormi quantità di potenza di calcolo possono essere impiegate per l’affinamento. Un esempio è fornito in questo contesto da un articolo di ricerca di Google Brain, che descrive l’addestramento di oltre 12.800 reti neurali al fine di ottenere un miglioramento nell’accuratezza dello 0,09% (Fedus, Zoph, Shazeer, 2021). La moderna ricerca sull’ia tende a concentrarsi sulla produzione di modelli più profondi e accurati a discapito dell’efficienza energetica.

			Concentrarsi fortemente sui miglioramenti dell’accuratezza piuttosto che su quelli dell’efficienza tende a creare un’elevata barriera all’ingresso, poiché solo i gruppi di ricerca molto ben finanziati possono permettersi la potenza di calcolo richiesta. I gruppi di ricerca di organizzazioni più piccole o di paesi in via di sviluppo sono quindi messi da parte. Inoltre, ciò istituzionalizza l’attitudine per cui “più grande è, meglio è” e incentiva miglioramenti incrementali anche se sono trascurabili in termini di utilità pratica. Alcuni ricercatori stanno cercando di ridurre il peso computazionale e il consumo energetico dell’ia attraverso miglioramenti algoritmici e costruendo modelli più efficienti. Ma è anche fondamentale considerare che numerosi modelli di ia, il cui addestramento richiede molta energia, tuttavia alleviano o sostituiscono del tutto compiti che altrimenti richiederebbero più tempo, spazio, sforzo umano e persino energia. Ciò che il settore deve riconsiderare è il suo impegno per la ricerca ad alta intensità di calcolo fine a se stessa, e discostarsi dalle metriche delle prestazioni che si concentrano esclusivamente sui miglioramenti dell’accuratezza ignorando i loro costi ambientali. L’Unione Europea ha una funzione fondamentale da esercitare in questo ambito. Dato il ruolo positivo che l’ia può svolgere nella lotta contro il cambiamento climatico e considerati gli obiettivi dell’Europa sia sul cambiamento climatico sia sulla digitalizzazione, la ue sarebbe un promotore perfetto nell’affrontare le complessità legate al contributo della tecnologia al problema e nel soddisfare l’esigenza di elaborare politiche coordinate e multilivello, per assicurare che soluzioni di ia siano adottate con successo. È il motivo per cui questo capitolo e le seguenti raccomandazioni sono scritti dalla prospettiva della ue. Come ho sottolineato in precedenza, non perché la ue sia l’unico o il miglior attore in questo settore, ma perché è un attore sufficientemente rilevante da fare la differenza e dare l’esempio.





12.5 Tredici raccomandazioni a favore dell’ia contro il cambiamento climatico


			Le seguenti raccomandazioni si concentrano sui due ampi obiettivi suggeriti sopra. In primo luogo, sfruttare le opportunità offerte dall’ia per contrastare il cambiamento climatico in modi eticamente corretti. E, in secondo luogo, ridurre al minimo la portata dell’impronta ecologica dell’ia. Le raccomandazioni esortano tutte le rilevanti parti interessate a valutare le capacità esistenti e le potenziali opportunità, a incentivare la creazione di nuove infrastrutture e a sviluppare nuovi approcci per consentire alla società di massimizzare il potenziale dell’ia nel contesto dei cambiamenti climatici, riducendo al minimo gli svantaggi etici e ambientali.

			12.5.1 Promuovere l’ia etica nella lotta al cambiamento climatico

			Realizzare indagini minuziose e organizzare conferenze globali non sembra sufficiente per raccogliere, documentare e analizzare l’uso dell’ia per combattere il cambiamento climatico. Occorre fare di più per identificare e promuovere tali sforzi. La strategia europea per i dati rileva che l’attuale mancanza di dati ostacola anche l’uso dei dati per il bene pubblico. Sono necessarie misure legislative e regolamentari per incoraggiare la condivisione dei dati tra imprese e tra imprese e governo, al fine di promuovere lo sviluppo di maggiori e migliori soluzioni basate sull’ia, sia come prodotti e servizi a scopo di lucro sia come sforzi per contrastare questioni legate al clima senza incentivi al profitto. Inoltre, dati gli obiettivi dell’Unione Europea in materia di cambiamento climatico e digitalizzazione, la ue sarebbe un promotore ideale di tali sforzi di incentivazione. Con l’accordo che parti del Coronavirus Recovery Fund dovrebbero essere dedicate specificamente alla lotta al cambiamento climatico e alla transizione digitale, mi pare che ci siano ampi margini per tradurre in realtà queste raccomandazioni. La ue è inoltre in una posizione perfetta per garantire l’adozione di misure volte a impedire che pregiudizi e discriminazioni si insinuino negli strumenti di ia e per assicurare che le metriche dell’ia siano trasparenti per tutte le parti interessate.

			1. Incentivare un’iniziativa a livello mondiale (un osservatorio) per documentare le evidenze dell’uso dell’ia per combattere il cambiamento climatico in tutto il mondo, desumere le migliori pratiche e le lezioni apprese, e diffondere i risultati tra ricercatori, responsabili politici e pubblico.

			2. Sviluppare standard di qualità, accuratezza, rilevanza e interoperabilità per i dati da includere nel prossimo spazio di dati del Green Deal europeo; identificare gli aspetti dell’azione per il clima per cui maggiori dati sarebbero più utili; e indagare, d’intesa con esperti del settore e organizzazioni della società civile, come questi dati potrebbero essere riuniti in uno spazio comune dei dati climatici globali.

			3. Incentivare le collaborazioni tra fornitori di dati ed esperti tecnici del settore privato con esperti del settore della società civile, sotto forma di “sfide”, per garantire che i dati nello spazio dei dati del Green Deal europeo siano utilizzati efficacemente contro il cambiamento climatico.

			4. Incentivare lo sviluppo di risposte sostenibili e modulari al cambiamento climatico che incorporino le tecnologie di ia, attingendo alle risorse stanziate dal Recovery Fund.

			5. Sviluppare meccanismi per l’audit etico dei sistemi di ia impiegati in contesti ad alto rischio di cambiamento climatico, in cui i dati personali possono essere utilizzati e/o il comportamento individuale può essere influenzato. Garantire che dichiarazioni chiare e accessibili che riguardano la scelta delle metriche per cui i sistemi di ia sono ottimizzati, e il perché tale scelta sia giustificata, siano messe a disposizione del pubblico prima dell’implementazione di questi sistemi. Dovrebbe essere garantita anche la possibilità per le parti interessate di mettere in discussione e contestare i risultati e il design del sistema.

			12.5.2 Misurazione e verifica dell’impronta ecologica dell’ia: ricercatori e sviluppatori

			Ci sono molte misure immediate che possono essere adottate da chi opera nel campo della ricerca e dello sviluppo per assicurare che l’impronta ecologica dell’ia sia correttamente misurata e tenuta sotto controllo. In effetti, sono già stati fatti molti passi, come incoraggiare nella sottoposizione di articoli a includere il codice sorgente al fine di garantire la riproducibilità. Per le attività di ricerca sono necessarie anche misurazioni sistematiche e accurate per valutare il consumo di energia e le emissioni di carbonio dell’ia. Le raccomandazioni 6 e 7 sono fondamentali per normalizzare la divulgazione delle informazioni relative all’impronta ecologica dell’ia e consentire ai ricercatori e alle organizzazioni di includere considerazioni ambientali nella scelta degli strumenti di ricerca.

			6. Sviluppare tramite conferenze e riviste liste di verifica che includano la divulgazione, inter alia, del consumo energetico, della complessità computazionale e degli esperimenti (per esempio, numero di sessioni di addestramento e di modelli prodotti), per creare uniformità nel settore con metriche comuni.

			7. Valutare l’impronta ecologica dei modelli di ia che appaiono su librerie e piattaforme popolari, come PyTorch, TensorFlow e Hugging Face, per informare gli utenti sui loro costi ambientali.

			8. Incentivare lo sviluppo di metriche di efficienza per la ricerca e lo sviluppo dell’ia (compreso l’addestramento dei modelli), promuovendo miglioramenti e obiettivi di efficienza in riviste, conferenze e sfide.

			12.5.3 Misurare e controllare l’impronta ecologica dell’ia: i responsabili delle politiche

			I responsabili politici svolgono anche un ruolo cruciale nel creare parità di condizioni per ciò che concerne l’accesso alla potenza di calcolo e la creazione di presupposti per una ricerca sull’ia più accessibile ed economicamente sostenibile. Un esempio a questo proposito è la proposta dei ricercatori negli Stati Uniti di nazionalizzare l’infrastruttura cloud per dare a più ricercatori un accesso a prezzi accessibili. Un equivalente europeo potrebbe consentire ai ricercatori nell’Unione Europea di competere in modo più efficace su scala globale, garantendo nel contempo che tale ricerca si svolga su una piattaforma efficiente e sostenibile.

			9. Sviluppare infrastrutture di dati più ecologiche, smart ed economiche (per esempio data center di ricerca europei) per ricercatori e università in tutta la ue.

			10. Valutare l’ia e la sua infrastruttura sottostante (per esempio data center) quando si formulano strategie di gestione dell’energia e di mitigazione del carbonio per garantire che il settore europeo dell’ia diventi sostenibile e competitivo in modo unico.

			11. Sviluppare standard di divulgazione e valutazione del carbonio per l’ia per creare uniformità sulle metriche nel settore, per aumentare la trasparenza della ricerca e comunicare efficacemente l’impronta ecologica tramite metodi come l’aggiunta di indicazioni relative al carbonio sulle tecnologie e sui modelli basati sull’ia elencati in riviste e classifiche online.

			12. Incentivare programmi di ricerca diversi finanziando e premiando progetti che divergono dall’attuale tendenza della ricerca sull’ia ad alta intensità di calcolo al fine di indagare l’ia efficiente dal punto di vista energetico.

			13. Incentivare la ricerca ecologica ed efficiente dal punto di vista energetico subordinando il finanziamento della ue alla capacità dei richiedenti di misurare e comunicare la stima del loro consumo energetico e delle emissioni di gas a effetto serra. I finanziamenti potrebbero variare in ragione degli sforzi ambientali compiuti (per esempio, utilizzo di apparecchiature efficienti, utilizzo di elettricità rinnovabile, efficienza nell’utilizzo dell’energia <1,5).





12.6 Conclusione: una società più sostenibile e una biosfera più sana


			L’ia rappresenta solo una parte dell’1,4% delle emissioni globali di gas serra associate alle ict (Malmodin, Lundén, 2018). Tuttavia, c’è il rischio che le attuali tendenze nella ricerca e nello sviluppo di ia possano accelerare rapidamente la sua impronta ecologica. A seconda dei futuri incrementi di efficienza e della diversificazione delle fonti energetiche, le stime indicano che il settore delle ict sarà responsabile per una cifra compresa tra l’1,4% (ipotizzando una crescita stagnante) e il 23% delle emissioni globali entro il 2030 (Andrae, Edler, 2015; Malmodin, Lundén, 2018; C2E2, 2018; Belkhir, Elmeligi, 2018; Jones, 2018; Hintemann, Hinterholzer, 2019). In questo contesto più ampio, mantenere l’impronta ecologica dell’ia sotto controllo dipende da misurazioni sistematiche e accurate e da costanti incrementi di efficienza energetica in rapporto all’aumento della domanda globale. L’impatto positivo dell’ia nella lotta contro il cambiamento climatico può essere molto significativo. Ma è basilare identificare e mitigare le potenziali insidie etiche e ambientali legate a tale tecnologia. L’impegno dell’Unione Europea a difesa dei diritti umani, la lotta contro il cambiamento climatico e la promozione della transizione al digitale aprono grandi opportunità per far sì che l’ia possa realizzare il proprio potenziale. Le giuste politiche sono cruciali. Se sono disegnate e attuate in modo appropriato dai responsabili politici, sarà possibile sfruttare la potenza dell’ia, mitigando al contempo il suo impatto negativo, e gettare le basi per una società equa e sostenibile e una biosfera più sana. Tale considerazione vale non solo per il cambiamento climatico, ma per tutti i diciassette obiettivi di sviluppo sostenibile fissati dalle Nazioni Unite, come spiegherò nel prossimo capitolo.



* * *





			 				 					1. https://ellis.eu/programs/machine-learning-for-earth-and-climate-sciences.



				 					2. https://copernicus-masters.com/prize/esa-challenge/#.



				 					3. https://www.uv.es/uvweb/uv-news/en/news/ai-understanding-modelling-earth-system-international-research-team-co-led-university-valencia-wins-erc -sovvenzione-sinergia-1285973304159/Novetat.html.



				 					4. https://www.exeter.ac.uk/research/environmental-intelligence/.



				 					5. https://ai4er-cdt.esc.cam.ac.uk.



				 					6. Ricerca della banca dati dei progetti di ricerca Cordis, condotta il 30 novembre 2020, relativamente ai progetti con la stringa di ricerca [(“cambiamento climatico” o “riscaldamento globale”) e (“intelligenza artificiale” o “machine learning”)], (n = 122).



				 					7. https://www.microsoft.com/en-us/ai/ai-for-earth-partners.



				 					8. https://microsoft.github.io/AIforEarth-Grantees/.



				 					9. https://ai.google/social-good/impact-challenge/.



				 					10. https://www.elementai.com/ai-for-climate.



				 					11. https://www.aiforsdgs.org.



				 					12. https://www.vodafone-institut.de/wp-content/uploads/2020/10/VFI-DE-Pulse_Climate.pdf.





13


			L’ia e gli obiettivi di sviluppo sostenibile delle Nazioni Unite

			Sommario In precedenza, nel dodicesimo capitolo, ho analizzato l’impatto positivo e negativo dell’ia sul cambiamento climatico e offerto alcune raccomandazioni per incrementare il primo e diminuire il secondo. Ho menzionato il fatto che il cambiamento climatico sia una delle aree in cui l’ia è utilizzata per supportare gli obiettivi di sviluppo sostenibile delle Nazioni Unite. Come visto nel nono capitolo, le iniziative che fanno affidamento sull’ia per produrre risultati socialmente benefici, la cosiddetta ia per il bene sociale (ai4sg), sono in aumento. Tuttavia, i tentativi correnti di comprendere e favorire iniziative di ai4sg sono stati finora limitati dalla mancanza di analisi normative e dalla carenza di dati empirici. In questo capitolo, sulla scorta delle analisi fornite nei capitoli nono e dodicesimo, affronto tali limiti, incoraggiando l’uso degli obiettivi delle Nazioni Unite di sviluppo sostenibile (oss) come metro di valutazione per tracciare la portata e la diffusione dell’ai4sg. Presento inoltre in modo più dettagliato un database di progetti di ai4sg (già menzionati nel dodicesimo capitolo) che sono raccolti in base a tale criterio di valutazione, ed esamino alcune idee chiave, tra cui la misura in cui differenti oss sono perseguiti. Lo scopo del capitolo è contribuire a identificare i problemi urgenti che, se disattesi, rischiano di ostacolare l’efficacia delle iniziative di ai4sg.





13.1 L’ia per il bene sociale e gli obiettivi di sviluppo sostenibile delle Nazioni Unite


			Gli obiettivi di sviluppo sostenibile (oss) sono stati fissati dall’Assemblea generale delle Nazioni Unite nel 2015 per integrare gli aspetti economici, sociali e ambientali dello sviluppo sostenibile.1 Sono priorità concordate a livello internazionale a sostegno dell’azione socialmente benefica e costituiscono dunque un metro di misura sufficientemente empirico e ragionevolmente condiviso per valutare l’impatto sociale positivo dell’ai4sg a livello globale. Usare gli oss per valutare le applicazioni di ai4sg significa equiparare l’ai4sg con l’ia che sostiene gli oss (iaxoss). Questa mossa, ai4sg = iaxoss (Cowls, Tsamados, Taddeo et al., 2021b), può sembrare limitativa perché vi sono certamente molti esempi di usi socialmente buoni dell’ia al di fuori dell’ambito degli oss. Tuttavia, questo approccio comporta cinque vantaggi significativi.

			In primo luogo, gli oss tracciano limiti chiari, ben definiti e condivisibili per identificare in positivo che cosa costituisca l’ia socialmente buona (che cosa dovrebbe essere fatto, in quanto opposto a ciò che dovrebbe essere evitato), anche se non dovrebbero essere intesi come un’indicazione di ciò che non costituisce l’ia socialmente buona.

			In secondo luogo, gli oss sono obiettivi di sviluppo concordati a livello internazionale che hanno iniziato a plasmare politiche rilevanti in tutto il mondo, cosicché sollevano minori questioni sulla relatività e sulla dipendenza culturale dei valori. Per quanto naturalmente migliorabili, gli oss sono comunque quanto vi è di più vicino a un consenso di tutta l’umanità su ciò che dovrebbe essere attuato per promuovere un cambiamento sociale positivo, migliori standard di vita e la conservazione del nostro ambiente naturale.

			In terzo luogo, l’attuale corpus di ricerca sugli oss include già studi e metriche su come misurare i progressi nel conseguimento di ciascuno dei 17 obiettivi e delle 169 finalità associate, definiti nell’Agenda 2030 per lo sviluppo sostenibile.2 Questi parametri possono essere applicati per valutare l’impatto dell’iaxoss (Vinuesa, Azizpour, Leite et al., 2020).

			In quarto luogo, concentrarsi sull’impatto dei progetti basati sull’ia su diversi oss può migliorare le sinergie esistenti e contribuire a stabilirne di nuove tra progetti che concernono diversi oss, sfruttando ulteriormente l’ia per ottenere informazioni da set di dati ampi e diversificati, e tutto ciò può gettare le basi per progetti di collaborazione più ambiziosi.

			Infine, comprendere l’ai4sg in termini di iaxoss consente una migliore pianificazione e allocazione delle risorse, allorché diventa chiaro quali oss sono più lontani dall’essere conseguiti e per quale motivo.





13.2 Valutare le evidenze dell’iaxoss


			In considerazione dei vantaggi derivanti dall’uso degli oss delle Nazioni Unite come termine di confronto per la valutazione dell’ai4sg, presso il Digital Ethics Lab abbiamo condotto un’indagine internazionale sui progetti di iaxoss. L’indagine ha avuto luogo tra luglio 2018 e novembre 2020 e ha implicato la raccolta di dati su progetti di iaxoss che soddisfacevano i seguenti cinque criteri:

			a)	solo progetti che abbiano effettivamente affrontato (anche se non esplicitamente) almeno uno dei 17 oss;

			b)	solo progetti concreti e calati nella vita reale che siano basati su una qualche forma effettiva di ia (ia simbolica, reti neurali, ml, robot intelligenti, elaborazione del linguaggio naturale ecc.), e non meramente riferiti all’ia, un problema non trascurabile tra le startup di ia più in generale;3

			c)	solo progetti costruiti e utilizzati “sul campo” da almeno sei mesi, piuttosto che progetti teorici o progetti di ricerca ancora in via di sviluppo (per esempio brevetti o programmi di sovvenzione fondi);

			d)	solo progetti con un impatto positivo documentato, per esempio attraverso un sito web, un articolo di giornale, un articolo scientifico, un rapporto di ong ecc.;

			e)	solo progetti con nessuna o minima evidenza di controindicazioni, effetti collaterali negativi o controeffetti significativi.

			I requisiti (c) e (d) sono stati essenziali per portare alla luce esempi concreti di iaxoss, ovvero progetti con una comprovata pratica d’impatto solido e positivo, rispetto all’identificazione di progetti di ricerca o strumenti sviluppati in laboratori e addestrati su dati che potevano rivelarsi inadeguati o inattuabili una volta che la tecnologia fosse stata implementata al di fuori di ambienti controllati. Non sono state assunte limitazioni relativamente a chi ha sviluppato il progetto, dove, da chi è stato utilizzato, chi lo ha sostenuto finanziariamente o se il progetto era open source, con l’eccezione che sono stati esclusi i progetti condotti esclusivamente da enti commerciali con sistemi interamente proprietari.

			I progetti sono stati individuati tramite una combinazione di risorse, tra cui database accademici (ArXiv e Scopus), comunicati stampa del governo, depositi di brevetti, rapporti che tracciano l’impegno pubblico delle organizzazioni negli oss delle Nazioni Unite e database esistenti, inclusi itu e il database del partenariato fair lac dell’American Development Bank. Questo approccio ha permesso di basarsi sul lavoro esistente di Vinuesa e coautori (2020), che ha utilizzato un processo di consultazione di esperti per accertare quale degli oss poteva essere potenzialmente influenzato dall’ia, offrendo prove empiriche dei benefici effettivi già avvertiti in relazione ai vari oss.

			Da un bacino più ampio, l’indagine ha individuato 108 progetti che soddisfano i criteri menzionati. I dati sui progetti di iaxoss raccolti in questo studio sono pubblicamente disponibili nel suddetto database (https://www.aiforsdgs.org/all-projects). Ciò fa parte dell’Oxford Research Initiative on Sustainable Development Goals and Artificial Intelligence (https://www.aiforsdgs.org/) che ho diretto in collaborazione con la Saïd Business School di Oxford. Abbiamo presentato i primi risultati della nostra ricerca nel settembre 2019 in un evento collaterale durante l’Assemblea generale annuale delle Nazioni Unite e in seguito nel 2020.

			L’analisi (vedi sotto) mostra che ogni oss è già affrontato da almeno un progetto basato sull’ia. L’analisi indica che l’utilizzo di iaxoss è un fenomeno sempre più globale, con progetti che operano in cinque continenti, ma mostra anche che si tratta di un fenomeno che può non essere equamente distribuito tra gli oss (vedi Figura 13.1). L’obiettivo 3 (“Salute e benessere”) è in cima, mentre gli obiettivi 5 (“Parità di genere”), 16 (“Pace, giustizia e istituzioni solide”) e 17 (“Partnership per gli obiettivi”) risultano affrontati da meno di cinque progetti.



			È importante osservare che l’uso dell’ia per affrontare almeno uno degli oss non corrisponde necessariamente a un successo. Si noti inoltre che un progetto potrebbe affrontare più oss contemporaneamente o in tempi diversi e in modi diversi. Inoltre, sarebbe estremamente improbabile che anche la piena realizzazione di un determinato progetto portasse all’eliminazione di tutte le sfide associate a un oss. Ciò è principalmente dovuto al fatto che ogni oss riguarda sfide radicate, diffuse e strutturali per loro natura. Questo è ben riflesso nel modo in cui sono organizzati gli oss: tutti i 17 obiettivi hanno diverse finalità, di cui talune hanno a loro volta più di una metrica per misurare il loro grado di successo. L’indagine mostra quali oss sono affrontati dall’ia ad alto livello, ma è necessaria un’analisi più dettagliata per valutare la portata dell’impatto positivo degli interventi basati sull’ia rispetto a specifici indicatori degli oss, nonché i possibili effetti a cascata e le conseguenze non volute. Come indicherò nelle conclusioni, c’è ancora molta ricerca da svolgere.

			L’allocazione disuguale degli sforzi rilevata dall’indagine può essere dovuta ai vincoli dei nostri criteri di rilevazione ma, dato il livello di ricerca e analisi sistematica condotte, è più probabile che segnali una sottostante divergenza relativa alla misura in cui l’utilizzo della tecnologia di ia risulti idonea al perseguimento di ciascun obiettivo. Per esempio, l’idoneità dell’ia per affrontare un determinato problema si basa anche sulla capacità di formalizzare tale problema al livello di astrazione utile (Floridi, 2008b). È possibile che obiettivi come “Parità di genere” o “Pace, giustizia e istituzioni solide” siano più difficili da formalizzare di problemi che riguardano più direttamente l’allocazione delle risorse, come per esempio “Energia pulita e accessibile” o “Acqua pulita e servizi igienico-sanitari”. Abbiamo anche osservato una diversa allocazione degli sforzi su base geografica. Per esempio, “Ridurre le disuguaglianze”, “Istruzione di qualità” e “Salute e benessere” sono stati i principali obiettivi perseguiti dai progetti intrapresi in Sudamerica (25 dei 108 progetti). Le questioni più dettagliate sollevate dall’indagine, come quelle concernenti i fattori che possano spiegare la divergenza osservata, in che modo questa possa essere superata o che cosa possa chiarire la diversa distribuzione geografica dei progetti, richiederanno ulteriore lavoro e saranno affrontati nel corso della nostra attuale ricerca.

			Vale la pena di sottolineare che, sebbene un criterio per l’indagine consistesse nel fatto che i progetti dovevano aver già dimostrato un impatto positivo, in molti casi questo impatto era “solo” locale o in fase iniziale. Resta dunque aperta la questione relativa al modo migliore per – o in ogni caso se – “estendere la scala” delle soluzioni esistenti per applicarle a livello regionale o addirittura a livello globale. L’idea di estendere la scala delle soluzioni è allettante, poiché implica che il successo dimostrabile in un settore o in un’area possa essere replicato altrove, riducendo i costi di duplicazione (per tacere dell’addestramento intensivo dal punto di vista computazionale e quindi problematico dal punto di vista ambientale dei sistemi di ia). In effetti, come evidenzieremo di seguito, apprendere dai successi e dai fallimenti è un’altra area decisiva per la ricerca futura. Ma, nel chiedersi come estendere i successi, è importante non trascurare il fatto che la maggior parte dei progetti nella nostra indagine rappresenta già un’“applicazione più limitata” della tecnologia esistente. Più in particolare, la maggior parte degli esempi di iaxoss mostra che esistenti strumenti e tecniche di ia (sviluppate in silico in contesti di ricerca accademica o industriale) sono utilizzati per un fine diverso adattandoli al problema specifico da affrontare. Questo può in parte spiegare perché un numero più elevato di progetti di iaxoss si sviluppi in alcuni settori come “Salute e benessere” rispetto ad altri, per esempio “Parità di genere”, in cui gli strumenti e le tecniche sono relativamente carenti o non ancora altrettanto evolute. Ciò suggerisce anche che l’iaxoss prevede prima un “incanalamento locale”, dove numerose opzioni (in silico e/o in vivo) sono considerate per affrontare un particolare oss in un determinato luogo, e quindi un’“apertura a ventaglio”, che comporta la diffusione e l’adozione ripetuta di successi verificati in settori e aree adiacenti.

			L’analisi suggerisce che i 108 progetti che soddisfano i criteri sono coerenti con i sette fattori essenziali per l’ia socialmente buona, identificati nel capitolo precedente: falsificabilità e implementazione incrementale; garanzie contro la manipolazione dei predittori; intervento contestualizzato in ragione del destinatario; spiegazione contestualizzata in ragione del destinatario e finalità trasparenti; tutela della privacy e consenso del soggetto interessato; equità concreta; e semantizzazione adatta all’umano. Ogni fattore si riferisce ad almeno uno dei cinque principi etici dell’ia – beneficenza, non maleficenza, giustizia, autonomia e spiegabilità – identificati nell’analisi comparativa fornita nel quarto capitolo. Questa coerenza è cruciale: l’iaxoss non può essere in contrasto con le cornici etiche che guidano il design e la valutazione di qualsiasi tipo di ia. Abbiamo visto anche nei capitoli quarto e dodicesimo che il principio di beneficenza è di particolare rilevanza se si considera l’iaxoss, in quanto afferma che l’uso di ia dovrebbe beneficiare l’umanità e il mondo naturale. Pertanto, i progetti di iaxoss dovrebbero rispettare e attuare questo principio. Tuttavia, mentre la beneficenza è una condizione necessaria per il successo dell’iaxoss, non ne è una condizione sufficiente. L’impatto benefico di un progetto di iaxoss può essere “controbilanciato” dalla creazione o dall’incremento di altri rischi o danni (vedi capitoli 5, 7, 8 e 12). Le analisi etiche che informano il design, lo sviluppo e l’implementazione (incluso il monitoraggio) delle iniziative di iaxoss svolgono un ruolo centrale nel mitigare i rischi prevedibili nel fronteggiare le conseguenze impreviste e gli eventuali cattivi utilizzi della tecnologia. Un esempio specifico può contribuire a chiarire il punto.





13.3 L’ia per promuovere l’“azione per il clima”


			Come anticipato nel dodicesimo capitolo, il tredicesimo obiettivo, “Agire per il clima” (oss 13), è al quarto posto, in termini di attenzione attuale, nel database di Oxford, in quanto affrontato da 28 iniziative su 108. E ciò nonostante i problemi etici e ambientali legati all’uso dell’ia, ovvero le forti esigenze dal punto di vista computazionale – e quindi di consumo energetico – richieste dall’addestramento di sistemi efficaci di deep learning (Dandres, Vandromme, Obrekht et al., 2017; Strubell, Ganesh, McCallum, 2019; Cowls, Tsamados, Taddeo et al., 2021a).

			Per capire in che misura l’ia è già sviluppata per affrontare l’oss 13 e con quali specifiche modalità ciò accade, è possibile incrociare le iniziative codificate nell’insieme di dati come riferite all’obiettivo “Agire per il clima” con le aree di 35 potenziali casi d’uso in 13 settori identificati in uno sforzo di mappatura su larga scala intrapreso da Rolnick e coautori (2019). Come illustrato nella Figura 13.2, almeno un’iniziativa nel nostro insieme di dati affronta otto dei tredici aspetti dell’azione per il clima identificati da Rolnick e coautori.



			I progetti che si affidano all’ia per sostenere “l’azione per il clima” nel nostro insieme di dati hanno luogo in un certo numero di paesi: ciò indica una ragionevole diffusione geografica, ma è importante osservare che la maggior parte di questi paesi (Australia, Francia, Germania, Giappone, Slovenia, Corea del Sud, Emirati Arabi Uniti, Regno Unito e Stati Uniti) sono considerati parte del Nord globale. Soltanto quattro progetti hanno base nel Sud del mondo, nello specifico in Argentina, Perù e Cile. Ciò non significa ovviamente che queste iniziative non abbiano un impatto in altre parti del mondo; per fare un esempio, Global Forest Watch è un progetto con sede nel Regno Unito, ma sta cercando di monitorare e proteggere le foreste in tutto il mondo. Tuttavia, questo risultato dell’analisi mette in evidenza il rischio che i progetti che hanno luogo (e sono finanziati) in una parte del mondo possono non rispondere necessariamente ai bisogni reali avvertiti altrove.

			Nel complesso, questo caso di studio fornisce prove preliminari promettenti che l’ia sia effettivamente usata per affrontare il cambiamento climatico e i problemi a esso legati. Come mostra lo sforzo di riferimento incrociato di cui sopra, ciò combacia con una ricerca più ampia, suggerendo che l’ia potrebbe e dovrebbe essere sviluppata e utilizzata a questo scopo. Come mostrano Rolnick e coautori (ibidem), l’ia potrebbe sostenere gli sforzi per mitigare i cambiamenti climatici in tredici settori esistenti e potenziali che vanno dalla rimozione di co2 all’ottimizzazione dei trasporti, alla protezione delle foreste. Il potenziale dell’ia per sostenere l’azione per il clima è stato riconosciuto anche da un consorzio di accademici, ong e società energetiche, che ha sottoposto al governo del Regno Unito nel 2019 la richiesta di istituire un centro internazionale per l’“ia, l’energia e il clima”.4

			La panoramica precedente mostra che esistono già “le truppe sul campo” che utilizzano l’ia per affrontare la crisi climatica, anche se tali sforzi sono solo in una fase iniziale. Dati i criteri rigorosi applicati nel nostro processo di campionamento (per esempio, che i progetti devono aver dato prova di impatto positivo), i risultati della nostra analisi mostrano una direzione positiva. Allo stesso tempo ciò indica che c’è ancora molto da fare, essendoci diverse lacune verso cui le iniziative future potrebbero orientare i propri sforzi.





13.4 Conclusione: un programma di ricerca per l’iaxoss


			C’è un numero crescente di progetti che si avvalgono dell’ia per il bene sociale affrontando gli oss delle Nazioni Unite. Le tecnologie di ia non sono una panacea, ma possono essere una parte importante della soluzione e contribuire ad affrontare le sfide principali, sia sociali sia ambientali, con cui l’umanità è oggi posta a confronto. Se ben disegnate, le tecnologie di ia possono favorire la produzione di risultati socialmente buoni su una scala e con un’efficienza senza precedenti. Perciò, è essenziale fornire una struttura coerente all’interno della quale possano prosperare i progetti attuali e nuovi di iaxoss. I passi successivi per comprendere l’ai4sg in termini di iaxoss sono i seguenti: esaminare quali fattori determinano il successo o il fallimento dei progetti di iaxoss, con particolare riferimento al loro specifico impatto “sul campo”; spiegare i divari e le discrepanze tra l’utilizzo dell’ia per affrontare differenti oss e gli indicatori, nonché i contrasti tra il luogo in cui i progetti hanno sede e quello in cui è maggiore la necessità relativa agli oss; e chiarire quale ruolo può essere svolto dalle principali parti interessate per promuovere il successo dei progetti di iaxoss e affrontare importanti divari e discrepanze. Tutto ciò dovrà basarsi, inevitabilmente, su un approccio multidisciplinare. E comporterà, inoltre, una disamina più approfondita dei progetti di iaxoss nei contesti e nelle comunità in cui sono sviluppati e implementati. Richiederà, infine, di adottare una prospettiva più ampia, che metta a fuoco quale tipo di progetto umano vogliamo disegnare e perseguire nel ventunesimo secolo. Questo è il tema del prossimo e ultimo capitolo.



* * *





			 				 					1. Programma delle Nazioni Unite per lo sviluppo (undp), Obiettivi di sviluppo sostenibile: https://www.undp.org/content/undp/en/home/sustainable-development-goals.html.



				 					2. Ibidem.



				 					3. Secondo uno studio, le startup di ia in Europa spesso non utilizzano l’ia (A. Ram, Financial Times, 4 marzo 2019).



				 					4. P. Shrestha, “I principali gruppi energetici e tecnologici fanno richiesta di un Centro internazionale per l’intelligenza artificiale, l’energia e il clima”, in Energy Live News, 20 agosto 2019. Notizie sull’energia su: https://www.energylivenews.com/2019/08/20/leading-energy-and-tech-groups-call-for-international-centre-for-ai-energy-and-climate/(2019).





14


			Conclusione: il verde e il blu

			Sommario Nei capitoli precedenti, ho esaminato l’ia, il suo impatto positivo e negativo, e le questioni etiche che ne discendono. In quest’ultimo capitolo, traggo alcune conclusioni generali e getto uno sguardo su ciò che potrebbe accadere: un passaggio dall’etica dell’agire artificiale alla politica delle azioni sociali.





14.1 Introduzione: dal divorzio tra agire e intelligenza al matrimonio tra il verde e il blu


			A volte dimentichiamo che la vita senza il contributo di una buona politica, di una scienza affidabile e di una robusta tecnologia diventa presto “solitaria, povera, sgradevole, brutale e breve”, per prendere in prestito le parole del Leviatano di Thomas Hobbes. La crisi del Covid-19 ci ha tragicamente ricordato che la natura può essere spietata. Solo l’ingegno umano e la buona volontà possono migliorare e salvaguardare il tenore di vita di miliardi di persone. Oggi, gran parte di tale ingegnosità è impegnata nel realizzare una rivoluzione epocale: la trasformazione di un mondo esclusivamente analogico in un mondo sempre più digitale. Gli effetti sono già diffusi: questa è la prima pandemia in cui un nuovo habitat, l’infosfera, ha contribuito a superare i pericoli della biosfera. Viviamo onlife (sia online sia offline) ormai da tempo, ma la pandemia ha trasformato l’esperienza onlife in una realtà che costituisce un punto di non ritorno per l’intero pianeta.

			Nei capitoli precedenti ho sostenuto che lo sviluppo dell’ia è un fattore importante in questa rivoluzione epocale. L’ia dovrebbe essere concepita come l’ingegnerizzazione di artefatti in grado di fare cose che richiederebbero intelligenza se dovessimo farle noi. Con un classico esempio che ho usato più volte, un telefono cellulare può battere quasi chiunque a scacchi, pur essendo intelligente come un tostapane. In altre parole, l’ia segna il divorzio senza precedenti tra la capacità di portare a termine compiti o risolvere problemi con successo in vista di un dato obiettivo e il bisogno di essere intelligenti per farlo. Questo riuscito divorzio è diventato possibile solo negli ultimi anni, grazie a gigantesche quantità di dati, strumenti statistici molto sofisticati, enorme potenza di calcolo e alla trasformazione dei nostri contesti di vita in luoghi sempre più adatti all’ia (avvolti intorno all’ia). Quanto più viviamo nell’infosfera e onlife, tanto più condividiamo le nostre realtà quotidiane con forme di agire ingegnerizzate, e tanto più l’ia può affrontare un numero crescente di problemi e compiti. Il limite dell’ia non è il cielo, ma l’ingegno umano.

			In questa prospettiva storica ed ecologica, l’ia è una straordinaria tecnologia che può essere una potente forza positiva, in due modi principali. Può aiutarci a conoscere, comprendere e prevedere di più e meglio le numerose sfide che stanno diventando così impellenti, in particolare il cambiamento climatico, l’ingiustizia sociale e la povertà globale. La corretta gestione di dati e processi da parte dell’ia può accelerare il circolo virtuoso tra maggiori informazioni, migliore scienza e politiche più avvedute. Eppure, la conoscenza è potere solo se si traduce in azione. Anche a questo riguardo, l’ia può essere una notevole forza positiva, aiutandoci a migliorare il mondo, e non soltanto la sua interpretazione. La pandemia ci ha ricordato che fronteggiamo problemi complessi, sistemici e globali. Non possiamo risolverli individualmente. Abbiamo bisogno di coordinarci (non dobbiamo intralciarci), collaborare (ognuno fa la sua parte) e cooperare (lavoriamo insieme) di più, meglio e a livello internazionale. L’ia può consentirci di realizzare queste 3C in modo più efficiente (più risultati con meno risorse), in modo efficace (migliori risultati) e in modo innovativo (nuovi risultati).

			Tuttavia, c’è un “ma”: sappiamo che l’ingegno umano, senza buona volontà, può essere pericoloso. Se l’ia non è controllata e guidata in modo equo e sostenibile, può esacerbare i problemi sociali, dai pregiudizi alla discriminazione; erodere l’autonomia e la responsabilità umana; amplificare i problemi del passato, dall’iniqua allocazione della ricchezza allo sviluppo di una cultura della mera distrazione, quella del “panem et digital circenses”. L’ia rischia di trasformarsi da parte della soluzione a parte del problema. Questo è il motivo per cui iniziative etiche come quelle descritte nel quarto capitolo e, in ultima analisi, buone norme internazionali sono essenziali per garantire che l’ia rimanga una potente forza per il bene.

			L’ia per il bene sociale è parte integrante di un nuovo matrimonio, tra il verde di tutti i nostri habitat – naturali, sintetici e artificiali, dalla biosfera all’infosfera, da ambienti urbani a contesti economici, sociali e politici – e il blu delle nostre tecnologie digitali – dai cellulari alle piattaforme sociali, dall’Internet delle Cose ai Big Data, dall’ia ai futuri computer quantistici. Il matrimonio tra il verde e il blu, con i suoi vantaggi, controbilancia il divorzio tra l’agire e l’intelligenza, con i suoi rischi. Siamo noi che abbiamo la responsabilità di disegnare e gestire entrambi con successo. La pandemia ha reso palese che la posta in gioco non risiede tanto nell’innovazione digitale, quanto piuttosto nella corretta governance del digitale. Le tecnologie aumentano e migliorano ogni giorno. Tuttavia, per salvare il nostro pianeta e noi stessi, anche da noi stessi, possiamo e dobbiamo utilizzarle molto meglio; basti pensare alla diffusione di disinformazione relativa al Covid-19 sui social media o all’inefficacia delle cosiddette app per il coronavirus. La pandemia è stata la prova generale di quello che dovrebbe essere il progetto umano per il ventunesimo secolo, un matrimonio stabile e fruttuoso tra il verde e il blu. Possiamo farne un successo insieme e facendo affidamento su più e migliore filosofia, non su meno.





14.2 Il ruolo della filosofia come design concettuale


			Per un certo tempo, il divario umano/macchina è stata la frontiera del cyberspazio. Oggi ci siamo spostati all’interno dell’infosfera. La sua natura onnicomprensiva dipende anche dalla misura in cui accettiamo la sua natura digitale come parte integrante della nostra realtà e come trasparente per noi, nel senso che non la percepiamo più come presente. Ciò che conta non sono tanto i bit in movimento invece degli atomi – questa è un’interpretazione della società dell’informazione obsoleta e basata sulla comunicazione che deve troppo alla sociologia dei mass media – quanto piuttosto il fatto molto più radicale che il nostro modo di comprendere e concepire l’essenza e il tessuto della realtà sta cambiando. In effetti, abbiamo cominciato ad accettare il virtuale come parzialmente reale e il reale come parzialmente virtuale. La società dell’informazione è concepita più adeguatamente come società neomanifatturiera in cui materie prime ed energia sono state sostituite da dati e informazioni, il nuovo oro digitale e la vera fonte di valore aggiunto. Le chiavi per una corretta comprensione della nostra situazione e per lo sviluppo di un’infosfera sostenibile non risiedono dunque solo nella comunicazione e nelle transazioni, ma nella creazione, nel design e nella gestione delle informazioni. Tale comprensione richiede una nuova narrazione, ossia un nuovo tipo di storia che raccontiamo a noi stessi sulla nostra situazione e sul progetto umano che desideriamo perseguire. Questo può sembrare un passo anacronistico nella direzione sbagliata. Fino a poco tempo fa, le “grandi narrazioni”, dal marxismo al liberalismo, alla cosiddetta “fine della storia”, sono state oggetto di numerose critiche. Ma la verità è che anche tale critica era solo un’altra narrazione, e non ha funzionato. Una critica sistematica delle grandi narrazioni è fatalmente parte del problema che cerca di risolvere. Capire perché ci sono narrazioni, che cosa le giustifica e quali narrazioni migliori possono sostituirle è un modo meno immaturo e più fruttuoso di procedere. Le ict stanno creando il nuovo ambiente informazionale in cui le generazioni future vivranno la maggior parte del loro tempo. Le precedenti rivoluzioni nella creazione di ricchezza, in particolare quella agricola e industriale, hanno portato a trasformazioni macroscopiche nelle nostre strutture sociali e politiche e negli ambienti architettonici, spesso in modo imprevisto e, di regola, con profonde implicazioni concettuali ed etiche. La rivoluzione dell’informazione – intesa sia in termini di creazione di ricchezza sia in termini di riconcettualizzazione di noi stessi – non è meno decisiva. Avremo grossi problemi, se non prendiamo sul serio il fatto che stiamo costruendo i nuovi ambienti che saranno abitati dalle generazioni future. Alla luce di questo importante cambiamento nel tipo di interazioni, mediate dalle ict, che intraprenderemo sempre più con altri agenti, sia biologici sia artificiali, e nella comprensione di noi stessi, un approccio etico è essenziale per affrontare le nuove sfide poste dalle ict. Deve essere un approccio che non privilegi il naturale o l’incontaminato, ma tratti come autentiche e genuine tutte le forme di esistenza e comportamento, anche quelle basate su artefatti artificiali, sintetici, ibridi e ingegnerizzati. Il compito è quello di formulare un quadro etico che possa trattare l’infosfera come un nuovo ambiente degno dell’attenzione morale e della cura degli inforg umani che lo abitano. Un simile quadro etico deve affrontare e risolvere le sfide senza precedenti che sorgono nel nuovo ambiente. Come ho sostenuto (in Floridi, 2013), deve essere un’etica e-cologica per l’intera infosfera. Questa sorta di e-cologismo sintetico (sia nel senso di olistico o inclusivo, sia nel senso di artificiale) richiederà un cambiamento nel modo in cui percepiamo noi stessi e i nostri ruoli rispetto alla realtà, in ciò che reputiamo degno di rispetto e cura, e nel modo in cui potremmo negoziare una nuova alleanza tra il naturale e l’artificiale. Ciò richiederà una seria riflessione sul progetto umano e una revisione critica delle nostre attuali narrazioni, a livello individuale, sociale e politico. Sono tutte questioni urgenti che meritano la nostra piena e totale attenzione. Purtroppo, temo che ci vorrà del tempo e un nuovo tipo di educazione e sensibilità per rendersi conto che l’infosfera è uno spazio comune, che deve essere preservato a beneficio di tutti. La filosofia come design concettuale (Floridi, 2019c) dovrebbe contribuire a tale cambiamento di prospettiva e dei nostri sforzi costruttivi. Dovrebbe anche aiutarci a mutare la comprensione che abbiamo di noi stessi, in quanto bellissimo errore di Natura.





14.3 Il bellissimo errore di natura


			Abbiamo visto nel secondo capitolo che Galileo riteneva che la natura fosse come un libro, scritto in simboli matematici, per essere letto dalla scienza. Poteva essere una forzatura metaforica ai suoi tempi, ma oggi il mondo in cui viviamo è certamente sempre più un libro scritto in cifre, per essere letto ed esteso dall’informatica e dalla scienza dei dati. Le tecnologie digitali hanno sempre più successo al suo interno perché, come i pesci nel mare, sono i veri nativi dell’infosfera. Ciò spiega anche perché le applicazioni di ia sono migliori di noi in un numero crescente di compiti: noi siamo semplici organismi analogici che cercano di adattarsi a un habitat così nuovo vivendo onlife. La trasformazione epocale del nostro ambiente in un’infosfera mista, sia analogica sia digitale, e il fatto di condividere l’infosfera con agenti artificiali sempre più smart, autonomi e sociali, ha profonde conseguenze. Alcune di queste non sono ancora distinguibili. Le scopriremo solo con il tempo. Altre sono appena riconoscibili all’orizzonte. Ed altre ancora sono davanti ai nostri occhi. Cominciamo da queste.

			Gli agenti di ia, siano essi software (app, webbot, algoritmi, software di ogni genere) o hardware (robot, auto senza conducente, orologi intelligenti e gadget di ogni genere) stanno sostituendo gli agenti umani in ambiti che pensavamo fossero al di fuori della portata di qualsiasi tecnologia fino a pochi anni fa: catalogare immagini, tradurre documenti, interpretare radiografie, pilotare droni, estrarre nuove informazioni da enormi quantità di dati e molte altre cose che solo i colletti bianchi avrebbero dovuto fare. I colletti marroni in agricoltura e quelli blu nell’industria avvertono da decenni la pressione del digitale; i servizi sono ora il nuovo target. Perciò, spariranno anche i posti di lavoro dei colletti bianchi. Possiamo solo tentare di fare una stima ragionevole di quanti ne spariranno e con quale rapidità, ma è probabile che lo sconvolgimento sia profondo. Ovunque gli esseri umani lavorino oggi come interfacce, per esempio tra un gps e un’automobile, tra due documenti in lingue diverse, tra alcuni ingredienti e un piatto, tra i sintomi e la malattia corrispondente, quel lavoro è a rischio. Al contempo, appariranno nuovi lavori – li ho chiamati colletti verdi – perché saranno necessarie nuove interfacce, tra i servizi forniti dai computer, tra i siti web, tra le applicazioni di ia, tra i risultati dell’ia e così via. Qualcuno dovrà controllare se una traduzione abbastanza buona sia una traduzione sufficientemente affidabile. Molte attività resteranno troppo costose per le applicazioni di ia, pur ammettendo che siano realizzabili con l’ia. Consideriamo il caso di Amazon. Fornisce “l’accesso a più di 500.000 lavoratori provenienti da 190 paesi”,1 i cosiddetti turchi, definiti anche da Amazon come “intelligenza artificiale [sic] artificiale”. La ripetizione è indicativa: si tratta di lavori che non richiedono intelligenza pagati pochi centesimi. Non è il tipo di lavoro che potremmo desiderare per i nostri figli, ma è comunque un lavoro di cui molte persone hanno bisogno e che non possono rifiutare. Se non elaboreremo migliori quadri normativi giuridici ed etici, l’ia creerà ulteriori polarizzazioni nella nostra società, specialmente tra i pochi al di sopra delle macchine – i nuovi patrizi – e coloro che stanno al di sotto di esse – la nuova plebe. Con i posti di lavoro, diminuiranno anche le tasse, anche se ciò accadrà un po’ più avanti nel futuro. Nessun lavoratore, nessun contribuente, questo è ovvio; e le aziende che sfrutteranno la delegazione di compiti all’ia non saranno generose come i loro ex dipendenti allorché si tratterà di sostenere il benessere sociale. Occorrerà fare qualcosa al riguardo, facendo pagare più tasse alle aziende e ai benestanti. Le norme giocheranno un ruolo importante anche nel determinare quali lavori dovranno essere mantenuti “umani”. I treni a guida autonoma sono una rarità anche per ragioni normative,2 eppure sono molto più facili da gestire rispetto ai taxi o agli autobus senza conducente. Chiaramente le regole contribuiranno in modo significativo al modo in cui disegniamo il futuro della nostra infosfera. Prima di considerare le conseguenze palesi, occorrono due ultime precisazioni. Molti compiti che scompariranno non faranno sparire i lavori corrispondenti: i giardinieri assistiti da uno dei tanti robot tosaerba esistenti avranno semplicemente più tempo per fare altre cose, diverse dal tagliare l’erba. Nessun robot li sostituirà. E molti compiti non scompariranno, verranno semplicemente riaffidati a noi come utenti: premiamo già i pulsanti dell’ascensore (quel lavoro non esiste più), siamo sempre più abituati a scansionare la merce al supermercato (anche il lavoro di cassa sta per sparire) e faremo certamente più lavori in prima persona in futuro.

			Che cosa dire delle altre conseguenze appena distinguibili all’orizzonte, quando l’ia non sarà più in mano a tecnici e manager, ma “democratizzata” nelle tasche di miliardi di persone? Al riguardo, posso essere soltanto piuttosto astratto e incerto. L’ia e più in generale gli agenti smart, autonomi e sociali, così come gli strumenti predittivi in grado di anticipare e manipolare le decisioni e le scelte umane offrono un’opportunità storica per ripensare l’eccezionalità umana non come qualcosa di errato quanto piuttosto come qualcosa di mal compreso. I nostri comportamenti intelligenti saranno messi alla prova dai comportamenti smart dell’ia, che possono risultare più efficaci nell’infosfera in termini di adattamento. I nostri comportamenti autonomi saranno messi alla prova dalla prevedibilità e manipolabilità delle nostre scelte razionali e dallo sviluppo dell’autonomia artificiale. E anche la nostra socievolezza sarà messa alla prova dalla sua controparte artificiale, rappresentata da compagni artificiali, ologrammi o semplici voci, servitori 3D o robot sessuali simili a umani, che possono essere attraenti per gli umani e talvolta indistinguibili da loro. Come andrà a finire tutto questo non è chiaro, ma una cosa è certa: lo sviluppo di agenti artificiali non darà luogo ad alcuna allarmante creazione di scenari fantascientifici, che sono irresponsabilmente fuorvianti. Come già detto, non c’è alcun Terminator in vista. L’ia è pressoché un ossimoro: le tecnologie smart saranno tanto stupide quanto le nostre vecchie tecnologie. Ma l’ia ci inviterà a riflettere più seriamente e con minore compiacimento su chi siamo, potremmo essere o vorremmo diventare, e quindi sulle nostre responsabilità e sulla comprensione che abbiamo di noi stessi. L’ia sfiderà profondamente il nostro modo di concepire ciò che intendiamo quando ci percepiamo come “speciali” dopo la quarta rivoluzione (vedi Floridi, 2014a). Non sto sostenendo che il nostro eccezionalismo sia errato. Suggerisco piuttosto che l’ia ci farà rendere conto che il nostro essere eccezionali risiede in un modo speciale e forse irriproducibile di essere disfunzionali con successo. Siamo un hapax legomenon nel Libro della Natura di Galileo, un po’ come l’espressione gopher (“legno di cipresso”), che si riferisce al materiale originario con cui è stata costruita l’arca di Noè e che ricorre una sola volta in tutta la Bibbia. Con una metafora più digitale e contemporanea, siamo un bellissimo errore nel grande software dell’universo, non l’app di maggior successo. Resteremo un bug, un errore unico e riuscito, mentre l’ia sarà ancora di più un elemento peculiare nel libro matematico della natura di Galileo. Un così bell’errore sarà sempre più responsabile della natura e della storia. In breve, Shakespeare aveva ragione:

			Gli uomini in certi momenti sono padroni dei loro destini.

			La colpa, caro Bruto, non è nelle nostre stelle.

			Ma in noi stessi, se siamo schiavi.

			(Shakespeare, Giulio Cesare, i, ii)

			La questione è che non potremo essere padroni dei nostri destini senza ripensare un’altra forma dell’agire, quella politica. Per questo, La politica dell’informazione sarà il tema del mio prossimo libro.



* * *





			 				 					1. https://requester.mturk.com/create/projects/new.



				 					2. https://www.vice.com/en/article/wnj75z/why-dont-we-have-driverless-trains-yet.





RINGRAZIAMENTI


			Molte delle idee e dei contenuti presentati in questo libro sono stati inizialmente testati come presentazioni a convegni (conferenze, workshop, seminari ecc.) o in articoli di riviste. Questo modo di lavorare sistematico e graduale è laborioso, ma mi pare fruttuoso e, sotto un certo profilo, anche inevitabile, data la natura innovativa del settore. Richiede, infatti, una perseveranza e un impegno che spero non siano mal esercitati. Ho voluto valutare le idee presentate in questo volume nel modo più completo possibile. Discutere questo materiale in convegni e pubblicare gli articoli corrispondenti mi ha dato l’opportunità e il privilegio di godere di una grande quantità di feedback, provenienti da un numero molto elevato di ottimi colleghi e revisori anonimi. Se non li ringrazio tutti qui, non è per mancanza di educazione o per mera ragione di spazio, ma perché gli opportuni ringraziamenti si trovano nelle pubblicazioni corrispondenti.

			Ci sono, tuttavia, alcune persone che mi piace menzionare esplicitamente perché hanno svolto un ruolo significativo durante la stesura e le revisioni del testo. Kia, prima di tutto. Senza di lei, non avrei mai avuto la fiducia per intraprendere un simile compito e l’energia spirituale per portarlo a termine. Richiede grande serenità investire così tanto tempo nel pensiero filosofico e Kia è la mia musa ispiratrice. Vorrei poter affermare di aver contribuito alla sua ricerca in neuroscienze anche la metà di quanto lei ha contribuito al mio sviluppo filosofico. È un immenso privilegio poter ricevere consigli da una mente così brillante.

			Nikita Aggarwal, Josh Cowls, Massimo Durante, Emmie Heine, Thomas C. King, Michelle Lee, Jakob Mökander, Jessica Morley, Carl Öhman, Huw Roberts, Andreas Tsamados, Ugo Pagallo, David Sutcliffe, Mariarosaria Taddeo, Vincent Wang, David Watson, nonché molti studenti, membri e visitatori del Digital Ethics Lab di Oxford, negli anni passati, sono stati molto generosi con il loro tempo e le loro idee, e hanno offerto numerose occasioni di confronto e approfondimento sui temi analizzati in questo libro. Mi hanno anche salvato da errori imbarazzanti più volte di quanto possa ricordare.

			Ho imparato molto dai colleghi con cui ho interagito, anche se avrei voluto imparare perfino di più. Ho appreso molto anche dalle mie interazioni con aziende come DeepMind, ey, Facebook, Fujitsu, Google, ibm, McKinsey, Microsoft, Vodafone, SoftBank e tante altre, nonché con istituzioni come la Commissione europea, il Garante europeo della protezione dei dati, il Consiglio d’Europa, la House of Commons, la House of Lords, il Centre for Data Ethics and Innovation, la Digital Catapult, l’ico, l’Alan Turing Institute, l’fta, il Vodafone Institute, l’Audi Foundation Beyond, Atomium e altri ancora. L’importanza e il significato di alcuni dei problemi che ho discusso in questo libro mi sarebbero stati meno chiari se non fosse stato per il riscontro fornito dal mondo degli affari e della politica.

			Peter Momtchiloff è stato essenziale per la realizzazione della versione inglese di questo libro, come per altri in passato, ma c’è un ringraziamento ulteriore che desidero ribadire qui: è stato lui a farmi capire che fosse meglio pubblicare Etica dell’intelligenza artificiale e La politica dell’informazione come volumi separati.

			Danuta Farah, la mia assistente personale, ha fornito l’eccezionale supporto e le impeccabili capacità manageriali senza le quali non avrei potuto portare a termine questo progetto.

			Vorrei ringraziare anche tutte le persone che hanno reso il compito di lavorare a questo libro più difficile di quanto avrebbe dovuto essere. Sono tanti e sono stati persistenti. Hanno fatto del loro meglio per assicurare che fallissi nei miei progetti. Sono grato per la loro costante disponibilità a scoraggiarmi, a minare i miei sforzi e in generale a rendere la mia vita intellettuale meno piacevole se non più difficile. Trovo che poche cose motivano di più del sentirsi dire che non si riuscirà. L’attrito è essenziale per fare progressi, e loro ne hanno offerto in abbondanza.

			La ricerca che ha condotto a questo libro è stata sostenuta in modi e gradi differenti da diversi finanziamenti negli anni passati. Desidero citare qui le seguenti fonti: The Alan Turing Institute; Atomium – Istituto Europeo per la Scienza, i Media e la Democrazia; epsrc (il Consiglio di ricerca in Ingegneria e Scienze fisiche della Gran Bretagna); Commissione europea, Horizon 2020; Commissione europea – Programma Marie Skłodowska-Curie Fellowship; Facebook; Fujitsu; Google; Microsoft; Tencent; l’Università di Oxford – Fondo John Fell.

			In termini di divulgazione, sono stato coinvolto in molte iniziative e progetti che riguardano gli argomenti trattati in questo libro. Temo che l’elenco sia lungo. Eccolo qui di seguito, in ordine cronologico, dal 2014, in qualità di: Chair, Ethics Board, Cluster Science of Intelligence (scioi), German Excellence Initiative, Deutsche Forschungsgemeinschaft (dfg, German Research Foundation); Membro, Advisory Board, Institute of ai, Foreign Office, Regno Unito; Membro, Advisory Board, Vodafone Institute for Society and Communications; Consiglio Scientifico, Humane Technology Lab, Università Cattolica del Sacro Cuore, Italia; Membro, Ethics Board, MediaFutures: Research Center for Responsible Media Technology & Innovation, University of Bergen, Norvegia; Presidente, Comitato Etico del progetto Machine Intelligence Garage, Digital Catapult, uk Innovation Program; Membro del Consiglio di amministrazione del Centre for Data Ethics an Innovation (cdei), Regno Unito; Membro, Technology Advisory Panel, Information Commissioner’s Office (ico), Regno Unito; Membro dell’ai Advisory Board di ey; Membro, Advisory Board, Fondazione Leonardo, Italia; Membro del gruppo di alto livello sull’intelligenza artificiale della Commissione europea; Membro dell’Advisory Board dell’Institute for Ethical ai in Education (ieaie), Regno Unito; Membro, Comitato Vaticano per l’Etica dell’ia; Membro, Advisory Board on Tech Ethics all’interno dell’All-Party Parliamentary Group (appg) on Data Analytics, Regno Unito; Membro, Advisory Group on Open Finance, Financial Conduct Authority (fca), Regno Unito; Membro del Comitato di esperti del Consiglio d’Europa sui diritti umani, Dimensioni dell’elaborazione automatizzata dei dati e diverse forme di intelligenza artificiale (msi-aut) – Comitato direttivo dei ministri sui media e la società dell’informazione (cdmsi); Membro, Advanced Technology Advisory Council esterno di Google; Membro del Consiglio del Forum economico mondiale sul futuro della tecnologia, dei valori e della politica; Chair, Comitato Scientifico di ai4People, “Il primo forum globale d’Europa sugli impatti sociali dell’intelligenza artificiale”; Presidente, Comitato consultivo della conferenza internazionale 2018 dei commissari per la protezione dei dati e la privacy, gepd, ue; Presidente, Comitato consultivo etico dell’imi-emif, il quadro europeo per l’informazione medica dell’ue; Presidente, gruppo di lavoro di Facebook sull’etica digitale; Presidente, Data Ethics Group, The Alan Turing Institute; Membro del Comitato scientifico del hub di ricerca Commitment to Privacy and Trust in Internet of Things Security (ComPaTrIoTS), epsrc, Regno Unito; Membro, Gruppo consultivo etico sulle dimensioni etiche della protezione dei dati, Garante europeo della protezione dei dati (gepd), ue; Membro del Gruppo di lavoro della Royal Society e della British Academy sulla governance dei dati; Co-presidente, Ethics in Data Science Working Group, Cabinet Office, Regno Unito; Membro del Consiglio consultivo di Google sul diritto all’oblio.

			La stesura originale di questo libro è stata resa possibile da un anno sabbatico, per cui sono molto grato all’Università di Oxford, e dall’eccezionale sostegno dell’Università di Bologna. Ho il privilegio di lavorare per istituzioni straordinarie.

			Desidero, infine, ringraziare, per questa traduzione italiana, Massimo Durante, non solo per il suo acume concettuale e linguistico, ma soprattutto per le bellissime conversazioni, sempre illuminanti e grazie alle quali continuo a imparare così tanto. Il mio debito intellettuale nei suoi confronti è ulteriormente cresciuto. Raffaello Cortina, Albertine Cerutti e Ester Ruberto sono stati cruciali per la realizzazione di questo libro. A loro va tutta la mia gratitudine per aver creduto prima di altri in un filosofo che si occupa di digitale, tanto tempo fa. Rappresentano un’editoria lungimirante e attenta alla cura dei contenuti ormai rara.





BIBLIOGRAFIA


			Abadi, Martín, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, Li Zhang (2016), “Deep learning with differential privacy”. In Proceedings of the 2016 acm sigsac Conference on Computer and Communications Security. acm, Wien, pp. 308-318.

			Abdella, Galal M., Murat Kucukvar, Nuri Cihat Onat, Hussein M. Al-Yafay, Muhammet Enis Bulak (2020), “Sustainability assessment and modeling based on supervised machine learning techniques: The case for food consumption”. In Journal of Cleaner Production, 251, 119661. doi: 10.1016/j.jclepro.2019.119661.

			Abebe, Rediet, Solon Barocas, Jon Kleinberg, Karen Levy, Manish Raghavan, David G. Robinson (2020), “Roles for computing in social change”. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency.

			Abrell, Jan, Mirjam Kosch, Sebastian Rausch (2019), How Effective Was the uk Carbon Tax? A Machine Learning Approach to Policy Evaluation. Social Science Research Network, Rochester, ny.

			Adams, John (1787), A Defence of the Constitutions of Government of the United States of America. C. Dilly, London.

			Aftab, Muhammad, Chien Chen, Chi-Kin Chau, Talal Rahwan (2017), “Automatic hvac control with real-time occupancy recognition and simulation-guided model predictive control in low-cost embedded system”. In Energy and Buildings, 154, pp. 141-156.

			Aggarwal, Nikita (2020), “The norms of algorithmic credit scoring”. In ssrn Electronic Journal. http://dx.doi.org/10.2139/ssrn.3569083.

			Al-Abdulkarim, Latifa, Katie Atkinson, Trevor Bench-Capon (2015), “Factors, issues and values: Revisiting reasoning with cases”. In Proceedings of the 15th International Conference on Artificial Intelligence and Law, pp. 3-12.

			Alaieri, Fahad, André Vellino (2016), “Ethical decision making in robots: Autonomy, trust and responsibility”. In International Conference on Social Robotics.

			Alazab, Mamoun, Roderic Broadhurst (2016), “Spam and criminal activity”. In Trends and Issues in Crime and Criminal Justice (Australian Institute of Criminology), 52.

			Algorithm Watch (2019), “The ai Ethics Guidelines Global Inventory”, 9 aprile. https://algorithmwatch.org/en/project/ai-ethics-guidelines-global-inventory/.

			Allen, Anita (2011), Unpopular Privacy: What Must We Hide? Oxford University Press, Oxford. https://doi.org/10.1093/acprof:oso/9780195141375.001.0001.

			Allo, Patrick (2010), Putting Information First: Luciano Floridi and the Philosophy of Information. Wiley-Blackwell, Oxford.

			Alterman, Hyman (1969), Counting People: The Census in History. Harcourt, Brace & World.

			Alvisi, Lorenzo, Allen Clement, Alessandro Epasto, Silvio Lattanzi, Alessandro Panconesi (2013), “Sok: The evolution of sybil defense via social networks”. ieee Symposium on Security and Privacy.

			Amodei, Dario, Danny Hernandez (2018), “ai and compute”. https://openai.com/blog/ai-and-compute/.

			Ananny, Mike, Kate Crawford (2018), “Seeing without knowing: Limitations of the transparency ideal and its application to algorithmic accountability”. In New Media & Society, 20 (3), pp. 973-989. doi: 10.1177/1461444816676645.

			Andrae, Anders, Tomas Edler (2015), “On global electricity usage of communication technology: Trends to 2030”. In Challenges, 6 (1), pp. 117-157. doi: 10.3390/challe6010117.

			Andrighetto, Giulia, Guido Governatori, Pablo Noriega, Leendert W.N. van der Torre (2013), Normative Multi-Agent Systems, vol. 4. Schloss Dagstuhl-Leibniz-Zentrum für Informatik.

			Angwin, Julia, Jeff Larson, Surya Mattu, Lauren Kirchner (2016), Machine Bias. https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing.

			Anthony, Lasse F. Wolff, Benjamin Kanding, Raghavendra Selvan (2020), “Carbontracker: Tracking and predicting the carbon footprint of training deep learning models”. ArXiv preprint, arXiv:2007.03051.

			Applin, Sally A., Michael D. Fischer (2016), “New technologies and mixed-use convergence: How humans and algorithms are adapting to each other”. ieee Xplore Abstract.

			Archbold, John Frederick (1991), Criminal Pleading, Evidence and Practice. Sweet & Maxwell.

			Arkin, Ronald C. (2008), “Governing lethal behavior: Embedding ethics in a hybrid deliberative/reactive robot architecture”. In Proceedings of the 3rd acm/ieee International Conference on Human Robot Interaction.

			Arkin, Ronald C., Patrick Ulam (2012), “Overriding ethical constraints in lethal autonomous systems”. Georgia Institute of Technology. Atlanta Mobile Robot Lab.

			Arnold, Matthew, Rachel K.E. Bellamy, Michael Hind, Stephanie Houde, Sameep Mehta, Aleksandra Mojsilovic, Ravi Nair, Karthikeyan Natesan Ramamurthy, Darrell Reimer, Alexandra Olteanu, David Piorkowski, Jason Tsay, Kush R. Varshney (2019), “FactSheets: Increasing trust in ai services through supplier’s declarations of conformity”. ArXiv preprint, arXiv:1808.07261.

			Arora, Sanjeev, Boaz Barak (2009), Computational Complexity: A Modern Approach. Cambridge University Press, Cambridge.

			Ashworth, Andrew (2010), “Should strict criminal liability be removed from all imprisonable offences?”. In Irish Jurist, vol. xlv, pp. 1-21.

			Avgerinou, Maria, Paolo Bertoldi, Luca Castellazzi (2017), “Trends in data centre energy consumption under the European Code of conduct for data centre energy efficiency”. In Energies, 10 (10), p. 1470. doi: 10.3390/en10101470.

			Bambauer, Jane, Tal Zarsky (2018), “The algorithmic game”. In Notre Dame Law Review, 94 (1), pp. 1-47.

			Banjo, Omotayo (2018), “Bias in maternal ai could hurt expectant black mothers”. In Motherboard, 17 agosto.

			Barnes, Elizabeth A., James W. Hurrell, Imme Ebert-Uphoff, Chuck Anderson, David Anderson (2019), “Viewing forced climate patterns through an ai lens”. In Geophysical Research Letters, 46 (22), pp. 13389-13398. doi:10.1029/2019GL084944.

			Barocas, Solon, Andrew D. Selbst (2016), “Big data’s disparate impact”. In ssrn Electronic Journal. https://doi.org/10.2139/ssrn.2477899.

			Bartneck, Christoph, Tony Belpaeme, Friederike Eyssel, Takayuki Kanda, Merel Keijsers, Selma Šabanović (2020), Human-Robot Interaction: An Introduction. Cambridge University Press, Cambridge.

			Baum, Seth D. (2020), “Social choice ethics in artificial intelligence”. In ai & Society, pp. 1-12.

			Baumer, Eric P.S. (2017), “Toward human-centered algorithm design”. In Big Data & Society, 4 (2), 205395171771885.

			Beauchamp, Tom L., James F. Childress (2013), Principles of Biomedical Ethics. 7a ed. Oxford University Press, New York.

			Beer, David (2017), “The social power of algorithms”. In Information Communication & Society, 20 (1), pp. 1-13. https://doi.org/10.1080/1369118X.2016.1216147.

			Beijing Academy of Artificial Intelligence (2019), “Beijing ai Principles”. https://www.baai.ac.cn/blog/beijing-ai-principles.

			Belkhir, Lotfi, Ahmed Elmeligi (2018), “Assessing ict global emissions footprint: Trends to 2040 & recommendations”. In Journal of Cleaner Production, 177, pp. 448-463. doi: 10.1016/j.jclepro.2017.12.239.

			Bendel, Oliver (2019), “The synthetization of human voices”. In Ai & Society, 34 (1), pp. 83-89.

			Benjamin, Ruha (2019), Race After Technology: Abolitionist Tools for the New Jim Code. Polity, Medford.

			Benkler, Yochai (2019), “Don’t let industry write the rules for ai”. In Nature, 569 (7754), pp. 161-162.

			Berk, R., H. Heidari, S. Jabbari, M. Kearns, A. Roth (2018), “Fairness in criminal justice risk assessments: The state of the art”. In Sociological Methods and Research. https://doi.org/10.1177/0049124118782533.

			Bilge, Leyla, Thorsten Strufe, Davide Balzarotti, Engin Kirda (2009), “All your contacts are belong to us: Automated identity theft attacks on social networks”. In Proceedings of the 18th International Conference on World Wide Web.

			Bilgic, Mustafa, Raymond Mooney (2005), “Explaining recommendations: Satisfaction vs. promotion”. In Proceedings of Beyond Personalization 2005: A Workshop on the Next Stage of Recommender Systems Research. The 2005 International Conference on Intelligent User Interfaces (1° gennaio), pp. 13-18.

			Binns, Reuben (2018), “Fairness in machine learning: Lessons from political philosophy”. ArXiv:1712.03586 [cs].

			Blyth, Colin R. (1972), “On Simpson’s paradox and the sure-thing principle”. In Journal of the American Statistical Association, 67 (338), pp. 364-366. https://doi.org/10.1080/01621459.1972.10482387.

			Boland, H. (2018), “Tencent executive urges Europe to focus on ethical uses of artificial intelligence”. In The Telegraph, 14 ottobre.

			Boshmaf, Yazan, Ildar Muslukhov, Konstantin Beznosov, Matei Ripeanu (2012), “Key challenges in defending against malicious socialbots”. Presented as part of the 5th {usenix} Workshop on Large-Scale Exploits and Emergent Threats.

			Boshmaf, Yazan, Ildar Muslukhov, Konstantin Beznosov, Matei Ripeanu (2013). “Design and analysis of a social botnet”. In Computer Networks, 57 (2), pp. 556-578.

			Bostrom, Nick (2014), Superintelligence: Paths, Dangers, Strategies. Oxford University Press, Oxford (Superintelligenza. Tendenze, pericoli, strategie. Tr. it. Bollati Boringhieri, Torino 2018).

			Boutilier, Craig (2002), “A pomdp formulation of preference elicitation problems”. aaai/iaai, pp. 239-246.

			Boyd, Danah, Kate Crawford (2012), “Critical questions for big data”. In Information, Communication & Society, 15 (5), pp. 662-679. https://doi.org/10.1080/1369118X.2012.678878.

			Bradshaw, Jeffrey M., Stuart Dutfield, Pete Benoit, John D. Woolley (1997), “KAoS: Toward an industrial-strength open agent architecture”. In Software Agents, 13, pp. 375-418.

			British Academy, The Royal Society (2017), “Data management and use: Governance in the 21st century – A joint report by the British Academy and the Royal Society”.

			Broadhurst, Roderic, Donald Maxim, Paige Brown, Harshit Trivedi, Joy Wang (2019), “Artificial intelligence and crime”. In ssrn Electronic Journal. https://ssrn.com/abstract=3407779.

			Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell (2020), “Language models are few-shot learners”. ArXiv preprint, arXiv:2005.14165.

			Brundage, Miles, Shahar Avin, Jack Clark, Helen Toner, Peter Eckersley, Ben Garfinkel, Allan Dafoe, Paul Scharre, Thomas Zeitzoff, Bobby Filar (2018), “The malicious use of artificial intelligence: Forecasting, prevention, and mitigation”. ArXiv preprint, arXiv:1802.07228.

			Brundage, Miles, Shahar Avin, Jasmine Wang, Haydn Belfield, Gretchen Krueger, Gillian Hadfield, Heidy Khlaaf, Jingying Yang, Helen Toner, Ruth Fong (2020), “Toward trustworthy ai development: Mechanisms for supporting verifiable claims”. ArXiv preprint, arXiv:2004.07213.

			Brundtland, Gro Harlem (1987), The Brundtland Report, World Commission on Environment and Development. Oxford University Press, Oxford.

			Buhmann, Alexander, Johannes Paßmann, Christian Fieseler (2019), “Managing algorithmic accountability: Balancing reputational concerns, engagement strategies, and the potential of rational discourse”. In Journal of Business Ethics. doi: 10.1007/s10551-019-04226-4.

			Burgess, Matt (2017), “nhs DeepMind deal broke data protection law, regulator rules”. In Wired uk, 3 luglio.

			Burke, Robin (2017), “Multisided fairness for recommendation”. ArXiv preprint, arXiv:1707.00093.

			Burns, Alistair, Peter Rabins (2000), “Carer burden in dementia”. In International Journal of Geriatric Psychiatry, 15 (S1), pp. S9-S13.

			Burrell, Jenna (2016), “How the machine ‘thinks’: Understanding opacity in machine learning algorithms”. In Big Data & Society, 3 (1), 205395171562251. https://doi.org/10.1177/2053951715622512.

			C2E2 (2018), “Greenhouse gas emissions in the ict sector”. https://c2e2.unepdtu.org/collection/c2e2-publications/, ultima modifica 2018.

			Cabinet Office, Government Digital Service (2016), Data Science Ethical Framework.

			Cai, Han, Chuang Gan, Tianzhe Wang, Zhekai Zhang, Song Han (2020), “Once-for-all: Train one network and specialize it for efficient deployment”. ArXiv:1908.09791 [cs, stat].

			Caldwell, M., J.T.A. Andrews, T. Tanay, L.D. Griffin (2020), “ai-enabled future crime”. In Crime Science, 9 (1), pp. 1-13.

			Caliskan, Aylin, Joanna J. Bryson, Arvind Narayanan (2017), “Semantics derived automatically from language corpora contain human-like biases”. In Science, 356 (6334), pp. 183-186. doi: 10.1126/science.aal4230.

			Callaway, Ewen (2020), “‘It will change everything’: DeepMind’s ai makes gigantic leap in solving protein structures”. In Nature, 588, pp. 203-204.

			Campbell, Murray, A. Joseph Hoane Jr, Feng-hsiung J. Hsu (2002), “Deep blue”. In Artificial intelligence, 134 (1-2), pp. 57-83.

			Carton, Samuel, Jennifer Helsby, Kenneth Joseph, Ayesha Mahmud, Youngsoo Park, Joe Walsh, Crystal Cody, C.P.T. Estella Patterson, Lauren Haynes, Rayid Ghani (2016), “Identifying police officers at risk of adverse events”. In Proceedings of the 22nd acm sigkdd International Conference on Knowledge Discovery and Data Mining, pp. 67-76.

			Cath, Corinne N.J., Ludovica Glorioso, Mariarosaria Taddeo (2017), “nato ccd coe Workshop on ‘Ethics and policies for cyber warfare’ – A Report”. In Ethics and Policies for Cyber Operations, a cura di Mariarosaria Taddeo e Ludovica Glorioso, pp. 231-241. Springer International Publishing, Cham.

			Cath, Corinne, Sandra Wachter, Brent Mittelstadt, Mariarosaria Taddeo, Luciano Floridi (2018), “Artificial intelligence and the ‘good society’: The us, eu, and uk approach”. In Science and Engineering Ethics, 24 (2), pp. 505-528. doi: 10.1007/s11948-017-9901-7.

			cdc (2019), “Pregnancy mortality surveillance system: Maternal and infant health”. Ultima modifica 16 gennaio 2019. T01:19:13Z/.

			Chajewska, Urszula, Daphne Koller, Ronald Parr (2000), “Making rational decisions using adaptive utility elicitation”. In Proceedings of the 17th National Conference on Artificial Intelligence and 12th Conference on Innovative Applications of Artificial Intelligence, pp. 363-369.

			Chakraborty, Abhijnan, Gourab K. Patro, Niloy Ganguly, Krihan P. Gummadi, Patrick Loiseau (2019), “Equality of voice: Towards fair representation in crowdsourced top-K recommendations”. In Proceedings of the Conference on Fairness, Accountability, and Transparency-fat* ’19, pp. 129-138. acm Press, Atlanta, ga. https://doi.org/10.1145/3287560.3287570.

			Chantler, Nic, Roderic Broadhurst (2008), “Social engineering and crime prevention in cyberspace”. In Proceedings of the Korean Institute of Criminology, pp. 65-92.

			Chattopadhyay, Ashesh, Pedram Hassanzadeh, Saba Pasha (2020), “Predicting clustered weather patterns: A test case for applications of convolutional neural networks to spatio-temporal climate data”. In Scientific Reports, 10 (1), pp. 1317. doi: 10.1038/s41598-020-57897-9.

			Chen, Ying-Chieh, Patrick S. Chen, Ronggong Song, Larry Korba (2004), “Online gaming crime and security issue-cases and countermeasures from Taiwan”. In Proceedings of the 2nd Annual Conference on Privacy, Security and Trust. Fredericton, New Brunswick, Canada. 13-15 ottobre, pp. 1-8.

			Chen, Ying-Chieh, Patrick S. Chen, Jing-Jang Hwang, Larry Korba, Ronggong Song, George Yee (2005), “An analysis of online gaming crime characteristics”. In Internet Research, vol. 15, 3, pp. 246-261.

			China State Council (2017), “State council notice on the issuance of the next generation artificial intelligence development plan”. 8 luglio. Consultato il 18 settembre 2018, su http://www.gov.cn/zhengce/content/2017-07/20/content_5211996.htm. Traduzione di R. Creemers, G. Webster, P. Triolo, E. Kania, https://www.newamerica.org/documents/1959/translation-fulltext-8.1.17.pdf.

			Choi, Changhyun, Jeonghwan Kim, Jongsung Kim, Donghyun Kim, Younghye Bae, Hung Soo Kim (2018), “Development of heavy rain damage prediction model using machine learning based on big data”. In Advances in Meteorology.

			Christian, Jon (2019), “Bill Gates compares artificial intelligence to nuclear weapons”. In Futurism, 19 marzo.

			Chu, Yi, Young Chol Song, Richard Levinson, Henry Kautz (2012), “Interactive activity recognition and prompting to assist people with cognitive disabilities”. In Journal of Ambient Intelligence and Smart Environments, 4 (5), pp. 443-459. doi: 10.3233/AIS-2012-0168.

			Chu, Zi, Steven Gianvecchio, Haining Wang, Sushil Jajodia (2010), “Who is tweeting on Twitter: Human, bot, or cyborg?”. In Proceedings of the 26th Annual Computer Security Applications Conference.

			Chui, Michael, James Manyika, Mehdi Miremadi, Nicolaus Henke, Rita Chung, Pieter Nel, Sankalp Malhotra (2018), “Notes from the ai frontier: Insights from hundreds of use cases”. McKinsey Global Institute.

			Cifuentes, Jenny, Geovanny Marulanda, Antonio Bello, Javier Reneses (2020), “Air temperature forecasting using machine learning techniques: A review”. In Energies, 13 (16), p. 4215.

			Cliff, Dave, Linda Northrop (2012), “The global financial markets: An ultra-large-scale systems perspective”. Monterey workshop.

			Cobb, Matthew (2020a), The Idea of the Brain: A History. Profile Books, London.

			Cobb, Matthew (2020b), “Why your brain is not a computer”. In The Guardian, 27 febbraio.

			Cohen, Julie E. (2000), “Examined lives: Informational privacy and the subject as object”. In Georgetown Law Faculty Publications and Other Works, gennaio. https://scholarship.law.georgetown.edu/facpub/810.

			Corbett-Davies, Sam, Sharad Goel (2018), “The measure and mismeasure of fairness: A critical review of fair machine learning”. ArXiv:1808.00023 [cs].

			Corea, Francesco (2018), “ai Knowledge Map: How to classify ai technologies, a sketch of a new ai technology landscape”. In Medium – Artificial Intelligence, 29 agosto. https://medium.com/@Francesco_AI/ai-knowledge-map-how-to-classify-ai-technologies-6c073b969020.

			Cowls, Josh, Thomas King, Mariarosaria Taddeo, Luciano Floridi (2019), “Designing ai for social good: Seven essential factors”. In ssrn Electronic Journal. doi: 10.2139/ssrn.3388669.

			Cowls, Josh, Marie-Thérèse Png, Yung Au (non pubblicato), “Some tentative foundations for ‘global’ algorithmic ethics”.

			Cowls, Josh, Andreas Tsamados, Mariarosaria Taddeo, Luciano Floridi (2021a), “The ai gambit – Leveraging artificial intelligence to combat climate change: Opportunities, challenges, and recommendations”. In ssrn Electronic Journal. http://dx.doi.org/10.2139/ssrn.3804983.

			Cowls, Josh, Andreas Tsamados, Mariarosaria Taddeo, Luciano Floridi (2021b), “A definition, benchmark and database of ai for social good initiatives”. In Nature Machine Intelligence, 3 (2), pp. 111-115. doi: 10.1038/s42256-021-00296-0.

			Cracked Readers (2014), “26 hilariously inaccurate predictions about the future”, 27 gennaio. http://www.cracked.com/photoplasty_777_26-hilariously-inaccurate-predictions-about-future/#ixzz3QWYlN4qg.

			Crain, Matthew (2018), “The limits of transparency: Data brokers and commodification”. In New Media & Society, 20 (1), pp. 88-104. https://doi.org/10.1177/1461444816657096.

			Crawford, Kate (2016), “Artificial intelligence’s white guy problem”. Ultima modifica 25 giugno 2016.

			Crawford, Kate, Jason Schultz (2014), “Big data and due process: Toward a framework to redress predictive privacy harms”. In bcl Review, 55, p. 93.

			Cummings, Mary (2012), “Automation bias in intelligent time critical decision support systems”. In aiaa 1st Intelligent Systems Technical Conference. American Institute of Aeronautics and Astronautics, Chicago, il. https://doi.org/10.2514/6.2004-6313.

			D’Agostino, Marcello, Luciano Floridi (2009), “The enduring scandal of deduction. Is propositional logic really uninformative?”. In Synthese, 167 (2), pp. 271-315.

			Dahl, E.S. (2018), “Appraising Black-boxed technology: The positive prospects”. In Philosophy & Technology, 31 (4), pp. 571-591. https://doi.org/10.1007/s13347-017-0275-1.

			Danaher, John (2017), “Robotic rape and robotic child sexual abuse: Should they be criminalised?”. In Criminal Law and Philosophy, 11 (1), pp. 71-95.

			Dandres, Thomas, Nathan Vandromme, Glasha Obrekht, Andy Wong, Kim Khoa Nguyen, Yves Lemieux, Mohamed Cheriet, Réjean Samson (2017), “Consequences of future data center deployment in Canada on electricity generation and environmental impacts: A 2015-2030 prospective study”. In Journal of Industrial Ecology, 21 (5), pp. 1312-1322.

			Danks, David, Alex John London (2017), “Algorithmic bias in autonomous systems”. In Proceedings of the 26th International Joint Conference on Artificial Intelligence, pp. 4691-4697. International Joint Conferences on Artificial Intelligence Organization, Melbourne, Australia. https://doi.org/10.24963/ijcai.2017/654.

			Darling, Kate (2015), “‘Who’s Johnny?’Anthropomorphic framing in human-robot interaction, integration, and policy”. In Robot Ethics 2.0, a cura di P. Lin, G. Bekey, K. Abney, R. Jenkins. Oxford University Press, Oxford 2017.

			Datta, Amit, Michael Carl Tschantz, Anupam Datta (2015), “Automated experiments on Ad privacy settings”. In Proceedings on Privacy Enhancing Technologies, 2015 (1), pp. 92–112. https://doi.org/10.1515/popets-2015-0007.

			Davenport, Thomas, Ravi Kalakota (2019), “The potential for artificial intelligence in healthcare”. In Future Healthcare Journal, 6 (2), p. 94.

			Davis, Ernest, Gary Marcus (2019), Rebooting ai: Building Artificial Intelligence We Can Trust. Pantheon Books, New York.

			De Angeli, Antonella (2009), “Ethical implications of verbal disinhibition with conversational agents”. In PsychNology Journal, 7 (1).

			De Angeli, Antonella, Sheryl Brahnam (2008), “I hate you! Disinhibition with virtual partners”. In Interacting with Computers, 20 (3), pp. 302-310.

			De Fauw, Jeffrey, Joseph R. Ledsam, Bernardino Romera-Paredes, Stanislav Nikolov, Nenad Tomasev, Sam Blackwell, Harry Askham, Xavier Glorot, Brendan O’Donoghue, Daniel Visentin, George van den Driessche, Balaji Lakshminarayanan, Clemens Meyer, Faith Mackinder, Simon Bouton, Kareem Ayoub, Reena Chopra, Dominic King, Alan Karthikesalingam, Cían O. Hughes, Rosalind Raine, Julian Hughes, Dawn A. Sim, Catherine Egan, Adnan Tufail, Hugh Montgomery, Demis Hassabis, Geraint Rees, Trevor Back, Peng T. Khaw, Mustafa Suleyman, Julien Cornebise, Pearse A. Keane, Olaf Ronneberger (2018), “Clinically applicable deep learning for diagnosis and referral in retinal disease”. In Nature Medicine, 24 (9), pp. 1342-1350.

			de Lima Salge, Carolina Alves, Nicholas Berente (2017), “Is that social bot behaving unethically?”. In Communications of the acm, 60 (9), pp. 29-31.

			Delamaire, Linda, Hussein Abdou, John Pointon (2009), “Credit card fraud and detection techniques: A review”. In Banks and Bank Systems, 4 (2), pp. 57-68.

			Delcker, Janosch (2018), “Europe’s silver bullet in global ai battle: Ethics”. In Politico, 3 marzo.

			Delmas, Magali A., Vanessa Cuerel Burbano (2011), “The drivers of greenwashing”. In California Management Review, 54 (1), pp. 64-87. doi: 10.1525/cmr.2011.54.1.64.

			Demir, Hilmi (2012), Luciano Floridi’s Philosophy of Technology: Critical Reflections. Springer, Dordrecht-London.

			Dennett, D.C. (1987), The Intentional Stance. mit Press, Cambridge, ma-London (L’atteggiamento intenzionale. Tr. it. il Mulino, Bologna 1993).

			Dennis, Louise, Michael Fisher, Marija Slavkovik, Matt Webster (2016), “Formal verification of ethical choices in autonomous systems”. In Robotics and Autonomous Systems, 77, pp. 1-14. doi: 10.1016/j.robot.2015.11.012.

			Di Piazza, A., M.C. Di Piazza, G. La Tona, M. Luna (2020), “An artificial neural network-based forecasting model of energy-related time series for electrical grid management”. In Mathematics and Computers in Simulation. doi: 10.1016/j.matcom.2020.05.010.

			Diakopoulos, Nicholas, Michael Koliska (2017), “Algorithmic transparency in the news media”. In Digital Journalism, 5 (7), pp. 809-828. https://doi.org/10.1080/21670811.2016.1208053.

			Dignum, Virginia, Matteo Baldoni, Christina Baroglio, Maurizio Caon, Raja Chatila, Louise Dennis, Gonzalo Génova, Galit Haim, Malte S. Kließ, Maite Lopez-Sanchez, Roberto Micalizio, Juan Pavón, Marija Slavkovik, Matthijs Smakman, Marlies van Steenbergen, Stefano Tedeschi, Leon van der Toree, Serena Villata, Tristan de Wildt (2018), “Ethics by design: Necessity or curse?”. In Proceedings of the 2018 aaai/acm Conference on ai, Ethics, and Society-aies ’18, pp. 60-66. acm Press, New Orleans, la. https://doi.org/10.1145/3278721.3278745.

			Ding, Jeffrey (2018), “Deciphering China’s ai dream”. In Future of Humanity Institute Technical Report. https://www.fhi.ox.ac.uk/wp-content/uploads/Deciphering_Chinas_AI-Dream.pdf.

			Dobbe, Roel, Oscar Sondermeijer, David Fridovich-Keil, Daniel Arnold, Duncan Callaway, Claire Tomlin (2019), “Toward distributed energy services: Decentralizing optimal power flow with machine learning”. In ieee Transactions on Smart Grid, 11 (2), pp. 1296-1306.

			Döring, Nicola, M. Rohangis Mohseni, Roberto Walter (2020), “Design, use, and effects of sex dolls and sex robots: Scoping review”. In Journal of Medical Internet Research, 22 (7), p. e18551.

			Doshi-Velez, Finale, Been Kim (2017), “Towards a rigorous science of interpretable machine learning”. ArXiv preprint, arXiv:1702.08608.

			Dremliuga, Roman, Natalia Prisekina (2020), “The concept of culpability in criminal law and ai systems”. In Journal of Politics & Law, 13, p. 256.

			Dreyfus, Hubert L. (1972), What Computers Can’t Do: A Critique of Artificial Reason. Harper & Row, New York-London.

			Dreyfus, Hubert L. (1979), What Computers Can’t Do: The Limits of Artificial Intelligence, ed. rivista. Harper colophon books. Harper & Row, New York (Che cosa non possono fare i computer. I limiti dell’intelligenza artificiale. Tr. it. Armando, Roma 1988).

			Dreyfus, Hubert L. (1992), What Computers Still Can’t Do: A Critique of Artificial Reason. 3a ed. mit Press, Cambridge, ma-London

			Durante, Massimo (2017), “Ethics, law and the politics of information: A guide to the philosophy of Luciano Floridi”. In The International Library of Ethics, Law and Technology. Springer, Dordrecht.

			Dworkin, Ronald M. (1967), “The model of rules”. In The University of Chicago Law Review, 35 (1), pp. 14-46.

			Edmonds, Bruce, Carlos Gershenson (2015), “Modelling complexity for policy: Opportunities and challenges”. In Handbook on Complexity and Public Policy. Edward Elgar Publishing, Cheltenham.

			edps Ethics Advisory Group (2018), “Towards a digital ethics”. edps Report.

			Edwards, Lilian, Michael Veale (2017), “Slave to the algorithm? Why a right to explanationn is probably not the remedy you are looking for”. In ssrn Electronic Journal. https://doi.org/10.2139/ssrn.2972855.

			ege (2018), “European Commission’s European group on ethics in science and new technologies, statement on artificial intelligence, robotics and ‘autonomous’ systems”. https://ec.europa.eu/info/news/ethics-artificial-intelligence-statement-ege-released-2018-apr-24_en.

			Eicher, Bobbie, Lalith Polepeddi, Ashok Goel (2017), “Jill Watson doesn’t care if you’re pregnant: Grounding ai ethics in empirical studies”. In aies ’18: Proceedings of the 2018 aaai/acm Conference on ai, Ethics, and Society.

			Ekstrand, Michael, Karen Levy (2018), fat* Network. https://fatconference.org/network.

			epa, us (2016), “Greenhouse gas emissions from a typical passenger vehicle”. [Overviews and Factsheets], ultima modifica 2016-01-12T16:29:25-05:00.

			Epstein, Robert (2016), “The empty brain”. In Aeon, 18 maggio.

			Estevez, David, Juan G. Victores, Raul Fernandez-Fernandez, Carlos Balaguer (2017), “Robotic ironing with 3D perception and force/torque feedback in household environments”. 2017 ieee/rsj International Conference on Intelligent Robots and Systems (iros).

			Etzioni, Amitai (1999), “Enhancing privacy, preserving the common good”. In Hastings Center Report, 29 (2), pp. 14-23.

			Eubanks, Virginia (2017), Automating Inequality: How High-Tech Tools Profile, Police, and Punish the Poor. 1a ed. St. Martin’s Press, New York, ny.

			European Commission (2019), “Ethics guidelines for trustworthy ai”, 8 aprile. https://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai.

			European Commission (2020), “Energy-efficient cloud computing technologies and policies for an eco-friendly cloud market”. doi: 10.2759/3320.

			European Commission (2021), “Proposal for a Regulation laying down harmonised rules on artificial intelligence (Artificial Intelligence Act)”. Disponibile online.

			Evans, Richard, Jim Gao (2016), “DeepMind ai reduces Google data centre cooling bill by 40%”. In DeepMind Blog.

			Ezrachi, Ariel, Maurice Stucke (2017), “Two artificial neural networks meet in an online hub and change the future (of competition, market dynamics and society). In Oxford Legal Studies Research Paper, 24, 1° luglio.

			Faltings, Boi, Pearl Pu, Marc Torrens, Paolo Viappiani (2004), “Designing example-critiquing interaction”. In Proceedings of the 9th International Conference on Intelligent User Interfaces.

			Fang, Fei, Thanh H. Nguyen, Rob Pickles, Wai Y. Lam, Gopalasamy R. Clements, Bo An, Amandeep Singh, Milind Tambe, Andrew Lemieux (2016), “Deploying paws: Field optimization of the protection assistant for wildlife security”. 28th iaai Conference, 5 marzo.

			Farmer, J. Doyne, Spyros Skouras (2013), “An ecological perspective on the future of computer trading”. In Quantitative Finance, 13 (3), pp. 325-346.

			Fathi, Soheil, Ravi Srinivasan, Andriel Fenner, Sahand Fathi (2020), “Machine learning applications in urban building energy performance forecasting: A systematic review”. In Renewable and Sustainable Energy Reviews, 133, p. 110287. doi: 10.1016/j.rser.2020.110287.

			Fedus, William, Barret Zoph, Noam Shazeer (2021), “Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity”. ArXiv preprint, arXiv:2101.03961.

			Ferguson, Christopher J., Richard D. Hartley (2009), “The pleasure is momentary… the expense damnable?: The influence of pornography on rape and sexual assault”. In Aggression and Violent Behavior, 14 (5), pp. 323-329.

			Ferrara, Emilio (2015), “‘Manipulation and abuse on social media’ by Emilio Ferrara with Ching-man Au Yeung as coordinator”. In acm sigweb Newsletter (primavera), pp. 1-9.

			Ferrara, Emilio, Onur Varol, Clayton Davis, Filippo Menczer, Alessandro Flammini (2016), “The rise of social bots”. In Communications of the acm, 59 (7), pp. 96-104.

			Floridi, Luciano (1999), Philosophy and Computing: An Introduction. Routledge, London-New York.

			Floridi, Luciano (2002), “On defining library and information science as applied philosophy of information”, In Social Epistemology, 16 (1), pp. 37-49.

			Floridi, Luciano (2003), “Informational realism”. In Selected Papers from Conference on Computers and Philosophy, volume 37, a cura di John Weckert e Yeslam Al-Saggaf. Australian Computer Society, pp. 7-12.

			Floridi, Luciano (2004), “lis as applied philosophy of information: A reappraisal”. In Library Trends, 52 (3), pp. 658-665.

			Floridi, Luciano (2005a), “The ontological interpretation of informational privacy”. In Ethics and Information Technology, 7 (4), pp. 185-200.

			Floridi, Luciano (2005b), “The philosophy of presence: From epistemic failure to successful observation”. In Presence: Teleoperators & Virtual Environments, 14 (6), pp. 656-667.

			Floridi, Luciano (2006), “Privacy in the age of Google”. In The Philosophers’ Magazine, 32.

			Floridi, Luciano (2008a), “Artificial intelligence’s new frontier: Artificial companions and the Fourth Revolution”. In Metaphilosophy, 39 (4/5), pp. 651-655.

			Floridi, Luciano (2008b), “The method of Levels of Abstraction”. In Minds and Machines, 18 (3), pp. 303-329. doi: 10.1007/s11023-008-9113-7.

			Floridi, Luciano (2008c), “Understanding epistemic relevance”. In Erkenntnis, 69 (1), pp. 69-92.

			Floridi, Luciano (2009), “The information society and its philosophy: Introduction to the special issue on “the philosophy of information, its nature, and future developments”. In The Information Society, 25 (3), pp. 153-158.

			Floridi, Luciano (2010a), The Cambridge Handbook of Information and Computer Ethics. Cambridge University Press, Cambridge, uk-New York.

			Floridi, Luciano (2010b), Information: A Very Short Introduction. Oxford University Press, Oxford.

			Floridi, Luciano (2011a), “A defence of constructionism: Philosophy as conceptual engineering”. In Metaphilosophy, 42 (3), pp. 282-304.

			Floridi, Luciano (2011b), The Philosophy of Information. Oxford University Press, Oxford.

			Floridi, Luciano (2012a), “Big Data and their epistemological challenge”. In Philosophy & Technology, 25 (4), pp. 435-437. doi: 10.1007/s13347-012-0093-4.

			Floridi, Luciano (2012b), “Distributed morality in an information society”. In Science and Engineering Ethics, 19 (3), pp. 727-743. doi: 10.1007/s11948-012-9413-4.

			Floridi, Luciano (2013), The Ethics of Information. Oxford University Press, Oxford.

			Floridi, Luciano (2014a), The Fourth Revolution. How the Infosphere Is Reshaping Human Reality. Oxford University Press, Oxford (La quarta rivoluzione. Come l’infosfera sta trasformando il mondo. Tr. it. Raffaello Cortina, Milano 2017).

			Floridi, Luciano (2014b) (a cura di), The Onlife Manifesto. Being Human in a Hyperconnected Era. Springer, New York.

			Floridi, Luciano (2014c), “Open data, data protection, and group privacy”. In Philosophy & Technology, 27 (1), pp. 1-3. doi: 10.1007/s13347-014-0157-8.

			Floridi, Luciano (2014d), “Technoscience and ethics foresight”. In Philosophy & Technology, 27 (4), pp. 499-501. doi: 10.1007/s13347-014-0180-9.

			Floridi, Luciano (2015a), “‘The right to be forgotten’: A philosophical view”. In Jahrbuch für Recht und Ethik – Annual Review of Law and Ethics, 23 (1), pp. 30-45.

			Floridi, Luciano (2015b), “Toleration and the design of norms”. In Science and Engineering Ethics, 21 (5), pp. 1095-1123. doi: 10.1007/s11948-014-9589-x.

			Floridi, Luciano (2016a), “Faultless responsibility: On the nature and allocation of moral responsibility for distributed moral actions”. In Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374 (2083), 20160112. doi: 10.1098/rsta.2016.0112.

			Floridi, Luciano (2016b), “Mature information societies – A matter of expectations”. In Philosophy & Technology, 29 (1), pp. 1-4. doi: 10.1007/s13347-016-0214-6.

			Floridi, Luciano (2016c), “On human dignity as a foundation for the right to privacy”. In Philosophy & Technology, 29 (4), pp. 307-312. doi: 10.1007/s13347-016-0220-8.

			Floridi, Luciano (2016d), “Should we be afraid of ai”. In Aeon Essays. https://aeon.co/essays/true-ai-is-both-logically-possible-and-utterly-implausible.

			Floridi, Luciano (2016e), “Technology and democracy: Three lessons from Brexit”. In Philosophy & Technology, 29 (3), pp. 189-193.

			Floridi, Luciano (2016f), “Tolerant paternalism: Pro-ethical design as a resolution of the dilemma of toleration”. In Science and Engineering Ethics, 22 (6), pp. 1669-1688. doi: 10.1007/s11948-015-9733-2.

			Floridi, Luciano (2017a), “Infraethics – On the conditions of possibility of morality”. In Philosophy & Technology, 30 (4), pp. 391-394. doi: 10.1007/s13347-017-0291-1.

			Floridi, Luciano (2017b), “The logic of design as a conceptual logic of information”. In Minds and Machines, 27 (3), pp. 495-519. doi: 10.1007/s11023-017-9438-1.

			Floridi, Luciano (2017c), “Robots, jobs, taxes, and responsibilities”. In Philosophy & Technology, 30 (1), pp. 1-4. doi: 10.1007/s13347-017-0257-3.

			Floridi, Luciano (2018), “What the maker’s knowledge could be”. In Synthese, 195 (1), pp. 465-481.

			Floridi, Luciano (2019a), “Establishing the rules for building trustworthy ai”. In Nature Machine Intelligence, 1 (6), pp. 261-262. doi: 10.1038/s42256-019-0055-y.

			Floridi, Luciano (2019b), “The green and the blue: Naïve ideas to improve politics in a mature information society”. In The 2018 Yearbook of the Digital Ethics Lab. Springer, pp. 183-221.

			Floridi, Luciano (2019c), The Logic of Information: A Theory of Philosophy as Conceptual Design. Oxford University Press, Oxford (Pensare l’infosfera. La filosofia come design concettuale. Tr. it. parziale Raffaello Cortina, Milano 2020).

			Floridi, Luciano (2019d), “What the near future of artificial intelligence could be”. In Philosophy & Technology, 32 (1), pp. 1-15. doi: 10.1007/s13347-019-00345-y.

			Floridi, Luciano (2020a), “ai and its new winter: From myths to realities”. In Philosophy & Technology, 33 (1), pp. 1-3.

			Floridi, Luciano (2020b), “The fight for digital sovereignty: What it is, and why it matters, especially for the eu”. In Philosophy & Technology, 33 (3), pp. 369-378. doi: 10.1007/s13347-020-00423-6.

			Floridi, Luciano, Massimo Chiriatti (2020), “gpt-3: Its nature, scope, limits, and consequences”. In Minds and Machines, pp. 1-14.

			Floridi, Luciano, Tim Lord Clement-Jones (2019), “The five principles key to any ethical framework for ai”. In New Statesman, 20 marzo. https://tech.newstatesman.com/policy/ai-ethics-framework.

			Floridi, Luciano, Josh Cowls (2019), “A unified framework of five principles for ai in society”. In Harvard Data Science Review, https://doi.org/10.1162/99608f92.8cd550d1.

			Floridi, Luciano, Josh Cowls, Monica Beltrametti, Raja Chatila, Patrice Chazerand, Virginia Dignum, Christoph Luetge, Robert Madelin, Ugo Pagallo, Francesca Rossi, Burkhard Schafer, Peggy Valcke, Effy Vayena (2018), “ai4People – An ethical framework for a good ai society: Opportunities, risks, principles, and recommendations”. In Minds and Machines, 28 (4), pp. 689-707. doi: 10.1007/s11023-018-9482-5.

			Floridi, Luciano, Josh Cowls, Thomas C. King, Mariarosaria Taddeo (2020), “How to design ai for social good: Seven essential factors”. In Science and Engineering Ethics, 26 (3), pp. 1771-1796. doi: 10.1007/s11948-020-00213-5.

			Floridi, Luciano, Phyllis Illari (2014), The Philosophy of Information Quality. Springer International Publishing Switzerland.

			Floridi, Luciano, Kia Nobre (2020), The Green and the Blue: How ai May Be a Force for Good. oecd. Disponibile online.

			Floridi, Luciano, Jeff W. Sanders (2004), “On the morality of artificial agents”. In Minds and Machines, 14 (3), pp. 349-379.

			Floridi, Luciano, Mariarosaria Taddeo, Matteo Turilli (2009), “Turing’s Imitation Game: Still an impossible challenge for all machines and some judges – An evaluation of the 2008 Loebner Contest”. In Minds and Machines, 19 (1), pp. 145-150. doi: 10.1007/s11023-008-9130-6.

			Floridi, Luciano, Mariarosaria Taddeo (2014), The Ethics of Information Warfare. Springer, New York.

			Floridi, Luciano, Mariarosaria Taddeo (2016), “What is data ethics?”. In Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374 (2083).

			Freier, Nathan G. (2008), “Children attribute moral standing to a personified agent”. In Proceedings of the sigchi Conference on Human Factors in Computing Systems.

			Freitas, Pedro Miguel, Francisco Andrade, Paulo Novais (2013), “Criminal liability of autonomous agents: From the unthinkable to the plausible”. International Workshop on ai Approaches to the Complexity of Legal Systems.

			Friedman, Batya, Helen Nissenbaum (1996), “Bias in computer systems”. In acm Transactions on Information Systems (tois), 14 (3), pp. 330-347.

			Fuster, Andreas, Paul Goldsmith-Pinkham, Tarun Ramadorai, Ansgar Walther (2017), “Predictably unequal? The effects of machine learning on credit markets”. In ssrn Electronic Journal. https://doi.org/10.2139/ssrn.3072038.

			Future of Life Institute (2017), “The Asilomar ai Principles”. https://futureoflife.org/ai-principles/.

			Gagne, David John, Hannah M. Christensen, Aneesh C. Subramanian, Adam H. Monahan (2020), “Machine learning for stochastic parameterization: Generative adversarial networks in the Lorenz ’96 model”. In Journal of Advances in Modeling Earth Systems, 12 (3), pp. e2019MS001896. doi: 10.1029/2019MS001896.

			Gajane Pratik, Mikola Pechenizkiy (2018), “On formalizing fairness in prediction with machine learning”. ArXiv preprint, arXiv:1710.03184.

			Ganascia, Jean-Gabriel (2010), “Epistemology of ai revisited in the light of the philosophy of information”. In Knowledge, Technology & Policy, 23 (1), pp. 57-73. doi: 10.1007/s12130-010-9101-0.

			García-Martín, Eva, Crefeda Faviola Rodrigues, Graham Riley, Håkan Grahn (2019), “Estimation of energy consumption in machine learning”. In Journal of Parallel and Distributed Computing, 134, pp. 75-88. doi: 10.1016/j.jpdc.2019.07.007.

			Gauci, Melvin, Jianing Chen, Wei Li, Tony J. Dodd, Roderich Gross (2014), “Clustering objects with robots that do not compute”. In Proceedings of the 2014 International Conference on Autonomous Agents and Multi-Agent Systems.

			Gebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé iii, Kate Crawford (2020), “Datasheets for datasets”. ArXiv preprint, arXiv:1803.09010.

			Ghani, Rayid (2016), “You say you want transparency and interpretability?”. In Rayid Ghani (blog), 29 aprile.

			Gillis, Talia B., Jann L. Spiess (2019), “Big data and discrimination”. In University of Chicago Law Review, 459.

			Gless, Sabine, Emily Silverman, Thomas Weigend (2016), “If robots cause harm, who is to blame? Self-driving cars and criminal liability”. In New Criminal Law Review, 19 (3), pp. 412-436.

			Goel, Ashok, Brian Creeden, Mithun Kumble, Shanu Salunke, Abhinaya Shetty, Bryan Wiltgen (2015), “Using Watson for enhancing human-computer co-creativity”. In Papers from the aaai 2015 Fall Symposium, pp. 22-29.

			Gogarty, Brendan, Meredith Hagger (2008), “The laws of man over vehicles unmanned: The legal response to robotic revolution on sea, land and air”. In Journal of Law, Information and Science, 19, p. 73.

			Golder, Scott A., Michael W. Macy (2011), “Diurnal and seasonal mood vary with work, sleep, and daylength across diverse cultures”. In Science, 333 (6051), pp. 1878-1881.

			González-González, Carina Soledad, Rosa María Gil-Iranzo, Patricia Paderewski-Rodríguez (2021), “Human-robot interaction and sexbots: A systematic literature review”. In Sensors, 21 (1), p. 216.

			Good, I.J. (1965), “Speculations concerning the first ultraintelligent machine”. In Advances in Computers, volume 6, a cura di F. Alt e M. Ruminoff, pp. 31-88.

			Goodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio (2014), “Generative adversarial nets”. In Advances in Neural Information Processing Systems.

			Goodhart, Charles A.E. (1984), “Problems of monetary management: The uk experience”. In Monetary Theory and Practice, pp. 91-121.

			Graeff, Erhardt (2013), “What we should do before the social bots take over: Online privacy protection and the political economy of our near future”. Presentato alla conferenza Media in Transition 8: Public Media, Private Media, mit, Cambridge, 5 maggio, accessibile su https://hdl.handle.net/1721.1/123463.

			Graeff, Erhardt (2014), What We Should Do Before the Social Bots Take Over: Online Privacy Protection and the Political Economy of Our Near Future. mit Media Arts and Sciences.

			Green, Ben (2019), “‘Good’ isn’t good enough”. In Proceedings of the ai for Social Good Workshop at Neurips.

			Green, Ben, Yiling Chen (2019), “Disparate Interactions: An algorithm-in-the-loop analysis of fairness in risk assessments. fat ’19”. In Proceedings of the Conference on Fairness, Accountability, and Transparency, gennaio, pp. 90-99. https://doi.org/10.1145/3287560.3287563.

			Green, Ben, Salomé Viljoen (2020), “Algorithmic realism: Expanding the boundaries of algorithmic thought”. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, pp. 19-31. acm, Barcelona. https://doi.org/10.1145/3351095.3372840.

			Gregor, Shirley, Izak Benbasat (1999), “Explanations from intelligent systems: Theoretical foundations and implications for practice”. In mis Quarterly, 23, pp. 497-530. doi: 10.2307/249487.

			Grgić-Hlača, Nina, Elissa M. Redmiles, Krishna P. Gummadi, Adrian Weller (2018), “Human perceptions of fairness in algorithmic decision making: A case study of criminal risk prediction”. ArXiv:1802.09548 [cs, stat].

			Grote, Thomas, Philipp Berens (2020), “On the ethics of algorithmic decision-making in healthcare”. In Journal of Medical Ethics, 46 (3), pp. 205-211. https://doi.org/10.1136/medethics-2019-105586.

			Grut, Chantal (2013), “The challenge of autonomous lethal robotics to international humanitarian law”. In Journal of Conflict and Security Law, 18 (1), pp. 5-23.

			Hage, Jaap (2018), “Two concepts of constitutive rules”. In Argumenta, 4, 1, pp. 21-39.

			Hagendorff, Thilo (2020), “The ethics of ai ethics: An evaluation of guidelines”. In Minds and Machines, 30 (1), pp. 99-120.

			Hager, Gregory D., Ann Drobnis, Fei Fang, Rayid Ghani, Amy Greenwald, Terah Lyons, David C. Parkes, Jason Schultz, Suchi Saria, Stephen F. Smith, Milind Tambe (2019), “Artificial intelligence for social good”. ArXiv preprint, arXiv:1901.05406 [cs].

			Hallevy, Gabriel (2011), “Unmanned vehicles: Subordination to criminal law under the modern concept of criminal liability”. In Journal of Law, Information and Science, 21, p. 200.

			Ham, Yoo-Geun, Jeong-Hwan Kim, Jing-Jia Luo (2019), “Deep learning for multi-year enso forecasts”. In Nature, 573 (7775), pp. 568-572. doi: 10.1038/s41586-019-1559-7.

			Haque, Albert, Michelle Guo, Alexandre Alahi, Serena Yeung, Zelun Luo, Alisha Rege, Jeffrey Jopling, Lance Downing, William Beninati, Amit Singh, Terry Platchek, Arnold Milstein, Li Fei-Fei (2017), “Towards vision-based smart hospitals: A system for tracking and monitoring hand hygiene compliance”. In Proceedings of the 2nd Machine Learning for Healthcare Conference, pmlr 68, pp. 75-87.

			Harel, David (2000), Computers Ltd: What They Really Can’t Do. Oxford University Press, Oxford.

			Harwell, Drew (2020), “Dating apps need women. Advertisers need diversity. ai companies offer a solution: Fake people”. In The Washington Post, 7 gennaio.

			Hauer, Tomas (2019), “Society caught in a labyrinth of algorithms: Disputes, promises, and limitations of the new order of things”. In Society, 56 (3), pp. 222-230. doi: 10.1007/s12115-019-00358-5.

			Haugen, Geir Marius Sætenes (2017), “Manipulation and deception with social bots: Strategies and indicators for minimizing impact”. ntnu Open.

			Hay, George A., Daniel Kelley (1974), “An empirical survey of price fixing conspiracies”. In The Journal of Law and Economics, 17 (1), pp. 13-38.

			Hayward, Keith J., Matthijs M. Maas (2020), “Artificial intelligence and crime: A primer for criminologists”. In Crime, Media, Culture, 17 (2), pp. 209-233.

			Hegel, Georg Wilhelm Friedrich (1807), The Phenomenology of Spirit, The Cambridge Hegel Translations. Tr. ing. Cambridge University Press, Cambridge 2009.

			Henderson, Peter, Jieru Hu, Joshua Romoff, Emma Brunskill, Dan Jurafsky, Joelle Pineau (2020), “Towards the systematic reporting of the energy and carbon footprints of machine learning”. In Journal of Machine Learning Research, 21 (248), pp. 1-43.

			Henderson, Peter, Koustuv Sinha, Nicolas Angelard-Gontier, Nan Rosemary Ke, Genevieve Fried, Ryan Lowe, Joelle Pineau (2018), “Ethical challenges in data-driven dialogue systems”. In Proceedings of the 2018 aaai/acm Conference on ai, Ethics, and Society, pp. 123-129. acm, New Orleans, la. https://doi.org/10.1145/3278721.3278777.

			Henry, Katharine E., David N. Hager, Peter J. Pronovost, Suchi Saria (2015), “A targeted real-time early warning score (trewScore) for septic shock”. In Science Translational Medicine, 7 (299), 299ra122. doi: 10.1126/scitranslmed.aab3719.

			Herlocker, Jonathan L., Joseph A. Konstan, John Riedl (2000), “Explaining collaborative filtering recommendations”. In Proceedings of the 2000 acm Conference on Computer Supported Cooperative Work, cscw, dicembre, pp. 241-250.

			Herritt, Robert (2014), “Google’s philosopher”. In Pacific Standard, 30 dicembre. http://www.psmag.com/nature-and-technology/googles-philosopher-technology-nature-identity-court-legal-policy-95456.

			Hilbert, Martin (2016), “Big data for development: A review of promises and challenges”. In Development Policy Review, 34 (1), pp. 135-174.

			Hildebrandt, Mireille (2008), “Ambient intelligence, criminal liability and democracy”. In Criminal Law and Philosophy, 2, pp. 163-180. doi: 10.1007/s11572-007-9042-1.

			Hill, Robin K. (2016), “What an algorithm is”. In Philosophy & Technology, 29 (1), pp. 35-59. doi: 10.1007/s13347-014-0184-5.

			Hintemann, Ralph, Simon Hinterholzer (2019), “Energy consumption of data centers worldwide”. The 6th International Conference on ict for Sustainability (ict4s), Lappeenranta.

			hlegai (2018), “High level expert group on artificial intelligence, eu – Draft ethics guidelines for trustworthy ai”, 18 dicembre. https://ec.europa.eu/digital-single-market/en/news/draft-ethics-guidelines-trustworthy-ai.

			hlegai (2019), “High level expert group on artificial intelligence, eu – Ethics guidelines for trustworthy ai”, 8 aprile. https://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai.

			Hoffmann, Anna Lauren, Sarah T. Roberts, Christine T. Wolf, Stacy Wood (2018), “Beyond fairness, accountability, and transparency in the ethics of algorithms: Contributions and perspectives from lis”. In Proceedings of the Assocociation for Information, Science and Technolohy, 55 (1), pp. 694-696. https://doi.org/10.1002/pra2.2018.14505501084.

			Holley, Peter (2014), “Stephen Hawking just got an artificial intelligence upgrade, but still thinks ai could bring an end to mankind”. In The Washington Post, 2 dicembre.

			Holley, Peter (2015), “Bill Gates on dangers of artificial intelligence: ‘I don’t understand why some people are not concerned’”. In The Washington Post, 29 gennaio.

			House of Lords – Artificial Intelligence Committee (2017), “ai in the uk: Ready, willing and able?”. In Report of Session 2017-19 hl Paper 100, 16 aprile.

			Howe, Bill, Julia Stoyanovich, Haoyue Ping, Bernease Herman, Matt Gee (2017), “Synthetic data for social good”. ArXiv preprint, arXiv:1710.08874.

			Hu, Margaret (2017), “Algorithmic Jim Crow”. In Fordham Law Review. https://ir.lawnet.fordham.edu/flr/vol86/iss2/13/.

			Hunt, Elle (2016), “Tay, Microsoft’s ai chatbot, gets a crash course in racism from Twitter”. In The Guardian, 24 marzo.

			Hutson, Matthew (2019), “Bringing machine learning to the masses”. In Science, 365 (6452), pp. 416-417. https://doi.org/10.1126/science.365.6452.416.

			ico (2020), “ico and the Turing consultation on explaining ai decisions guidance”. ico. 30 marzo. https://ico.org.uk/about-the-ico/ico-and-stakeholder-consultations/ico-and-the-turing-consultation-on-explaining-ai-decisions-guidance/.

			ieee (2017), “Ethically aligned design: A vision for prioritizing human well-being with autonomous and intelligent systems, version 2”. https://standards.ieee.org/news/2017/ead_v2.html.

			Imperial College London (2017), “Written Submission to House of Lords Select Committee on Artificial Intelligence [AIC0214]”, 11 ottobre. Consultato il 18 settembre 2018 su http://bit.ly/2yleuET.

			Inderwildi, Oliver, Chuan Zhang, Xiaonan Wang, Markus Kraft (2020), “The impact of intelligent cyber-physical systems on the decarbonization of energy”. In Energy & Environmental Science, 13 (3), pp. 744-771. doi: 10.1039/C9EE01919G.

			Information Commissioner’s Office (2017), “Royal Free – Google DeepMind trial failed to comply with data protection law”. Ultima modifica 3 luglio 2017.

			Ise, Takeshi, Yurika Oba (2019), “Forecasting climatic trends using neural networks: An experimental study using global historical data”. In Frontiers in Robotics and ai, 6. doi: 10.3389/frobt.2019.00032.

			Jaafari, Abolfazl, Eric K. Zenner, Mahdi Panahi, Himan Shahabi (2019), “Hybrid artificial intelligence models based on a neuro-fuzzy system and metaheuristic optimization algorithms for spatial prediction of wildfire probability”. In Agricultural and Forest Meteorology, 266-267, pp. 198-207. doi: 10.1016/j.agrformet.2018.12.015.

			Jagatic, Tom N., Nathaniel A. Johnson, Markus Jakobsson, Filippo Menczer (2007), “Social phishing”. In Communications of the acm, 50 (10), pp. 94-100.

			James, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani (2013), An Introduction to Statistical Learning. Springer, New York.

			Janoff-Bulman, Ronnie (2007), “Erroneous assumptions: Popular belief in the effectiveness of torture interrogation”. In Peace and Conflict: Journal of Peace Psychology, 13 (4), pp. 429-435.

			Jezard, Adam (2018), “China is now home to the world’s most valuable ai start-up”, 11 aprile. https://www.weforum.org/agenda/2018/04/chart-of-the-day-china-now-has-the-worlds-most-valuable-ai-startup/.

			Jobin, Anna, Marcello Ienca, Effy Vayena (2019), “The global landscape of ai ethics guidelines”. In Nature Machine Intelligence, 1 (9), pp. 389-399.

			Joh, Elizabeth E. (2016), “Policing police robots”. In ucla Law Review Discourse, 64, p. 516.

			Jones, Nicola (2018), “How to stop data centres from gobbling up the world’s electricity”. In Nature, 561 (7722), pp. 163-167.

			Karppi, Tero (2018), “The computer said so: On the ethics, effectiveness, and cultural techniques of predictive policing”. In Social Media + Society, 4 (2), 205630511876829. https://doi.org/10.1177/2056305118768296.

			Karras, Tero, Samuli Laine, Timo Aila (2019), “A style-based generator architecture for generative adversarial networks”. ArXiv preprint, arXiv: 1812.04948.

			Katell, Michael, Meg Young, Dharma Dailey, Bernease Herman, Vivian Guetler, Aaron Tam, Corinne Binz, Daniella Raz, P.M. Krafft (2020), “Toward situated interventions for algorithmic equity: Lessons from the field”. fat* ’20: Conference on Fairness, Accountability, and Transparency, 27 gennaio.

			Kaye, Jane, Edgar A. Whitley, David Lund, Michael Morrison, Harriet Teare, Karen Melham (2015), “Dynamic consent: A patient interface for twenty-first century research networks”. In European Journal of Human Genetics, 23 (2), pp. 141-146. doi: 10.1038/ejhg.2014.71.

			Kerr, Ian R. (2003), “Bots, babes and the californication of commerce”. In Univerity of Ottawa Law & Technology Journal, 1, p. 285.

			Kerr, Ian R., Marcus Bornfreund (2005), “Buddy bots: How Turing’s fast friends are undermining consumer privacy”. In Presence: Teleoperators & Virtual Environments, 14 (6), pp. 647-655.

			King, Gary, Nathaniel Persily (2020), “Unprecedented Facebook urls dataset now available for academic research through Social Science One”. In Social Science One, 13 febbraio.

			King, Thomas C., Nikita Aggarwal, Mariarosaria Taddeo, Luciano Floridi (2019), “Artificial intelligence crime: An interdisciplinary analysis of foreseeable threats and solutions”. In Science and Engineering Ethics. doi: 10.1007/s11948-018-00081-0.

			Kizilcec, René F. (2016), “How much information?”. In Proceedings of the 2016 chi Conference on Human Factors in Computing Systems, pp. 2390-2395. https://doi.org/10.1145/2858036.2858402.

			Klee, Robert (1996), Introduction to the Philosophy of Science: Cutting Nature at Its Seams. Oxford University Press, Oxford.

			Kleinberg, Jon, Sendhil Mullainathan, Manish Raghavan (2016), “Inherent trade-offs in the fair determination of risk scores”. ArXiv preprint, arXiv:1609.05807 [cs, stat].

			Kortylewski, Adam, Bernhard Egger, Andreas Schneider, Thomas Gerig, Andreas Morel-Forster, Thomas Vetter (2019), “Analyzing and reducing the damage of dataset bias to face recognition with synthetic data”. http://openaccess.thecvf.com/content_CVPRW_2019/html/BEFA/Kortylewski_Analyzing_and_Reducing_the_Damage_of_Dataset_Bias_to_Face_CVPRW_2019_paper.html.

			Labati, Ruggero Donida, Angelo Genovese, Enrique Muñoz, Vincenzo Piuri, Fabio Scotti, Gianluca Sforza (2016), “Biometric recognition in automated border control: A survey”. In acm Computing Surveys, 49 (2), pp. 1-39. doi: 10.1145/2933241.

			Lacoste, Alexandre, Alexandra Luccioni, Victor Schmidt, Thomas Dandres (2019), “Quantifying the carbon emissions of machine learning”. ArXiv preprint, arXiv:1910.09700.

			Lagioia, Francesca, Giovanni Sartor (2019), “ai systems under criminal law: A legal analysis and a regulatory perspective”. In Philosophy & Technology, 33, pp. 433-465.

			Lakkaraju, Himabindu, Everaldo Aguiar, Carl Shan, David Miller, Nasir Bhanpuri, Rayid Ghani, Kecia L. Addison (2015), “A machine learning framework to identify students at risk of adverse academic outcomes”. In Proceedings of the 21th acm sigkdd International Conference on Knowledge Discovery and Data Mining, kdd, agosto, pp. 1909-1918.

			Lambrecht, Anja, Catherine Tucker (2019), “Algorithmic bias? An empirical study of apparent gender-based discrimination in the display of stem career ads”. In Management Science, 65 (7), pp. 2966-2981. https://doi.org/10.1287/mnsc.2018.3093.

			Larraondo, Pablo R., Luigi J. Renzullo, Albert I.J.M. Van Dijk, Inaki Inza, Jose A. Lozano (2020), “Optimization of deep learning precipitation models using categorical binary metrics”. In Journal of Advances in Modeling Earth Systems, 12 (5), e2019MS001909. doi: 10.1029/2019MS001909.

			Larson, Brian (2017), “Gender as a variable in natural-language processing: Ethical considerations”. In Proceedings of the First acl Workshop on Ethics in Natural Language Processing, pp. 1-11. Association for Computational Linguistics, Valencia. https://doi.org/10.18653/v1/W17-1601.

			Lee, Kai-Fu, Paul Triolo (2017), “China’s artificial intelligence revolution: Understanding Beijing’s structural advantages – Eurasian Group”, dicembre. https://www.eurasiagroup.net/live-post/ai-in-china-cutting-through-the-hype.

			Lee, Michelle Seng Ah, Luciano Floridi (2020), “Algorithmic fairness in mortgage lending: From absolute conditions to relational trade-offs”. In Minds and Machines. doi: 10.1007/s11023-020-09529-4.

			Lee, Michelle Seng Ah, Luciano Floridi, Alexander Denev (2020), “Innovating with confidence: Embedding ai governance and fairness in a financial services risk management framework”. In Berkeley Technology Law Journal, 34 (2), pp. 1-19.

			Lee, Min Kyung (2018), “Understanding perception of algorithmic decisions: Fairness, trust, and emotion in response to algorithmic management”. In Big Data & Society, 5 (1), 205395171875668. https://doi.org/10.1177/2053951718756684.

			Lee, Min Kyung, Ji Tae Kim, Leah Lizarondo (2017), “A human-centered approach to algorithmic services: Considerations for fair and motivating smart community service management that allocates donations to non-profit organizations”. In Proceedings of the 2017 chi Conference on Human Factors in Computing Systems-chi ’17, pp. 3365-3376. acm Press, Denver, co. https://doi.org/10.1145/3025453.3025884.

			Lee, Richard B., Richard Heywood Daly (1999), The Cambridge Encyclopedia of Hunters and Gatherers. Cambridge University Press, Cambridge.

			Legg, Shane, Marcus Hutter (2007), “A collection of definitions of intelligence”. In Advances in Artificial General Intelligence: Concepts, Architecture, and Algorithms, a cura di Ben Goertzel e Pei Wang. ios Press, Amsterdam, pp. 17-24.

			Lepri, Bruno, Nuria Oliver, Emmanuel Letouzé, Alex Pentland, Patrick Vinck (2018), “Fair, transparent, and accountable algorithmic decision-making processes: The premise, the proposed solutions, and the open challenges”. In Philosophy & Technology, 31 (4), pp. 611-627. https://doi.org/10.1007/s13347-017-0279-x.

			Leslie, David (2019), “Raging robots, hapless humans: The ai dystopia”. In Nature, 574 (7776), pp. 32-34.

			Lessig, Lawrence (1999), Code: And Other Laws of Cyberspace. Basic Books, New York.

			Lewis, Dev (2019), “Social credit case study: City citizen scores in Xiamen and Fuzhou”. In Medium: Berkman Klein Center Collection.

			Liakos, Konstantinos G., Patrizia Busato, Dimitrios Moshou, Simon Pearson, Dionysis Bochtis (2018), “Machine learning in agriculture: A review”. In Sensors, 18 (8), p. 2674.

			Liang, Huiying, Brian Y. Tsui, Hao Ni, Carolina C.S. Valentim, Sally L. Baxter, Guangjian Liu, Wenjia Cai, Daniel S. Kermany, Xin Sun, Jiancong Chen, Liya He, Jie Zhu, Pin Tian, Hua Shao, Lianghong Zheng, Rui Hou, Sierra Hewett, Gen Li, Ping Liang, Xuan Zang, Zhiqi Zhang, Liyan Pan, Huimin Cai, Rujuan Ling, Shuhua Li, Yongwang Cui, Shusheng Tang, Hong Ye, Xiaoyan Huang, Waner He, Wenqing Liang, Qing Zhang, Jianmin Jiang, Wei Yu, Jianqun Gao, Wanxing Ou, Yingmin Deng, Qiaozhen Hou, Bei Wang, Cuichan Yao, Yan Liang, Shu Zhang, Yaou Duan, Runze Zhang, Sarah Gibson, Charlotte L. Zhang, Oulan Li, Edward D. Zhang, Gabriel Karin, Nathan Nguyen, Xiaokang Wu, Cindy Wen, Jie Xu, Wenqin Xu, Bochu Wang, Winston Wang, Jing Li, Bianca Pizzato, Caroline Bao, Daoman Xiang, Wanting He, Suiqin He, Yugui Zhou, Weldon Haw, Michael Goldbaum, Adriana Tremoulet, Chun-Nan Hsu, Hannah Carter, Long Zhu, Kang Zhang, Huimin Xia (2019), “Evaluation and accurate diagnoses of pediatric diseases using artificial intelligence”. In Nature Medicine, 25, pp. 433-438.

			Lin, Tom C.W. (2016), “The new market manipulation”. In Emory Law Journal, 66, p. 1253.

			Lipworth, Wendy, Paul H. Mason, Ian Kerridge, John P.A. Ioannidis (2017), “Ethics and epistemology in big data research”. In Journal of Bioethical Inquiry, 14 (4), pp. 489-500. https://doi.org/10.1007/s11673-017-9771-3.

			Lu, Haonan, Mubarik Arshad, Andrew Thornton, Giacomo Avesani, Paula Cunnea, Ed Curry, Fahdi Kanavati, Jack Liang, Katherine Nixon, Sophie T. Williams, Mona Ali Hassan, David D.L. Bowtell, Hani Gabra, Christina Fotopoulou, Andrea Rockall, Eric O. Aboagye (2019), “A mathematical-descriptor of tumor-mesoscopic-structure from computed-tomography images annotates prognostic- and molecular-phenotypes of epithelial ovarian cancer”. In Nature Communications, 10 (1), p. 764. doi: 10.1038/s41467-019-08718-9.

			Lu, Hongfang, Xin Ma, Kun Huang, Mohammadamin Azimi (2020), “Carbon trading volume and price forecasting in China using multiple machine learning models”. In Journal of Cleaner Production, 249, p. 119386. doi: 10.1016/j.jclepro.2019.119386.

			Luccioni, Alexandra, Victor Schmidt, Vahe Vardanyan, Yoshua Bengio (2021), “Using artificial intelligence to visualize the impacts of climate change”. In ieee Computer Graphics and Applications, 41 (1), pp. 8-14.

			Luhmann, Niklas, John Bednarz (1995), Social Systems, Writing Science. Stanford University Press, Stanford, ca.

			Lum, Kristian, William Isaac (2016), “To predict and serve?”. In Significance, 13 (5), pp. 14-19. doi: 10.1111/j.1740-9713.2016.00960.x.

			Lynskey, Orla (2015), The Foundations of eu Data Protection Law, Oxford Studies in European Law. Oxford University Press, Oxford-New York.

			Mack, Eric (2015), “Bill Gates says you should worry about artificial intelligence”. In Forbes, 28 gennaio.

			Magalhães, João Carlos (2018), “Do algorithms shape character? Considering algorithmic ethical subjectivation”. In Social Media + Society, 4 (2), p. 205630511876830. https://doi.org/10.1177/2056305118768301.

			Malhotra, Charru, Vinod Kotwal, Surabhi Dalal (2018), “Ethical framework for machine learning”. In 2018 itu Kaleidoscope: machine learning for a 5G Future (itu k), pp. 1-8. ieee, Santa Fe. https://doi.org/10.23919/ITU-WT.2018.8597767.

			Malmodin, Jens, Dag Lundén (2018), “The energy and carbon footprint of the global ict and e&m sectors 2010-2015”. In Sustainability, 10 (9), p. 3027. doi: 10.3390/su10093027.

			Manheim, David, Scott Garrabrant (2018), “Categorizing variants of Goodhart’s Law”. ArXiv preprint, arXiv:1803.04585.

			Mardani, Abbas, Huchang Liao, Mehrbakhsh Nilashi, Melfi Alrasheedi, Fausto Cavallaro (2020), “A multi-stage method to predict carbon dioxide emissions using dimensionality reduction, clustering, and machine learning techniques”. In Journal of Cleaner Production, 275, p. 122942. doi: 10.1016/j.jclepro.2020.122942.

			Marrero, Tony (2016), “Record pacific cocaine haul brings hundreds of cases to Tampa court”. In Tampa Bay Times, 10 settembre.

			Martin, Kirsten (2019), “Ethical implications and accountability of algorithms”. In Journal of Business Ethics, 160 (4), pp. 835-850. https://doi.org/10.1007/s10551-018-3921-3.

			Martínez-Miranda, Enrique, Peter McBurney, Matthew J.W. Howard (2016), “Learning unfair trading: A market manipulation analysis from the reinforcement learning perspective”. ieee Conference on Evolving and Adaptive Intelligent Systems (eais).

			Martı́nez-Miranda, Juan, Arantza Aldea (2005), “Emotions in human and artificial intelligence”. In Computers in Human Behavior, 21 (2), pp. 323-341. doi: 10.1016/j.chb.2004.02.010.

			Masanet, Eric, Arman Shehabi, Nuoa Lei, Sarah Smith, Jonathan Koomey (2020), “Recalibrating global data center energy-use estimates”. In Science, 367 (6481), pp. 984-986. doi: 10.1126/science.aba3758.

			Mayson, Sandra G. (2019), “Bias in, bias out”. In Yale Law Journal, n. 128. https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3257004.

			Mazzini, Gabriele (2019), “A system of governance for artificial intelligence through the lens of emerging intersections between ai and eu law”. In Digital Revolution – New Challenges for Law, a cura di A. De Franceschi, R. Schulze, M. Graziadei, O. Pollicino, F. Riente, S. Sica e P. Sirena. In ssrn Electronic Journal. https://ssrn.com/abstract=3369266.

			McAllister, Amanda (2016), “Stranger than science fiction: The rise of ai interrogation in the dawn of autonomous robots and the need for an additional protocol to the un convention against torture”. In Minnesota Law Review, 101, pp. 2527-2573.

			McBurney, Peter, Matthew J. Howard (2015), “Learning unfair trading: A market manipulation analysis from the reinforcement learning perspective”. ArXiv preprint, arXiv:1511.00740.

			McCarthy, John (1997), “Review of Kasparov vs. Deep Blue by Monty Newborn”. In Science, 6 giugno.

			McCarthy, John, Marvin L. Minsky, Nathaniel Rochester, Claude E. Shannon (2006), “A proposal for the Dartmouth summer research project on artificial intelligence, august 31, 1955”. In ai magazine, 27 (4), p. 12.

			McFarland, Matt (2014), “Elon Musk: ‘With artificial intelligence we are summoning the demon’”. In The Washington Post, 24 ottobre.

			McFarlane, Daniel (1999), “Interruption of people in human-computer interaction: A general unifying definition of human interruption and taxonomy”. In Interact, pp. 295-303.

			McFarlane, Daniel, Kara Latorella (2002), “The scope and importance of human interruption in human-computer interaction design”. In Human-computer Interaction, 17, pp. 1-61. doi: 10.1207/S15327051HCI1701_1.

			McKelvey, Fenwick, Elizabeth Dubois (2017), “Computational propaganda in Canada: The use of political bots”. In Working Paper 2017.6. Project on Computational Propaganda, Oxford. https://blogs.oii.ox.ac.uk/politicalbots/wp-content/uploads/sites/89/2017/06/Comprop-Canada.pdf.

			McLuhan, Marshall, Barrington Nevitt (1972), Take Today; The Executive as Dropout. Harcourt Brace Jovanovich, New York.

			Menad, Nait Amar, Abdolhossein Hemmati-Sarapardeh, Amir Varamesh, Shahaboddin Shamshirband (2019), “Predicting solubility of co2 in brine by advanced machine learning systems: Application to carbon capture and sequestration”. In Journal of co2 Utilization, 33, pp. 83-95.

			Mendelsohn, Robert, Ariel Dinar, Larry Williams (2006), “The distributional impact of climate change on rich and poor countries”. In Environment and Development Economics, 11 (2), pp. 159-178.

			Meneguzzi, Felipe Rech, Michael Luck (2009), “Norm-based behaviour modification in bdi agents”. In International Joint Conference aamas, vol. 1, pp. 177-184.

			Microsoft (2018), The Carbon Benefits of Cloud Computing: A Study on the Microsoft Cloud. https://www.microsoft.com/en-us/download/details.aspx?id=56950.

			Milano, Silvia, Mariarosaria Taddeo, Luciano Floridi (2019), “Ethical aspects of multi-stakeholder recommendation systems”. In ssrn Electronic Journal. http://dx.doi.org/10.2139/ssrn.3493202.

			Milano, Silvia, Mariarosaria Taddeo, Luciano Floridi (2020), “Recommender systems and their ethical challenges”. In ai & Society, 35 (4), pp. 957-967.

			Milano, Silvia, Mariarosaria Taddeo, Luciano Floridi (2021), “Ethical aspects of multi-stakeholder recommendation systems”. In The Information Society, 37 (1), pp. 35-45.

			Mill, John Stuart (1861), Considerations on Representative Government. 2a ed. Parker, Son, and Bourn, London.

			Mittelstadt, Brent Daniel, Patrick Allo, Mariarosaria Taddeo, Sandra Wachter, Luciano Floridi (2016), “The ethics of algorithms: Mapping the debate”. In Big Data & Society, 3 (2). doi: 10.1177/2053951716679679.

			Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski (2015), “Human-level control through deep reinforcement learning”. In Nature, 518 (7540), pp. 529-533.

			Mohanty, Suchitra, Rahul Bhatia (2017), “Indian court’s privacy ruling is blow to government”. In Reuters, 25 agosto. Accesso 2019-04-13 08:41:27.

			Mojsilovic, Alexandra (2018), Introducing ai Explainability 360. https://www.ibm.com/blogs/research/2019/08/ai-explainability-360/.

			Mökander, Jakob, Luciano Floridi (2021), “Ethics-based auditing to develop trustworthy ai”. In Minds and Machines, 10.

			Mökander, Jakob, Jessica Morley, Mariarosaria Taddeo, Luciano Floridi (2021), “Ethics-based auditing of automated decision-making systems: Nature, scope, and limitations”. In Science and Engineering Ethics, 27 (44).

			Möller, Judith, Damian Trilling, Natali Helberger, Bram van Es (2018), “Do not blame it on the algorithm: An empirical assessment of multiple recommender systems and their impact on content diversity”. In Information Communication & Society, 21 (7), pp. 959-977. https://doi.org/10.1080/1369118X.2018.1444076.

			Moor, James H. (1985), “What is computer ethics?*”. In Metaphilosophy, 16 (4), pp. 266-275. doi: 10.1111/j.1467-9973.1985.tb00173.x.

			Moore, Jared (2019), “ai for not bad”. In Frontiers in Big Data, 2, p. 32.

			Morley, Jessica, Josh Cowls, Mariarosaria Taddeo, Luciano Floridi (2020), “Ethical guidelines for covid-19 tracing apps”. In Nature, 582, pp. 29-31.

			Morley, Jessica, Luciano Floridi, Libby Kinsey, Anat Elhalal (2020), “From what to how: An initial review of publicly available ai ethics tools, methods and research to translate principles into practices”. In Science and Engineering Ethics, 26 (4), pp. 2141-2168. doi: 10.1007/s11948-019-00165-5.

			Morley, Jessica, Caio C.V. Machado, Christopher Burr, Josh Cowls, Indra Joshi, Mariarosaria Taddeo, Luciano Floridi (2020), “The ethics of ai in health care: A mapping review”. In Social Science & Medicine, 260, p. 113172. doi: 10.1016/j.socscimed.2020.113172.

			Morley, Jessica, Caroline Morton, Kassandra Karpathakis, Mariarosaria Taddeo, Luciano Floridi (2021), “Towards a framework for evaluating the safety, acceptability and efficacy of ai systems for health: An initial synthesis”. ArXiv preprint, arXiv:2104.06910.

			Murgia, Madhumita (2018), “DeepMind’s move to transfer health unit to Google stirs data fears”. In Financial Times, 13 novembre.

			Narciso, Diogo A.C., F.G. Martins (2020), “Application of machine learning tools for energy efficiency in industry: A review”. In Energy Reports, 6, pp. 1181-1199.

			Neff, Gina, Peter Nagy (2016a), “Automation, algorithms, and politics – Talking to Bots: Symbiotic agency and the case of Tay”. In International Journal of Communication, 10, p. 17.

			Neff, Gina, Peter Nagy (2016b), “Talking to bots: Symbiotic agency and the case of Tay”. In International Journal of Communication, 10, pp. 4915-4931.

			Neufeld, Eric, Sonje Finnestad (2020), “In defense of the Turing test”. In ai & Society, pp. 1-9.

			Nield, Thomas (2019), “Is deep learning already hitting its limitations? And is another ai winter coming?”. In Towards Data Science, 5 gennaio. https://towardsdatascience.com/is-deep-learning-already-hitting-its-limitations-c81826082ac3.

			Nietzsche, Friedrich Wilhelm (1889), Twilight of the Idols, or, How to Philosophize with a Hammer. Tr. ing. Oxford University Press, Oxford 2008.

			Nijhawan, Lokesh P., Manthan Janodia, Muddu Krishna, Kishore Bhat, Laxminarayana Bairy, Nayanabhirama Udupa, Prashant Musmade (2013), “Informed consent: Issues and challenges”. In Journal of Advanced Pharmaceutical Technology & Research, vol. 4, 3, pp. 134-140.

			Nissenbaum, Helen (2009), Privacy in Context: Technology, Policy, and the Integrity of Social Life. Stanford University Press, Stanford, ca.

			Nissenbaum, Helen (2011), “A contextual approach to privacy online”. In Daedalus, 140 (4), pp. 32-48.

			Noble, Safiya Umoja (2018), Algorithms of Oppression: How Search Engines Reinforce Racism. New York University Press, New York.

			Nordling, Linda (2018), “Europe’s biggest research fund cracks down on ‘ethics dumping’”. In Nature, 559 (7712), p. 17.

			Nunamaker, Jay F., Douglas C. Derrick, Aaron C. Elkins, Judee K. Burgoon, Mark W. Patton (2011), “Embodied conversational agent-based kiosk for automated interviewing”. In Journal of Management Information Systems, 28 (1), pp. 17-48.

			Obermeyer, Ziad, Brian Powers, Christine Vogeli, Sendhil Mullainathan (2019), “Dissecting racial bias in an algorithm used to manage the health of populations”. In Science, 366 (6464), pp. 447-453. doi: 10.1126/science.aax2342.

			Ochigame, Rodrigo (2019), The Invention of “Ethical ai”. https://theintercept.com/2019/12/20/mit-ethical-ai-artificial-intelligence/.

			ocse (2019), “Forty-two countries adopt new oecd principles on artificial intelligence”. https://www.oecd.org/science/forty-two-countries-adopt-new-oecd-principles-on-artificial-intelligence.htm.

			Olhede, S.C., P.J. Wolfe (2018), “The growing ubiquity of algorithms in society: Implications, impacts and innovations”. In Philosophical Transactions of the Royal Society A. Mathematical, Physical and Engineering Sciences, 376 (2128), 20170364. https://doi.org/10.1098/rsta.2017.0364.

			Olteanu, Alexandra, Carlos Castillo, Fernando Diaz, Emre Kiciman (2016), “Social data: Biases, methodological pitfalls, and ethical boundaries”. In ssrn Electronic Journal. https://doi.org/10.2139/ssrn.2886526.

			OpenAI (2019), “Microsoft invests in and partners with Openai to support us building beneficial agi”. In Openai Official Blog.

			Oswald, Marion (2018), “Algorithm-assisted decision-making in the public sector: Framing the issues using administrative law rules governing discretionary power”. In Philosophical Transactions of the Royal Society A. Mathematical, Physical and Engineering Sciences, 376 (2128), 20170359. https://doi.org/10.1098/rsta.2017.0359.

			Pagallo, Ugo (2011), “Killers, fridges, and slaves: A legal journey in robotics”. In ai & Society, 26 (4), pp. 347-354.

			Pagallo, Ugo (2015), “Good onlife governance: On law, spontaneous orders, and design”. In The Onlife Manifesto. Springer, Cham, pp. 161-177.

			Pagallo, Ugo (2017), “From automation to autonomous systems: A legal phenomenology with problems of accountability”. 26th International Joint Conference on Artificial Intelligence (ijcai).

			Paraschakis, Dimitris (2017), “Towards an ethical recommendation framework”. 11th International Conference on Research Challenges in Information Science (rcis), maggio.

			Paraschakis, Dimitris (2018), Algorithmic and Ethical Aspects of Recommender Systems in E-commerce. Malmö Universitet, Malmö.

			Partnership on ai (2018), “Tenets of the Partnership on ai”. https://www.partnershiponai.org/tenets/.

			Pedreschi, Dino, Salvatore Ruggieri, Franco Turini (2008), “Discrimination-aware data mining”. In Proceedings of the 14th acm sigkdd International Conference on Knowledge Discovery and Data Mining, 24 agosto.

			Perera, Lokukaluge P., Brage Mo (2016), “Machine intelligence for energy efficient ships: A big data solution”. In Guedes Soares, T.A. Santos (a cura di), Maritime Engineering and Technology iii. crc Press, London, pp. 143-150.

			Perrault, Raymond, Yoav Shoham, Erik Brynjolfsson, Clark Jack, John Etchmendy, Barbara Grosz, Terah Lyons, James Manyika, Saurabh Mishra, Juan Carlos Niebles (2019), “The ai Index 2019 Annual Report”. ai Index Steering Committee, Human-Centered ai Institute, Stanford University, Stanford, ca, dicembre.

			Perumalla, Kalyan S. (2014), Introduction to Reversible Computing. Chapman & Hall/crc Computational Science Series. crc Press, Boca Raton.

			Pirolli, Peter (2007), Information Foraging Theory: Adaptive Interaction with Information. Oxford University Press, Oxford.

			Pirolli, Peter, Stuart Card (1995), “Information foraging in information access environments”. In Proceedings of the sigchi Conference on Human Factors in Computing Systems, pp. 51-58.

			Pirolli, Peter, Stuart Card (1999), “Information foraging”. In Psychological Review, 106 (4), p. 643.

			Pogue, David (2012), “Use it better: The worst tech predictions of all time – Plus, flawed forecasts about Apple’s certain demise and the poor prognostication skills of Bill Gates”, 18 gennaio. http://www.scientificamerican.com/article/pogue-all-time-worst-tech-predictions/.

			Pontificia Accademia per la Vita (2020), “Rome call for an ai ethics”. https://www.romecall.org/.

			Popkin, Richard H. (1966), The Philosophy of the Sixteenth and Seventeenth Centuries. Free Press-Collier-Macmillan, New York-London.

			Prasad, Mahendra (2018), “Social choice and the value alignment problem”. In Artificial Intelligence Safety and Security. Chapman & Hall/crc, pp. 291-314.

			Prates, Marcelo O.R., Pedro H. Avelar, Luís C. Lamb (2019), “Assessing gender bias in machine translation: A case study with Google translate”. In Neural Computing and Applications, 32, pp. 6363-6381. https://doi.org/10.1007/s00521-019-04144-6.

			Preston, John, Mark Bishop (2002), Views Into the Chinese Room: New Essays on Searle and Artificial Intelligence. Clarendon Press, Oxford.

			Price, W. Nicholson, I. Glenn Cohen (2019), “Privacy in the age of medical big data”. In Nature Medicine, 25 (1), p. 37. doi: 10.1038/s41591-018-0272-7.

			Puaschunder, Julia M. (2020), “The potential for artificial intelligence in healthcare”. In ssrn Electronic Journal. http://dx.doi.org/10.2139/ssrn.3525037.

			Rachels, James (1975), “Why privacy is important”. In Philosophy and Public Affairs, 4 (4), pp. 323-333.

			Rahwan, Iyad (2018), “Society-in-the-loop: Programming the algorithmic social contract”. In Ethics and InformationTechnology, 20 (1), pp. 5-14. https://doi.org/10.1007/s10676-017-9430-8.

			Raji, Inioluwa Deborah, Andrew Smart, Rebecca N. White, Margaret Mitchell, Timnit Gebru, Ben Hutchinson, Jamila Smith-Loud, Daniel Theron, Parker Barnes (2020), “Closing the ai accountability gap: Defining an end-to-end framework for internal algorithmic auditing”. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, pp. 33-44.

			Ramchurn, Sarvapali D., Perukrishnen Vytelingum, Alex Rogers, Nicholas R. Jennings (2012), “Putting the ‘smarts’ into the smart grid: A grand challenge for artificial intelligence”. In Communications of the acm, 55 (4), pp. 86-97.

			Ras, Gabrielle, Marcel van Gerven, Pim Haselager (2018), “Explanation methods in deep learning: Users, values, concerns and challenges”. ArXiv preprint, arXiv:1803.07517.

			Rasp, Stephan, Michael S. Pritchard, Pierre Gentine (2018), “Deep learning to represent subgrid processes in climate models”. In Proceedings of the National Academy of Sciences, 115 (39), pp. 9684-9689. doi: 10.1073/pnas.1810286115.

			Ratkiewicz, Jacob, Michael Conover, Mark Meiss, Bruno Gonçalves, Snehal Patil, Alessandro Flammini, Filippo Menczer (2011), “Truthy: Mapping the spread of astroturf in microblog streams”. In Proceedings of the 20th International Conference Companion on World Wide Web.

			Rawlinson, Kevin (2015), “Microsoft’s Bill Gates insists ai is a threat”. In bbc News, 29 gennaio.

			Rawls, John (1955), “Two concepts of rules”. In The Philosophical Review, 64 (1), pp. 3-32.

			Reddy, Elizabeth, Baki Cakici, Andrea Ballestero (2019), “Beyond mystery: Putting algorithmic accountability in context”. In Big Data & Society, 6 (1), 205395171982685. https://doi.org/10.1177/2053951719826856.

			Reed, Chris (2018), “How should we regulate artificial intelligence?”. In Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 376 (2128), 20170360.

			Rehm, Matthias (2008), “‘She is just stupid’ – Analyzing user-agent interactions in emotional game situations”. In Interacting with Computers, 20 (3), pp. 311-325.

			Reisman, Dillon, Jason Schultz, Kate Crawford, Meredith Whittaker (2018), “Algorithmic impact assessments: A practical framework for public agency accountability”. ai Now Institute. https://ainowinstitute.org/aiareport2018.pdf.

			Richardson, Rashida, Jason Schultz, Kate Crawford (2019), “Dirty data, bad predictions: How civil rights violations impact police data, predictive policing systems, and justice”. In ssrn Electronic Journal. https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3333423.

			Ridwan, Wanie M., Michelle Sapitang, Awatif Aziz, Khairul Faizal Kushiar, Ali Najah Ahmed, Ahmed El-Shafie (2020), “Rainfall forecasting model using machine learning methods: Case study Terengganu, Malaysia”. In Ain Shams Engineering Journal. doi: 10.1016/j.asej.2020.09.011.

			Robbins, Scott (2019), “A misdirected principle with a catch: Explicability for ai”. In Minds and Machines, 29 (4), pp. 495-514. https://doi.org/10.1007/s11023-019-09509-3.

			Roberts, Huw, Josh Cowls, Emmie Hine, Jessica Morley, Mariarosaria Taddeo, Vincent Wang, Luciano Floridi (2021), “China’s artificial intelligence strategy: Lessons from the European Union’s ‘ethics-first’ approach”. In ssrn Electronic Journal. https://ssrn.com/abstract=3811034.

			Roberts, Huw, Josh Cowls, Jessica Morley, Mariarosaria Taddeo, Vincent Wang, Luciano Floridi (2021), “The Chinese approach to artificial intelligence: An analysis of policy, ethics, and regulation”. In ai & Society, 36 (1), pp. 59-77. doi: 10.1007/s00146-020-00992-2.

			Robinson, Caleb, Bistra Dilkina (2018), “A machine learning approach to modeling human migration”, 20 giugno. In Proceedings of the 1st acm sigcas Conference on Computing and Sustainable Societies, pp. 1-8.

			Rolnick, David, Priya L. Donti, Lynn H. Kaack, Kelly Kochanski, Alexandre Lacoste, Kris Sankaran, Andrew Slavin Ross, Nikola Milojevic-Dupont, Natasha Jaques, Anna Waldman-Brown (2019), “Tackling climate change with machine learning”. ArXiv preprint, arXiv:1906.05433.

			Rosenblueth, Arturo, Norbert Wiener (1945), “The role of models in science”. In Philosophy of Science, 12 (4), pp. 316-321.

			Ross, Casey, Ike Swetlitz (2017), “ibm pitched Watson as a revolution in cancer care. It’s nowhere close”. Ultima modifica 2017-09-05T20:30:44+00:00.

			Rössler, Beate (2015), The Value of Privacy. https://philpapers.org/rec/ROSTVO-9.

			Rubel, Alan, Clinton Castro, Adam Pham (2019), “Agency laundering and information technologies”. In Ethical Theory and Moral Practice, 22 (4), pp. 1017-1041. https://doi.org/10.1007/s10677-019-10030-w.

			Russell, Bertrand (1952), “Is there a God?”. In The Collected Papers of Bertrand Russell, vol. 11: Last Philosophical Testament, 1943-68. Routledge, London, pp. 542-548.

			Russell, S. (2019), Human Compatible: Artificial Intelligence and the Problem of Control. Penguin, London.

			Russell, Stuart J., Peter Norvig (2016), Artificial Intelligence: A Modern Approach. 3a ed. Pearson, Harlow (Intelligenza artificiale. Un approccio moderno. Tr. it. Pearson-Italia, Milano-Torino 2021).

			Samuel, Arthur L. (1960), “Some moral and technical consequences of automation – A refutation”. In Science, 132 (3429), pp. 741-742.

			Sandvig, Christian, Kevin Hamilton, Karrie Karahalios, Cedric Langbort (2016), “When the algorithm itself is a racist: Diagnosing ethical harm in the basic components of software”. In Internationl Journal of Communication, 10, pp. 4972-4990.

			Saxena, Nripsuta, Karen Huang, Evan DeFilippis, Goran Radanovic, David Parkes, Yang Liu (2019), “How do fairness definitions fare? Examining public attitudes towards algorithmic definitions of fairness”. ArXiv preprint, arXiv:1811.03654.

			Sayed-Mouchaweh, Moamar (2020), Artificial Intelligence Techniques for a Scalable Energy Transition: Advanced Methods, Digital Technologies, Decision Support Tools, and Applications. Springer, Berlin.

			Schott, Ben (2010), “Bluewashing”. In The New York Times, 4 febbraio.

			Schuchmann, Sebastian (2019), “Probability of an approaching ai winter”. In Towards Data Science, 17 agosto. https://towardsdatascience.com/probability-of-an-approaching-ai-winter-c2d818fb338a.

			Schwartz, Roy, Jesse Dodge, Noah A. Smith, Oren Etzioni (2020), “Green ai”. In Communications of the acm, 63 (12), pp. 54-63.

			Searle, John R. (2014), “What your computer can’t know”. In The New York Review of Books, 9 ottobre. http://www.nybooks.com/articles/archives/2014/oct/09/what-your-computer-cant-know/.

			Searle, John R. (2018), “Constitutive rules”. In Argumenta, 4 (1), pp. 51-54.

			Selbst, Andrew D., Dana Boyd, Sorelle A. Friedler, Suresh Venkatasubramanian, Janet Vertesi (2019), “Fairness and abstraction in sociotechnical systems”. In Proceedings of the Conference on Fairness, Accountability, and Transparency-fat* ’19. acm Press, Atlanta, ga, pp. 59-68. https://doi.org/10.1145/3287560.3287598.

			Seymour, John, Philip Tully (2016), “Weaponizing data science for social engineering: Automated e2e spear phishing on Twitter”. In Black Hat usa, 37, pp. 1-39.

			Shaffer, Gregory C., Mark A. Pollack (2009), “Hard vs. soft law: Alternatives, complements, and antagonists in international governance”. In Minnesota Law Review, 94, p. 706.

			Shah, Hetan (2018), “Algorithmic accountability”. In Philosophical Transactions of the Royal Society A. Mathematical, Physical and Engineering Sciences, 376 (2128), 20170362. https://doi.org/10.1098/rsta.2017.0362.

			Sharkey, Noel, Marc Goodman, Nick Ross (2010), “The coming robot crime wave”. In Computer, 43 (8), pp. 115-116.

			Shehabi, Arman, Sarah J. Smith, Eric Masanet, Jonathan Koomey (2018), “Data center growth in the United States: Decoupling the demand for services from electricity use”. In Environmental Research Letters, 13 (12), 124030. doi: 10.1088/1748-9326/aaec9c.

			Shin, Donghee, Yong Jin Park (2019), “Role of fairness, accountability, and transparency in algorithmic affordance”. In Computers in Human Behavior, 98 (settembre), pp. 277-284. https://doi.org/10.1016/j.chb.2019.04.019.

			Shortliffe, Edward H., Bruce G. Buchanan (1975), “A model of inexact reasoning in medicine”. In Mathematical Biosciences, 23 (3), pp. 351-379. doi: 10.1016/0025-5564(75)90047-4.

			Shrestha, Manish, Sujal Manandhar, Sangam Shrestha (2020), “Forecasting water demand under climate change using artificial neural network: A case study of Kathmandu Valley, Nepal”. In Water Supply, 20 (5), pp. 1823-1833. doi: 10.2166/ws.2020.090.

			Sibai, Fadi N. (2020), “ai crimes: A classification”. 2020 International Conference on Cyber Security and Protection of Digital Services (Cyber Security).

			Silver, David, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, Demis Hassabis (2016), “Mastering the game of Go with deep neural networks and tree search”. In Nature, 529 (7587), pp. 484-489. doi: 10.1038/nature16961. http://www.nature.com/nature/journal/v529/n7587/abs/nature16961.html#supplementary-information.

			Silver, David, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, Demis Hassabis (2018), “A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play”. In Science, 362 (6419), pp. 1140-1144.

			Simon, Herbert A. (1996), The Sciences of the Artificial. 3a ed. mit Press, Cambridge, ma-London.

			Sipser, Michael (2012), Introduction to the Theory of Computation. 3a ed. Cengage Learning, Boston, ma (Introduzione alla teoria della computazione. Tr. it. Maggioli, Santarcangelo di Romagna 2016).

			Sloan, Robert H., Richard Warner (2018), “When is an algorithm transparent? Predictive analytics, privacy, and public policy”. ieee Security & Privacy, 16 (3), pp. 18-25. doi: 10.1109/MSP.2018.2701166.

			Solis, Gary D. (2016), The Law of Armed Conflict: International Humanitarian Law in War. Cambridge University Press, Cambridge.

			Solove, Daniel J. (2008), Understanding Privacy. Harvard University Press, Cambridge, ma-London.

			Sønderby, Casper Kaae, Lasse Espeholt, Jonathan Heek, Mostafa Dehghani, Avital Oliver, Tim Salimans, Shreya Agrawal, Jason Hickey, Nal Kalchbrenner (2020), “MetNet: A neural weather model for precipitation forecasting”. ArXiv preprint, arXiv:2003.12140 [physics, stat].

			Spatt, Chester (2014), “Security market manipulation”. In Annual Review of Financial Economics, 6 (1), pp. 405-418.

			Stilgoe, Jack (2018), “Machine learning, social learning and the governance of self-driving cars”. In Social Studies of Science, 48 (1), pp. 25-56. https://doi.org/10.1177/0306312717741687

			Strathern, Marilyn (1997), “‘Improving ratings’: Audit in the British university system”. In European Review, 5 (3), pp. 305-321.

			Strickland, Eliza (2019), “How ibm Watson overpromised and underdelivered on ai health care”. In ieee Spectrum. Ultima modifica 4/2/2019.

			Strubell, Emma, Ananya Ganesh, Andrew McCallum (2019), “Energy and policy considerations for deep learning in nlp”. ArXiv preprint, arXiv:1906.02243.

			Swearingen, Kirsten, Rashmi Sinha (2002), “Interaction design for recommender systems”. In Designing Interactive Systems, acm, pp. 1-10.

			Szegedy, Christian, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, Rob Fergus (2014), “Intriguing properties of neural networks”. ArXiv preprint, arXiv:1312.6199 [Cs].

			Tabuchi, Hiroko, David Gelles (2019), “Doomed Boeing jets lacked 2 safety features that company sold only as extras”. In The New York Times, 2019-04-05T00:55:24.836Z, Business. Accesso 2019-04-13 09:27:43.

			Taddeo, Mariarosaria (2009), “Defining trust and e-trust”. In International Journal of Technology and Human Interaction, 5 (2), pp. 23-35. doi: 10.4018/jthi.2009040102.

			Taddeo, Mariarosaria (2010), “Modelling trust in artificial agents, a first step toward the analysis of e-trust”. In Minds and Machines, 20 (2), pp. 243-257. doi: 10.1007/s11023-010-9201-3.

			Taddeo, Mariarosaria (2014), “The struggle between liberties and authorities in the information age”. In Science and Engineering Ethics, pp. 1-14. doi: 10.1007/s11948-014-9586-0.

			Taddeo, Mariarosaria (2017a), “Deterrence by norms to stop interstate cyber attacks”. In Minds and Machines, 27 (3), pp. 387-392. doi: 10.1007/s11023-017-9446-1.

			Taddeo, Mariarosaria (2017b), “The limits of deterrence theory in cyberspace”. In Philosophy & Technology. doi: 10.1007/s13347-017-0290-2.

			Taddeo, Mariarosaria (2017c), “Trusting digital technologies correctly”. In Minds and Machines, 27 (4), pp. 565-568. doi: 10.1007/s11023-017-9450-5.

			Taddeo, Mariarosaria, Luciano Floridi (2005), “Solving the symbol grounding problem: A critical review of fifteen years of research”. In Journal of Experimental & Theoretical Artificial Intelligence, 17 (4), pp. 419-445. doi: 10.1080/09528130500284053.

			Taddeo, Mariarosaria, Luciano Floridi (2007), “A praxical solution of the symbol grounding problem”. In Minds and Machines, 17 (4), pp. 369-389. doi: 10.1007/s11023-007-9081-3.

			Taddeo, Mariarosaria, Luciano Floridi (2011), “The case for e-trust”. In Ethics and Information Technology, 13 (1), pp. 1-3. doi: 10.1007/s10676-010-9263-1.

			Taddeo, Mariarosaria, Luciano Floridi (2015), “The debate on the moral responsibilities of online service providers”. In Science and Engineering Ethics, pp. 1-29. doi: 10.1007/s11948-015-9734-1.

			Taddeo, Mariarosaria, Luciano Floridi (2018a), “How ai can be a force for good”. In Science, 361 (6404), pp. 751-752.

			Taddeo, Mariarosaria, Luciano Floridi (2018b), “Regulate artificial intelligence to avert cyber arms race”. In Nature, 556 (7701), pp. 296-298. doi: 10.1038/d41586-018-04602-6.

			Taddeo, Mariarosaria, Tom McCutcheon, Luciano Floridi (2019), “Trusting artificial intelligence in cybersecurity is a double-edged sword”. In Nature Machine Intelligence, 1 (12), pp. 557-560. doi: 10.1038/s42256-019-0109-1.

			Tan, Zhi Ming, Nikita Aggarwal, Josh Cowls, Jessica Morley, Mariarosaria Taddeo, Luciano Floridi (2021), “The ethical debate about the gig economy: A review and critical analysis”. In Technology in Society, 65, 101594.

			Taylor, Linnet, Dennis Broeders (2015), “In the name of development: Power, profit and the datafication of the global South”. In Geoforum, 64, pp. 229-237.

			Taylor, Linnet, Luciano Floridi, Bart van der Sloot (2016), Group Privacy: New Challenges of Data Technologies. Springer, Berlin.

			Taylor, Linnet, Ralph Schroeder (2015), “Is bigger better? The emergence of big data as a tool for international development policy”. In GeoJournal, 80 (4), pp. 503-518.

			The Economist (2014a), “Waiting on hold – Ebola and big data”, 27 ottobre.

			The Economist (2014b), “Turkzilla!”, 27 novembre. http://www.economist.com/blogs/graphicdetail/2014/11/daily-chart-16.

			The Economist (2016), “The end of Moore’s law”, 19 aprile.

			Thelisson, Eva, Kirtan Padh, L. Elisa Celis (2017), “Regulatory mechanisms and algorithms towards trust in ai/ml”. In ijcai Workshop on Explainable ai (xai), pp. 53-57.

			Thilakarathna, P.S.M., S. Seo, K.S. Kristombu Baduge, H. Lee, P. Mendis, G. Foliente (2020), “Embodied carbon analysis and benchmarking emissions of high and ultra-high strength concrete using machine learning algorithms”. In Journal of Cleaner Production, 262, p. 121281. doi: 10.1016/j.jclepro.2020.121281.

			Thompson, Neil C., Kristjan Greenewald, Keeheon Lee, Gabriel F. Manso (2020), “The computational limits of deep learning”. ArXiv preprint, arXiv:2007.05558.

			Tickle, A.B., R. Andrews, M. Golea, J. Diederich (1998), “The truth will come to light: Directions and challenges in extracting the knowledge embedded within trained artificial neural networks”. In ieee Transactions on Neural Network, 9 (6), pp. 1057-1068. https://doi.org/10.1109/72.728352.

			Toffler, Alvin (1980), The Third Wave. Collins, London (La terza ondata, tr. it. Sperling & Kupfer, Milano 1987).

			Tonti, Gianluca, Jeffrey M. Bradshaw, Renia Jeffers, Rebecca Montanari, Niranjan Suri, Andrzej Uszok (2003), “Semantic Web languages for policy representation and reasoning: A comparison of KAoS, Rei, and Ponder”. International Semantic Web Conference.

			Torpey, John (2000), The Invention of the Passport: Surveillance, Citizenship and the State. Cambridge University Press, Cambridge.

			Tsamados, Andreas, Nikita Aggarwal, Josh Cowls, Jessica Morley, Huw Roberts, Mariarosaria Taddeo, Luciano Floridi (2021), “The ethics of algorithms: Key problems and solutions”. In ai & Society.

			Turilli, Matteo, Luciano Floridi (2009), “The ethics of information transparency”. In Ethics and Information Technology, 11 (2), pp. 105-112. doi: 10.1007/s10676-009-9187-9.

			Turing, A.M. (1950), “Computing machinery and intelligence”. In Mind, 59 (236), pp. 433-460.

			Turing, Alan (1951), “Alan Turing’s lost radio broadcast rerecorded – On the 15th of May 1951 the bbc broadcasted a short lecture on the radio by the mathematician Alan Turing”. bbc Radio. https://www.youtube.com/watch?v=cMxbSsRntv4.

			Turner Lee, Nicol (2018), “Detecting racial bias in algorithms and machine learning”. In Journal of Information, Communication and Ethics in Society, 16 (3), pp. 252-260. https://doi.org/10.1108/ JICES-06-2018-0056.

			Università di Montréal (2017), “Montreal declaration for responsible ai”. https://www.montrealdeclaration-responsibleai.com/.

			Uszok, Andrzej, Jeffrey Bradshaw, Renia Jeffers, Niranjan Suri, Patrick Hayes, Maggie Breedy, Larry Bunch, Matt Johnson, Shriniwas Kulkarni, James Lott (2003), “kaos policy and domain services: Toward a description-logic approach to policy representation, deconfliction, and enforcement”. In Proceedings policy 2003. ieee 4th International Workshop on Policies for Distributed Systems and Networks.

			Valiant, L.G. (1984), “A theory of the learnable”. In Communications of the acm, 27 (11), pp. 1134-1142. https://doi.org/10.1145/1968.1972

			Van de Poel, Ibo, Jessica Nihlén Fahlquist, Neelke Doorn, Sjoerd Zwart, Lamber Royakkers (2012), “The problem of many hands: Climate change as an example”. In Science and Engineering Ethics, 18 (1), pp. 49-67.

			van Lier, Ben (2016), “From high frequency trading to self-organizing moral machines”. In International Journal of Technoethics (ijt), 7 (1), pp. 34-50.

			Van Riemsdijk, M. Birna, Louise A. Dennis, Michael Fisher, Koen V. Hindriks (2013), “Agent reasoning for norm compliance: A semantic approach”. In Proceedings of the 2013 International Conference on Autonomous Agents and Multiagent Systems.

			Van Riemsdijk, M. Birna, Louise Dennis, Michael Fisher, Koen V. Hindriks (2015), “A semantic framework for socially adaptive agents: Towards strong norm compliance”. In Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems.

			Vanderelst, Dieter, Alan Winfield (2018a), “An architecture for ethical robots inspired by the simulation theory of cognition”. In Cognitive Systems Research, 48, pp. 56-66.

			Vanderelst, Dieter, Alan Winfield (2018b), “The dark side of ethical robots”. In Proceedings of the 2018 aaai/acm Conference on ai, Ethics, and Society.

			Veale, Michael, Reuben Binns (2017), “Fairer machine learning in the real world: Mitigating discrimination without collecting sensitive data”. In Big Data & Society, 4 (2), 205395171774353. doi: 10.1177/2053951717743530.

			Vedder, Anton, Laurens Naudts (2017), “Accountability for the use of algorithms in a big data environment”. In International Review of Law, Compututers & Technology, 31 (2), pp. 206-224. https://doi.org/10.1080/13600869.2017.1298547.

			Veletsianos, George, Cassandra Scharber, Aaron Doering (2008), “When sex, drugs, and violence enter the classroom: Conversations between adolescents and a female pedagogical agent”. In Interacting with Computers, 20 (3), pp. 292-301.

			Vinuesa, Ricardo, Hossein Azizpour, Iolanda Leite, Madeline Balaam, Virginia Dignum, Sami Domisch, Anna Felländer, Simone Daniela Langhans, Max Tegmark, Francesco Fuso Nerini (2020), “The role of artificial intelligence in achieving the Sustainable Development Goals”. In Nature Communications, 11 (1), pp. 1-10.

			Vodafone Institute for Society and Communications (2018), “New technologies: India and China see enormous potential – Europeans more sceptical”. https://www.vodafone-institut.de/digitising-europe/digitisation-india-and-china-see-enormous-potential/.

			Wachter, Sandra, Brent Mittelstadt, Luciano Floridi (2017a), “Transparent, explainable, and accountable ai for robotics”. In Science Robotics, 2 (6), eaan6080. doi: 10.1126/scirobotics.aan6080.

			Wachter, Sandra, Brent Mittelstadt, Luciano Floridi (2017b), “Why a right to explanation of automated decision-making does not exist in the general data protection regulation”. In International Data Privacy Law, 7 (2), pp. 76-99.

			Walch, Kathleen (2019), “Are we heading for another ai winter soon?”. In Forbes, 20 ottobre. https://www.forbes.com/sites/cognitiveworld/2019/10/20/are-we-heading-for-another-ai-winter-soon/#783bf81256d6.

			Waldrop, M. Mitchell (2016), “The chips are down for Moore’s law”. In Nature, 530, pp. 144-147.

			Wang, Dayong, Aditya Khosla, Rishab Gargeya, Humayun Irshad, Andrew H. Beck (2016), “Deep learning for identifying metastatic breast cancer”. ArXiv preprint, arXiv:1606.05718.

			Wang, Gang, Manish Mohanlal, Christo Wilson, Xiao Wang, Miriam Metzger, Haitao Zheng, Ben Y. Zhao (2012), “Social Turing tests: Crowdsourcing sybil detection”. ArXiv preprint, arXiv:1205.3856.

			Wang, Shuang, Xiaoqian Jiang, Siddharth Singh, Rebecca Marmor, Luca Bonomi, Dov Fox, Michelle Dow, Lucila Ohno-Machado (2017), “Genome privacy: Challenges, technical approaches to mitigate risk, and ethical considerations in the United States”. Annals of the New York Academy of Sciences, 1387 (1), pp. 73-83. https://doi.org/10.1111/nyas.13259.

			Wang, Yilun, Michal Kosinski (2018), “Deep neural networks are more accurate than humans at detecting sexual orientation from facial images”. In Journal of Personality and Social Psychology, 114 (2), p. 246.

			Warman, Matt (2011), “Stephen Hawking tells Google ‘philosophy is dead’”. In The Telegraph, 17 maggio. http://www.telegraph.co.uk/technology/google/8520033/Stephen-Hawking-tells-Google-philosophy-is-dead.html.

			Warren, Samuel D., Louis D. Brandeis (1890), “The right to privacy”. In Harvard Law Review, pp. 193-220.

			Watson, David, Limor Gultchin, Ankur Taly, Luciano Floridi (2021), “Local explanations via necessity and sufficiency: Unifying theory and practice”. ArXiv preprint, arXiv:2103.14651.

			Watson, David S., Luciano Floridi (2020), “The explanation game: A formal framework for interpretable machine learning”. In Synthese, pp. 1-32.

			Watson, David S., Jenny Krutzinna, Ian N. Bruce, Christopher E.M. Griffiths, Iain B. McInnes, Michael R. Barnes, Luciano Floridi (2019), “Clinical applications of machine learning algorithms: Beyond the black box”. In bmj, p. l886. doi: 10.1136/bmj.l886.

			Webb, Helena, Menisha Patel, Michael Rovatsos, Alan Davoust, Sofia Ceppi, Ansgar Koene, Liz Dowthwaite, Virginia Portillo, Marina Jirotka, Monica Cano (2019) “‘It would be pretty immoral to choose a random algorithm’: Opening up algorithmic interpretability and transparency”. In Journal of Information, Communication and Ethics in Society, 17 (2), pp. 210-228. https://doi.org/10.1108/JICES-11-2018-0092.

			Wei, Sun, Wang Yuwei, Zhang Chongchong (2018), “Forecasting co2 emissions in Hebei, China, through moth-flame optimization based on the random forest and extreme learning machine”. In Environmental Science and Pollution Research, 25 (29), pp. 28985-28997. doi: 10.1007/s11356-018-2738-z.

			Weizenbaum, Joseph (1976), Computer Power and Human Reason: From Judgment to Calculation, W.-H. Freeman and Company, New York.

			Weller, Adrian (2019), “Transparency: Motivations and challenges”. ArXiv preprint, arXiv:1708.01870.

			Wellman, Michael P., Uday Rajan (2017), “Ethical issues for autonomous trading agents”. In Minds and Machines, 27 (4), pp. 609-624.

			Wexler, James (2018), The What-If Tool: Code-free Probing of Machine Learning Models. https://ai.googleblog.com/2018/09/the-what-if-tool-code-free-probing-of.html.

			Whitby, Blay (2008), “Sometimes it’s hard to be a robot: A call for action on the ethics of abusing artificial agents”. In Interacting with Computers, 20 (3), pp. 326-333.

			White, Geoff (2018), “Child advice chatbots fail sex abuse test”. 2018-12-11, Technology. Accesso 2019-04-13 09:00:09.

			Whitman, Madisson, Chien-yi Hsiang, Kendall Roark (2018), “Potential for participatory big data ethics and algorithm design: A scoping mapping review”. In Proceedings of the 15th participatory design conference on short papers, situated actions, workshops and tutorial – pdc ’18. acm Press, Hasselt and Genk, pp. 1-6. https://doi.org/10.1145/3210604.3210644.

			Wiener, Norbert (1954), The Human Use of Human Beings: Cybernetics and Society. Ed. rivista. Houghton Mifflin, Boston (Introduzione alla cibernetica. L’uso umano degli esseri umani. Tr. it. Bollati Boringhieri, Torino 1966).

			Wiener, Norbert (1960), “Some moral and technical consequences of automation”. In Science, 131 (3410), pp. 1355-1358.

			Williams, Rebecca (2017), “Lords select committee, artificial intelligence committee, written evidence (AIC0206)”. http://data.parliament.uk/writtenevidence/committeeevidence.svc/evidencedocument/artificial-intelligence-committee/artificial-intelligence/written/70496.html#_ftn13.

			Wilson, Ira Gaulbert, Marthann E. Wilson (1970), What Computers Cannot Do. Vertex, Princeton.

			Winfield, Alan (2019), “An updated round up of ethical principles of robotics and ai”, 18 aprile. http://alanwinfield.blogspot.com/2019/04/an-updated-round-up-of-ethical.html.

			Winner, Langdon (1980), “Do artifacts have politics?”. In Modern Technology: Problem or Opportunity?, 109 (1), pp. 121-136.

			Wong, Pak-Hang (2019), “Democratizing algorithmic fairness”. In Philosophy & Technology, 33, pp. 225-244. https://doi.org/10.1007/s13347-019-00355-w.

			Xenochristou, Maria, Chris Hutton, Jan Hofman, Zoran Kapelan (2020), “Water demand forecasting accuracy and influencing factors at different spatial scales using a Gradient Boosting Machine”. In Water Resources Research, 56 (8), e2019WR026304.

			Xian, Zhengzheng, Quiliang Li, Xiaoyu Huang, Lei Li (2017), “New svd-based collaborative filtering algorithms with differential privacy”. In Journal of Intelligent and Fuzzy Systems, 33 (4), pp. 2133-2144. https://doi.org/10.3233/JIFS-162053.

			Xu, Depeng, Shuhan Yuan, Lu Zhang, Xintao Wu (2018), “Fairgan: fairness-aware generative adversarial networks”. In 2018 ieee International Conference on Big Data (Big Data). ieee, Seattle, wa. https://doi.org/10.1109/BigData.2018.8622525

			Yadav, Amulya, Hau Chan, Albert Jiang, Eric Rice, Ece Kamar, Barbara Grosz, Milind Tambe (2016a), “pomdps for assisting homeless shelters – Computational and deployment challenges”. In Proceedings of the 2nd International Workshop on Issues with Deployment of Emerging Agent-based Systems.

			Yadav, Amulya, Hau Chan, Albert Xin Jiang, Haifeng Xu, Eric Rice, Milind Tambe (2016b), “Using social networks to aid homeless shelters: Dynamic influence maximization under uncertainty”. In Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems, maggio, pp. 740-748.

			Yadav, Amulya, Bryan Wilder, Eric Rice, Robin Petering, Jaih Craddock, Amanda Yoshioka- Maxwell, Mary Hemler, Laura Onasch-Vera, Milind Tambe, Darlene Woo (2018), “Bridging the gap between theory and practice in influence maximization: Raising awareness about hiv among homeless youth”. In ijcai, pp. 5399-5403.

			Yampolsky, Roman V. (2018) (a cura di), Artificial Intelligence Safety and Security. Routledge, London.

			Yang, Guang-Zhong, Jim Bellingham, Pierre E. Dupont, Peer Fischer, Luciano Floridi, Robert Full, Neil Jacobstein, Vijay Kumar, Marcia McNutt, Robert Merrifield, Bradley J. Nelson, Brian Scassellati, Mariarosaria Taddeo, Russell Taylor, Manuela Veloso, Zhong Lin Wang, Robert Wood (2018), “The grand challenges of Science Robotics”. In Science Robotics, 3 (14), eaar7650. doi: 10.1126/scirobotics.aar7650.

			Yu, Meng, Guodong Du (2019), “Why are Chinese courts turning to ai?”. In The Diplomat.

			Zerilli John, Alistair Knott, James Maclaurin, Colin Gavaghan (2019), “Transparency in algorithmic and human decision-making: Is there a double standard?”. In Philosopy & Technology, 32 (4), pp. 661-683. https://doi.org/10.1007/s13347-018-0330-6.

			Zheng, Gang, Xiaofeng Li, Rong-Hua Zhang, Bin Liu (2020), “Purely satellite data-driven deep learning forecast of complicated tropical instability waves”. In Science Advances, 6 (29), eaba1482. doi: 10.1126/sciadv.aba1482.

			Zhou, Na, Chuan-Tao Zhang, Hong-Ying Lv, Chen-Xing Hao, Tian-Jun Li, Jing-Juan Zhu, Hua Zhu, Man Jiang, Ke-Wei Liu, He-Lei Hou, Dong Liu, Ai-Qin Li, Guo-Qing Zhang, Zi-Bin Tian, Xiao-Chun Zhang (2019), “Concordance study between ibm Watson for oncology and clinical practice for patients with cancer in China”. In The Oncologist, 24 (6), pp. 812-819. doi: 10.1634/theoncologist.2018-0255.

			Zhou, Wei, Gaurav Kapoor (2011), “Detecting evolutionary financial statement fraud”. In Decision Support Systems, 50 (3), pp. 570-575. doi: 10.1016/j.dss.2010.08.007.

			Zhou, Yadi, Fei Wang, Jian Tang, Ruth Nussinov, Feixiong Cheng (2020), “Artificial intelligence in covid-19 drug repurposing”. In The Lancet Digital Health.

			Zhou, Zhifang, Tian Xiao, Xiaohong Chen, Chang Wang (2016), “A carbon risk prediction model for Chinese heavy-polluting industrial enterprises based on support vector machine”. In Chaos, Solitons & Fractals, 89, pp. 304-315.





