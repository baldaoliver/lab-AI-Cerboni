LUCIANO FLORIDI

ETICA DELL'INTELLIGENZA ARTIFICIALE


Indice


			Prefazione

			Prima parte

			Comprendere l’intelligenza artificiale

			1. Passato: l’origine dell’intelligenza artificiale

			1.1 Introduzione: la rivoluzione digitale e l’intelligenza artificiale

			1.2 Il potere di scissione del digitale: tagliare e incollare la modernità

			1.3 Nuove forme dell’agire

			1.4 ia: un ambito di ricerca in cerca di una definizione

			1.5 Conclusione: etica, governance e design

			2. Presente: ia come nuova forma dell’agire e non dell’intelligenza

			2.1 Introduzione: che cos’è l’ia? “La riconosco quando la vedo”

			2.2 ia come controfattuale

			2.3 Le due anime dell’ia: ingegneristica e cognitiva

			2.4 ia: un divorzio riuscito nell’infosfera

			2.5 L’uso umano degli esseri umani e delle interfacce

			2.6 Conclusione: chi si adatterà a chi?

			3. Futuro: lo sviluppo prevedibile dell’ia

			3.1 Introduzione: scrutare nei semi del tempo

			3.2 Dati storici, ibridi e sintetici e il bisogno di ludicizzazione

			3.3 Problemi difficili, problemi complessi e il bisogno di avvolgimento

			3.4 Il design come futuro dell’ia

			3.5 Conclusione: l’ia e le sue stagioni


		 			Prefazione

			Istruzione, affari e industria, viaggi e logistica, banche, vendita al dettaglio e shopping, intrattenimento, welfare e sanità, politica e relazioni sociali, in breve la vita stessa per come la conosciamo oggi è diventata inconcepibile senza la presenza di pratiche, prodotti, servizi e tecnologie digitali. Chiunque non sia stupito di fronte a una tale rivoluzione digitale non ne ha afferrato la portata. Stiamo parlando di un nuovo capitolo della storia umana. Naturalmente, molti altri capitoli l’hanno preceduto. Erano tutti ugualmente significativi. L’umanità ha sperimentato un mondo prima e dopo la ruota, la lavorazione del ferro, l’alfabeto, la stampa, il motore, l’elettricità, la televisione o il telefono. Ogni trasformazione è unica. Alcune di queste hanno cambiato in maniera irreversibile il modo in cui comprendiamo noi stessi, la nostra realtà e l’esperienza che ne facciamo, con implicazioni complesse e di lungo periodo. Stiamo ancora scoprendo nuovi modi per sfruttare la ruota, basti pensare alla ghiera cliccabile dell’iPod. Al contempo, è inimmaginabile ciò che l’umanità potrà ottenere grazie alle tecnologie digitali. Nessuno nel 1964 (vedi capitolo 1) avrebbe potuto immaginare come sarebbe stato il mondo solo cinquant’anni dopo. I futurologi sono i nuovi astrologi. Eppure, è anche vero che la rivoluzione digitale accade una volta sola, e cioè adesso. Questa particolare pagina della storia umana è stata voltata ed è iniziato un nuovo capitolo. Le generazioni future non sapranno mai com’era una realtà esclusivamente analogica, offline, predigitale. Siamo l’ultima generazione che l’avrà vissuta.

			Il prezzo di un posto così speciale nella storia lo si paga con incertezze che destano preoccupazioni. Le trasformazioni indotte dalle tecnologie digitali sono sorprendenti. Giustificano un po’ di confusione e di apprensione. Basta guardare i titoli dei giornali. Tuttavia, il nostro posto speciale in questo spartiacque storico, tra una realtà completamente analogica e una sempre più digitale, porta con sé anche straordinarie opportunità. Proprio perché la rivoluzione digitale è appena iniziata, abbiamo la possibilità di plasmarla in modi positivi che possono fare progredire sia l’umanità sia il nostro pianeta. Come disse una volta Winston Churchill, “prima siamo noi a dare forma agli edifici; poi sono questi a dare forma a noi”. Siamo nella primissima fase di costruzione delle nostre realtà digitali. Possiamo costruirle bene, prima che inizino a influenzare e modellare noi e le generazioni future nel modo sbagliato. La discussione sul bicchiere mezzo vuoto o mezzo pieno è inutile perché la questione davvero interessante è come possiamo riempirlo.

			Per individuare la strada migliore da percorrere nello sviluppo delle nostre tecnologie digitali, il primo, fondamentale passo è cercare di averne una maggiore e migliore comprensione. Non dovremmo sonnecchiare nella creazione di un mondo sempre più digitale. L’insonnia della ragione è vitale, perché il suo sonno genera errori mostruosi. Comprendere le trasformazioni tecnologiche in atto sotto i nostri occhi è cruciale, se vogliamo guidare la rivoluzione digitale in una direzione che sia preferibile (equa) dal punto di vista sociale e sostenibile da quello ambientale. Ciò può tradursi solo in uno sforzo collaborativo. Pertanto, in questo libro, offro il mio contributo condividendo alcune idee su un particolare tipo di tecnologia digitale, l’intelligenza artificiale (ia), e un problema specifico, la sua etica.

			Il libro fa parte di un più ampio progetto di ricerca, che investe le trasformazioni della capacità di agire indotte dalla rivoluzione digitale. Inizialmente avevo pensato di poter lavorare sia sull’intelligenza artificiale – intesa come forma dell’agire artificiale, il tema di questo libro – sia sull’agire politico, inteso come forma dell’agire collettivo sostenuta e influenzata dalle interazioni digitali. In effetti, quando sono stato invitato nel 2018 a tenere le Ryle Lectures, ho tentato di fare proprio questo, affrontando entrambi gli argomenti come due aspetti di una medesima e più profonda trasformazione. Organizzatori e partecipanti hanno detto (forse per gentilezza) che l’esperimento non era fallito. Personalmente, non l’ho trovato un grande successo. Non perché approcciarsi all’etica dell’ia e alla politica dell’informazione da un unico punto di vista, basato sulla capacità di agire, sia un errore, ma perché tale approccio funziona bene solo se si è disposti a rinunciare ai dettagli e a scambiare la profondità con l’ampiezza. Questo può andare bene in una serie di conferenze, ma trattare entrambi i temi in un’unica monografia di ricerca avrebbe prodotto un fermaporta di fascino ancora inferiore a quello che potrebbe avere questo libro. Perciò, ho deciso di dividere il progetto in due parti, questo libro sull’etica dell’ia, e il prossimo, sulla politica dell’informazione.

			Possiamo, ora, dare un rapido sguardo ai suoi contenuti. Il compito di questo volume è quello di contribuire allo sviluppo di una filosofia del nostro tempo per il nostro tempo, come ho scritto più volte. Tale sforzo è attuato in modo sistematico (l’architettura concettuale è una caratteristica preziosa del pensiero filosofico) piuttosto che in modo esauriente, perseguendo due obiettivi.

			Il primo obiettivo è metateorico ed è soddisfatto dalla prima parte. Questa comprende i primi tre capitoli, in cui offro un’interpretazione del passato (capitolo 1), del presente (capitolo 2) e del futuro dell’ia (capitolo 3). Questa prima parte non è un’introduzione all’ia in senso tecnico, o una sorta di ia per principianti. Ci sono tanti ottimi libri a questo scopo. È piuttosto un’interpretazione filosofica dell’ia come tecnologia. La tesi principale che sviluppo nel libro consiste nel dire che l’ia costituisce un divorzio senza precedenti tra l’intelligenza e la capacità di agire.

			Sulla base di questa prima parte, la seconda svolge una disamina non metateorica ma teorica delle conseguenze di tale divorzio analizzate nella prima parte.

			Nel capitolo 4, offro una prospettiva unificata sui molti principi che sono stati proposti per inquadrare l’etica dell’ia. Ciò porta a esaminare, nel capitolo 5, i potenziali rischi che possono pregiudicare l’applicazione di tali principi e quindi, nel capitolo 6, a un’analisi della relazione tra principi etici e norme giuridiche e alla definizione di etica soft come etica post-compliance. Dopo questi tre capitoli, analizzo le sfide etiche poste dallo sviluppo e dall’uso dell’ia nel capitolo 7, gli usi cattivi dell’ia nel capitolo 8 e le buone pratiche nell’applicazione dell’ia nel capitolo 9. L’ultimo gruppo di capitoli è dedicato a una serie di questioni rilevanti dal punto di vista del dibattito etico sull’ia e, in particolare, del design, sviluppo e implementazione dell’ia per il bene sociale. Pertanto, nel capitolo 10, getto luce su una questione attuale ma fuorviante, quella della singolarità. Nel capitolo 11, esamino più da vicino la natura e le caratteristiche dell’ia per il bene sociale. Nel capitolo 12, ricostruisco l’impatto positivo e negativo che l’ia ha sull’ambiente e in che modo l’ia può essere una forza positiva nella lotta ai cambiamenti climatici, ma non senza rischi e costi, che devono essere evitati o minimizzati. Nel capitolo 13, approfondisco l’analisi presentata nel capitolo 9 e discuto la possibilità di avvalersi dell’ia a sostegno dei 17 obiettivi di sviluppo sostenibile delle Nazioni Unite. In questo contesto, presento l’iniziativa di ricerca dell’Università di Oxford sull’ia per gli obiettivi di sviluppo sostenibile che ho diretto a Oxford. Infine, nel capitolo 14, concludo sostenendo l’esigenza di un nuovo matrimonio tra il verde di tutti i nostri habitat e il blu di tutte le nostre tecnologie digitali, per sostenere e sviluppare una società migliore e una biosfera più sana. Il libro si chiude con alcuni richiami a concetti che saranno centrali nel prossimo libro, La politica dell’informazione, dedicato, come accennato sopra, all’impatto delle tecnologie digitali sull’agire sociopolitico.

			Tutti i capitoli sono strettamente correlati, per cui ho aggiunto riferimenti interni ogni volta che risultava utile. I capitoli potrebbero essere letti in un ordine leggermente diverso, come ha osservato uno dei revisori anonimi. Sono d’accordo. Ho creato la struttura che mi sembrava più utile, ma alcuni lettori potrebbero, per esempio, voler leggere insieme i capitoli 9-12-13.

			In termini di radici filosofiche, come per altri libri in precedenza, anche questo è un libro tedesco, scritto da una prospettiva post-analitico-continentale, che ho l’impressione stia svanendo. Il lettore più attento collocherà facilmente quest’opera nella tradizione che collega il pragmatismo, in particolare Charles Sanders Peirce, con la filosofia della tecnologia, in particolare Herbert Simon.1 Questo volume è meno neokantiano, platonico e cartesiano di quanto mi aspettassi. In sintesi, scriverlo mi ha reso consapevole che sto uscendo dal cono d’ombra dei miei tre eroi filosofici. Non c’è niente di programmatico, ma questo è ciò che accade quando segui il tuo ragionamento ovunque ti porti. Amici Plato, Cartesius et Kant, sed magis amica veritas. In The Ethics of Information ho osservato che “alcuni libri scrivono i loro autori”. Adesso ho l’impressione che soltanto i brutti libri siano totalmente controllati dai loro autori. Si chiamano romanzi d’aeroporto e telenovele.

			La differenza principale rispetto ai libri passati è che ora sono sempre più convinto che la filosofia sia nella sua forma migliore design concettuale, e il design concettuale offre progetti mirati – comprendere il mondo per migliorarlo – e semantizzazione – dare senso e significato all’Essere, e prendersi cura del capitale semantico dell’umanità. Tutto è iniziato rendendosi conto di qualcosa di ovvio, in un caso specifico riguardante un celeberrimo filosofo di Oxford: la vera eredità di Locke è il suo pensiero politico, non la sua epistemologia. Forse Kant non voleva indurci a credere che l’epistemologia e l’ontologia fossero le regine del regno filosofico, ma è così che sono stato educato a pensare la filosofia moderna. E forse né Wittgenstein né Heidegger pensavano che la logica, il linguaggio e la sua filosofia dovessero sostituire le due regine come loro unici eredi legittimi, ma è così che sono stato anche educato a pensare la filosofia contemporanea. A ogni modo, oggi non metto più al centro dell’impresa filosofica queste discipline, ma piuttosto l’etica, la filosofia politica e del diritto. Ricerca, comprensione, formazione, realizzazione e negoziazione di ciò che è moralmente buono e giusto sono il nucleo della riflessione filosofica. Tutto il resto è il viaggio necessario per raggiungere un luogo simile, ma non va confuso con esso.

			Riguardo allo stile e alla struttura di questo libro, ripeto qui quanto ho scritto nella prefazione ai miei precedenti libri. Resto dolorosamente consapevole che anche questo non è un libro che si legge tutto d’un fiato, per utilizzare un eufemismo, nonostante i miei tentativi di renderlo il più interessante e di facile lettura possibile. Resto convinto che la ricerca esoterica (in senso tecnico) in filosofia sia l’unico modo per sviluppare nuove idee. Ma la filosofia essoterica ha il suo posto cruciale. È come la punta più accessibile e visibile rispetto alla parte più oscura e tuttavia necessaria dell’iceberg sotto la superficie della vita quotidiana. Il lettore interessato a una lettura molto più leggera potrebbe fare riferimento a La quarta rivoluzione. Come l’infosfera sta trasformando il mondo (Floridi, 2014a) o forse al testo ancora più accessibile Information: A Very Short Introduction (Floridi, 2010b).

			Anche questo libro richiede non solo una conoscenza di livello universitario della filosofia, un po’ di pazienza e del tempo, ma pure una mente aperta. Queste ultime tre sono risorse scarse. Negli ultimi due decenni circa di dibattiti, mi sono reso pienamente conto, talora attraverso confronti molto meno amichevoli di quanto avrei desiderato, che talune delle idee che sostengo in questo e nei volumi precedenti sono controverse. Negli ultimi anni ho anche notato lo scetticismo dei colleghi riguardo alla possibilità che qualcuno possa costruire una ricerca che si estende al di là di un unico, circoscritto tema di specializzazione, come se si potesse fare buona filosofia solo se questa diviene scolastica. Ricordo un incontro in cui un collega chiese, con visibile scetticismo, se qualcuno potesse affrontare più di una manciata di temi quando si fa ricerca filosofica. Non sono d’accordo. Sono grato, però, a tutte le persone che mi hanno più volte sfidato, per avermi fatto capire che preferisco essere un esploratore piuttosto che un colonizzatore. Il loro scetticismo è stato rinvigorente.

			Mi sono reso conto anche di quanto spesso si commettano errori facendo affidamento su “attrattori sistemici”: se una nuova idea assomiglia un po’ a una vecchia idea che già possediamo, allora quella vecchia idea diventa una calamita da cui la nuova è fortemente attratta, in modo pressoché irresistibile. Finiamo per pensare che “il nuovo” sia proprio come “il vecchio”, e se non ci piace “quel vecchio” allora non ci piace neanche “quel nuovo”. Cattiva filosofia indubbiamente, ma ci vogliono esercizio e forza mentale per resistere a un cambiamento così forte. Nel caso di questo libro, temo che alcuni lettori possano essere tentati dall’idea che si tratti di un libro contro la tecnologia o di un libro in cui indico i limiti dell’ia, ciò che “l’ia non può fare”, o addirittura dall’idea opposta, per cui questo libro sarebbe troppo ottimista riguardo alla tecnologia, troppo innamorato della rivoluzione digitale e dell’intelligenza artificiale come se fosse una sorta di panacea. Hanno torto entrambi. Il libro è un tentativo di stare nel mezzo, né l’inferno né il paradiso, ma il laborioso purgatorio degli sforzi umani. Naturalmente, sarei deluso nel sentirmi dire che il mio tentativo è fallito, ma sarei frustrato se il tentativo dovesse essere frainteso. Ci sono molti modi di comprendere la tecnologia: uno di questi consiste nel concepirla in termini di buon design e governance etica. In realtà credo che sia l’approccio migliore. Il lettore non deve concordare con me su tutto. Ma non vorrei essere frainteso sulla direzione che sto prendendo.

			Ho pensato che due caratteristiche possono aiutare il lettore ad accedere più facilmente ai contenuti di questo libro: i sommari e le conclusioni all’inizio e alla fine di ciascun capitolo, e qualche ridondanza. Per quanto riguarda la prima caratteristica, so che è poco ortodossa, ma la soluzione, già adottata in altri volumi, di iniziare ogni capitolo con un “In precedenza nel capitolo x…” dovrebbe consentire al lettore di navigare nel testo o passare a capitoli successivi senza perderne la trama essenziale.

			Per quanto riguarda la seconda caratteristica, in fase di redazione della versione finale del libro ho deciso di lasciare nei capitoli alcune ripetizioni e qualche riformulazione di temi ricorrenti, laddove ritenevo che il punto in cui era stato introdotto il contenuto originale fosse troppo distante, in termini di pagine o di contesto teorico. Se talora il lettore sperimenta un senso di déjà vu, spero che ciò vada a vantaggio della chiarezza, in quanto caratteristica saliente e non in quanto difetto.

			Un’ultima breve considerazione su quello che il lettore non troverà nelle pagine seguenti. Non si tratta di un’introduzione all’ia o all’etica dell’ia. Né cerco di fornire un esame esaustivo di tutte le questioni che possono essere rubricate sotto l’etichetta di “etica dell’ia”. Vi sono temi che saranno investigati e approfonditi in futuro, perché ancora poco esplorati; e altri, che investono considerazioni geopolitiche sulle politiche dell’ia, che intendo affrontare in termini più specifici in La politica dell’informazione. Il lettore interessato potrebbe voler vedere Cath e coautori (2018) sugli approcci all’ia di Stati Uniti, Unione Europea e Regno Unito, e Roberts e coautori (2021) sulla Cina, per esempio. Non è neppure un libro sugli aspetti statistici e computazionali dei temi cosiddetti fat (fairness, accountability, and transparency) o xai (explainable ai) o sulla normativa che li concerne. Questi sono tutti argomenti solo accennati nei capitoli che seguono (per maggiori informazioni al loro riguardo si vedano Watson, Floridi, 2020; Lee, Floridi, 2020; Lee, Floridi, Denev, 2020). Si tratta di un libro filosofico sulle radici di alcuni dei problemi digitali del nostro tempo, non sulle loro foglie. Un libro che investe e mette a tema una nuova forma dell’agire, la sua natura, la sua portata e le sue sfide, e il modo in cui sfruttarla a beneficio dell’umanità e dell’ambiente.



prima Parte


			Comprendere l’intelligenza artificiale



		 			Questa prima parte del libro può essere letta come una breve introduzione filosofica al passato, presente e futuro dell’intelligenza artificiale (ia). Si compone di tre capitoli. Unitariamente considerati, forniscono la cornice concettuale necessaria per comprendere la seconda parte, dove affronterò alcune delle più urgenti questioni etiche sollevate dall’ia. Nel primo capitolo, ricostruisco l’origine dell’ia, non da un punto di vista storico o tecnologico, ma da un punto di vista concettuale, concentrando l’attenzione sulle trasformazioni che hanno condotto ai sistemi di ia che usiamo oggi. Nel secondo capitolo, articolo un’interpretazione dell’ia contemporanea in termini di riserva di capacità di agire, che è stata resa possibile da due fattori: 1) il divorzio tra la capacità di risolvere problemi e di portare a termine con successo compiti, al fine di raggiungere un obiettivo, e la necessità di essere intelligenti nel farlo; e 2) la progressiva trasformazione del nostro ambiente in un’infosfera adattata all’ia, che rende tale divorzio non solo possibile ma anche efficace. Nel terzo capitolo, completerò questa prima parte osservando i possibili sviluppi dell’ia nel prossimo futuro, ancora una volta non dal punto di vista tecnico o tecnologico, ma da quello concettuale, in relazione alle tipologie preferibili di dati richiesti dai sistemi di ia e di problemi che l’ia è più facilmente in grado di risolvere.


1


			Passato: l’origine dell’intelligenza artificiale

			Sommario Nel primo paragrafo, intendo fornire una rapida panoramica di come gli sviluppi della digitalizzazione abbiano creato le condizioni per l’attuale diffusione e per il successo dei sistemi di ia. Nel secondo paragrafo, sostengo che l’impatto dirompente di tecnologie, scienze, pratiche, prodotti e servizi digitali – in breve, del digitale – sia dovuto alla loro capacità di scindere e ricomporre realtà e idee che abbiamo ereditato dalla modernità. Definisco questo aspetto potere di scissione del digitale. Tale potere di scissione è illustrato tramite esempi concreti ed è utilizzato per interpretare l’ia come una nuova forma di agire intelligente, determinata dal disallineamento digitale tra azione e intelligenza, un fenomeno senza precedenti che ha causato alcuni malintesi. Nel terzo paragrafo, presento una breve disamina dell’agire politico, che costituisce l’altra importante tipologia di azione che è stata trasformata dal potere di scissione del digitale, e spiego brevemente perché tale aspetto è importante e pertinente in questo contesto d’esame, sebbene in generale ecceda lo scopo del libro. Nel quarto paragrafo, torno sul tema principale di un’interpretazione concettuale dell’ia e introduco il secondo capitolo, ricordando al lettore quanto risulti difficile definire e tratteggiare con esattezza cosa sia precisamente l’ia e cosa conti come ia. Nell’ultimo paragrafo, sostengo che il design sia la controparte del potere di scissione del digitale e anticipo alcuni dei temi discussi nella seconda parte del libro.





1.1 Introduzione: la rivoluzione digitale e l’intelligenza artificiale


			Nel 1964, la Paramount Pictures distribuì Robinson Crusoe su Marte. Il film descriveva le avventure del comandante Christopher “Kit” Draper (Paul Mantee), un astronauta statunitense naufragato su Marte. Se lo guardiamo su YouTube anche solo per pochi minuti, ci rendiamo conto di quanto radicalmente sia cambiato il mondo in pochi decenni. In particolare, il computer che compare all’inizio del film sembra un motore di epoca vittoriana, con leve, ingranaggi e quadranti. Un pezzo di archeologia che avrebbe potuto usare il dottor Frankenstein. Eppure, verso la fine della storia, Friday (Victor Lundin) viene rintracciato da un’astronave aliena attraverso i suoi braccialetti. Un pezzo di futurologia che sembra inquietantemente preveggente.

			Robinson Crusoe su Marte apparteneva a un’epoca diversa, tecnologicamente e culturalmente più vicina al secolo scorso che al nostro. Descrive una realtà moderna, non contemporanea, basata sull’hardware e non sul software. Computer portatili, Internet, servizi web, touch screen, smartphone, orologi intelligenti, social media, shopping online, video e musica in streaming, automobili a guida autonoma, tosaerba robotizzati o assistenti virtuali non esistono ancora. L’intelligenza artificiale è più un progetto che una realtà. Il film mostra una tecnologia fatta di dadi, bulloni e meccanismi che seguono le goffe leggi della fisica newtoniana. È una realtà del tutto analogica, basata sugli atomi piuttosto che sui byte, di cui i Millennials non hanno esperienza, essendo nati dopo i primi anni Ottanta. Per loro un mondo senza tecnologie digitali assomiglia a ciò che era per me (nato nel 1964) un mondo senza automobili: qualcosa di cui ho sentito parlare solo da mia nonna.

			Si fa spesso osservare che uno smartphone racchiude molta più potenza di calcolo in pochissimo spazio di quanto la nasa potesse mettere insieme quando Armstrong atterrò sulla Luna cinque anni dopo Robinson Crusoe su Marte, nel 1969, e tutto questo a un costo pressoché trascurabile. Molti articoli hanno tracciato alcuni raffronti precisi nel 2019, per il cinquantesimo anniversario dello sbarco sulla Luna, facendo emergere alcuni fatti sorprendenti. L’Apollo Guidance Computer (agc) a bordo dell’Apollo 11 aveva 32.768 bit di ram (random access memory) e 589.824 bit (72 kb) di rom (read only memory): uno spazio di memoria su cui non saremmo stati in grado di salvare questo libro. Cinquant’anni dopo, un cellulare possiede, in media, 4 gb di ram e 512 gb di rom: vale a dire, un milione di volte in più di ram e sette milioni di volte in più di rom. Per quanto riguarda il processore, l’agc funzionava a 0,043 mhz. Un processore per iPhone funziona, in media, a circa 2490 mhz: vuol dire che è circa 58.000 volte più veloce. Per afferrare meglio il senso di questa accelerazione, forse un paragone può esserci di aiuto. In media, una persona cammina alla velocità di 5 km/h. Oggi, alcuni jet supersonici possono viaggiare alla velocità di 6100 km/h, vale a dire a una velocità più di cinque volte superiore a quella del suono (1235 km/h). Eppure è soltanto poco più di mille volte più veloce che camminare. Immaginiamo di moltiplicarlo per 58.000.

			Dove sono andate a finire tutta questa velocità e questa potenza di calcolo? La risposta è duplice: fattibilità e usabilità. Possiamo fare sempre di più, in termini di applicazioni, e possiamo farlo in modi sempre più semplici, non solo per ciò che concerne la programmazione, ma soprattutto per ciò che riguarda l’esperienza dell’utente. I video, per esempio, sono avidi di potenza di calcolo, così come i sistemi operativi. L’ia oggi è possibile anche perché abbiamo la potenza di calcolo necessaria per far funzionare il suo software.

			Grazie a questa crescita sbalorditiva delle capacità di archiviazione ed elaborazione, a costi sempre più contenuti, oggi miliardi di persone sono connessi e trascorrono molte ore online ogni giorno. Nel Regno Unito, per esempio, “nel 2018, il tempo medio trascorso utilizzando Internet è stato di 25,3 ore a settimana. Si è trattato di un incremento di 15,4 ore rispetto al 2005”.1 Ciò è tutt’altro che insolito. E la pandemia ha generato un incremento ulteriore. Tornerò su questo punto nel prossimo capitolo, ma un altro dei motivi per cui l’ia è possibile oggi è proprio il fatto che noi esseri umani trascorriamo sempre più tempo in contesti digitali e, pertanto, adattati all’ia.

			Più memoria, più velocità e più ambienti e interazioni digitali hanno generato più dati, in quantità immense. Tutti abbiamo osservato diagrammi con curve esponenziali, che indicavano quantità che non sappiamo neppure come rappresentare. Secondo la società di analisi di mercato idc,2 nel 2018 abbiamo raggiunto 18 zettabyte di dati creati, catturati o riprodotti, e questa sorprendente crescita di dati non mostra segni di rallentamento: a quanto pare, diventeranno 175 zettabyte nel 2025. Si tratta di un aspetto difficile da cogliere in termini di quantità, ma due conseguenze meritano un momento di riflessione (sono entrambe esaminate in Floridi, 2014a). La velocità e la memoria delle nostre tecnologie digitali non crescono alla stessa velocità dell’universo dei dati. Ne consegue che stiamo rapidamente passando da una cultura della registrazione a una della cancellazione: la questione non è più che cosa salvare ma che cosa eliminare per fare spazio ai nuovi dati. La maggior parte dei dati disponibili è stata creata a partire dagli anni Novanta, anche se contiamo ogni parola pronunciata, scritta o stampata nella storia dell’umanità e ogni biblioteca o archivio mai esistiti. Per rendersene conto, basta osservare uno dei tanti diagrammi disponibili online che illustrano l’esplosione dei dati: l’aspetto sorprendente non sta solo sulla parte destra, dove la freccia della crescita sale, ma anche sulla parte sinistra, dove la crescita ha avuto inizio: si tratta solo di una manciata di anni fa. Poiché tutti questi dati sono stati creati dalla generazione attuale, stanno anche invecchiando insieme, in termini di supporti e tecnologie obsolete. Per questo motivo la loro conservazione costituirà una questione sempre più urgente.

			Una maggiore potenza di calcolo e una maggiore quantità di dati hanno reso possibile il passaggio dalla logica alla statistica. Le reti neurali che erano interessanti solo da un punto di vista teorico (ne ho discusse alcune in un libro precedente già alla fine degli anni novanta: Floridi, 1999) sono diventate strumenti ordinari nell’ambito dell’apprendimento automatico. La vecchia ia era per lo più simbolica e poteva essere interpretata come una branca della logica matematica, ma la nuova ia è principalmente connessionista e potrebbe essere interpretata come una branca della statistica. Il principale cavallo di battaglia dell’ia non è più la deduzione logica ma l’inferenza e la correlazione statistica.

			La potenza e la velocità di calcolo, le dimensioni della memoria, la quantità di dati, i potenti algoritmi, gli strumenti statistici e le interazioni online sono tutti fattori che stanno crescendo in modo incredibilmente rapido. Ciò accade anche perché (e la relazione causale procede in entrambe le direzioni) il numero di dispositivi digitali che interagiscono tra loro è già notevolmente superiore alla popolazione umana. Perciò, la maggior parte delle comunicazioni avviene da macchina a macchina, senza coinvolgimento umano. Ci sono robot computerizzati su Marte controllati a distanza dalla Terra. Il comandante Christopher “Kit” Draper li avrebbe trovati assolutamente fantastici.

			Tutte le tendenze precedenti continueranno a crescere, inarrestabilmente, nel prossimo futuro. Queste tendenze hanno modificato il modo in cui impariamo, giochiamo, lavoriamo, amiamo, odiamo, scegliamo, decidiamo, produciamo, vendiamo, compriamo, consumiamo, pubblicizziamo, ci divertiamo, ci preoccupiamo di qualcosa e ce ne prendiamo cura, socializziamo, comunichiamo e così via. Sembra impossibile trovare un qualsiasi aspetto della nostra vita che non sia stato influenzato dalla rivoluzione digitale. Nell’ultimo mezzo secolo circa, la nostra realtà è diventata sempre più digitale, fatta di zero e uno, gestita da software e dati, piuttosto che da hardware e atomi. Un numero crescente di persone vive sempre più diffusamente onlife, sia online sia offline, e nell’infosfera, sia digitalmente sia analogicamente. Questa rivoluzione digitale ha anche influito sul modo in cui concepiamo e comprendiamo le nostre realtà, che sono sempre più interpretate in termini computazionali e digitali. Basti pensare alla “vecchia” analogia tra il nostro dna e il nostro “codice”, che ora diamo per scontata. Tutto ciò ha anche alimentato lo sviluppo dell’ia, dal momento che condividiamo le nostre esperienze onlife e gli ambiti dell’infosfera con agenti artificiali, siano essi algoritmi, bot o robot. Per capire in che cosa effettivamente consista l’ia – la mia tesi è che si tratti di una nuova forma di agire, e non di intelligenza – occorre dunque dire qualcosa di più sull’impatto della rivoluzione digitale: è quanto mi propongo di fare nella parte restante di questo capitolo. È solo comprendendo la traiettoria concettuale delle sue implicazioni che possiamo avere una corretta prospettiva sulla natura dell’ia (secondo capitolo), i suoi probabili sviluppi (terzo capitolo) e le sue sfide etiche (seconda parte).





1.2 Il potere di scissione del digitale: tagliare e incollare la modernità


			Le tecnologie, le scienze, le pratiche, i prodotti e i servizi digitali, in breve il digitale come fenomeno globale sta profondamente trasformando la realtà. Tutto questo è piuttosto ovvio e pacifico. Le vere domande consistono casomai nel chiedersi perché, come e con quali conseguenze, soprattutto in relazione all’ia. In ciascuno di questi casi, la risposta è tutt’altro che banale e sicuramente aperta al dibattito. Per chiarire quali siano le risposte che ritengo più convincenti e introdurre nel prossimo capitolo un’interpretazione dell’ia come una riserva crescente di capacità di agire intelligente, conviene entrare in medias res e partire dal “come”. Diventerà allora più facile fare un passo indietro, per comprendere il “perché”, e poi procedere in avanti, per affrontare le “conseguenze” e collegare le risposte all’emergere dell’ia.

			Il digitale “taglia e incolla” le nostre realtà sia ontologicamente sia epistemologicamente. Con questo intendo dire che incolla, scolla o rincolla certi aspetti del mondo – e quindi le nostre corrispondenti ipotesi su di essi – che pensavamo fossero immutabili. Separa e riunisce, per così dire, gli “atomi” delle nostre esperienza e cultura “moderne”. Modifica il letto del fiume, per usare la metafora di Wittgenstein. Alcuni esempi lampanti possono chiarire l’idea in modo più diretto.

			Consideriamo, anzitutto, uno dei casi più significativi di incollamento. La nostra identità e i nostri dati personali non sono mai stati incollati insieme così indistinguibilmente come accade oggi, allorché si parla di identità personale dei soggetti interessati.3 I conteggi dei censimenti sono molto datati (Alterman, 1969). L’invenzione della fotografia ha avuto un forte impatto sulla privacy (Warren, Brandeis, 1890). I governi europei hanno reso obbligatorio viaggiare con il passaporto durante la Prima guerra mondiale, per motivi di migrazione e sicurezza, estendendo così il controllo dello Stato sui mezzi di mobilità (Torpey, 2000). Ma è soltanto il digitale, con il suo enorme potere di registrare, monitorare, condividere e processare quantità illimitate di dati su Alice, che ha saldato insieme chi è Alice, la sua identità e il suo profilo individuali, con le informazioni personali su di lei. La privacy è diventata una questione urgente anche, se non principalmente, a causa di tale incollamento, e oggi, almeno nella normativa dell’Unione Europea, la protezione dei dati personali è discussa in termini di dignità umana (Floridi, 2016c) e identità personale (Floridi, 2005a, 2006), con i cittadini descritti come soggetti interessati.

			L’esempio successivo riguarda la posizione e la presenza, e il loro scollamento. In un mondo digitale, è ovvio che uno può trovarsi fisicamente in un posto, diciamo un bar, ed essere presente interattivamente in un altro, diciamo una pagina su Facebook. Eppure tutte le generazioni passate che vivevano in un mondo esclusivamente analogico hanno concepito e sperimentato posizione e presenza come due lati inseparabili della stessa situazione umana: l’essere situati nello spazio e nel tempo, qui e ora. L’azione a distanza e la telepresenza appartenevano a mondi magici o alla fantascienza. Oggi, questo scollamento è un semplice tratto dell’esperienza comune in qualsiasi società dell’informazione (Floridi, 2005b). Siamo la prima generazione per la quale “Dove sei?” non è soltanto una domanda retorica. Ovviamente, tale scollamento non ha interrotto tutti i collegamenti. La geolocalizzazione funziona solo se è possibile monitorare la telepresenza di Alice. E la telepresenza di Alice è possibile solo se si trova all’interno di un ambiente fisicamente connesso. Ma questi due aspetti sono ora totalmente distinti e in effetti il loro scollamento ha almeno in parte declassato la posizione a favore della presenza. Perché se tutto ciò di cui Alice ha bisogno e si preoccupa è di essere presente digitalmente e interattiva in un particolare angolo dell’infosfera, non importa in quale parte del mondo si trovi analogicamente, se a casa, in treno o nel suo ufficio. Questo è il motivo per cui banche, librerie, biblioteche e negozi di vendita al dettaglio sono tutti luoghi disegnati per la localizzazione oggi alla ricerca di un riutilizzo in termini di presenza. Quando un negozio apre una caffetteria, sta cercando di ricongiungere la presenza e la localizzazione dei clienti che è stata separata dall’esperienza digitale.

			Consideriamo, inoltre, lo scollamento tra legge e territorialità. Per secoli, all’incirca dalla pace di Westfalia (1648), la geografia politica ha fornito alla giurisprudenza una pronta risposta alla questione relativa all’ambito di applicazione di una sentenza: la decisione del giudice si applica nel perimetro dei confini nazionali entro cui opera l’autorità della legge. Tale connessione potrebbe essere riassunta nei seguenti termini: “Il mio posto le mie regole, il tuo posto le tue regole”. Ora può sembrare scontato, ma ci sono voluti molto tempo ed enorme fatica per raggiungere un approccio così semplice, che funziona ancora oggi molto bene, purché si operi solo all’interno di uno spazio fisico, analogico. Tuttavia, Internet non è uno spazio fisico, e il problema della territorialità si profila a partire dallo scollamento ontologico tra lo spazio normativo del diritto, lo spazio fisico della geografia e lo spazio logico del digitale. Si tratta di una nuova “geometria” variabile che stiamo ancora imparando a gestire. Per esempio, lo scollamento tra legge e territorialità è diventato tanto palese quanto problematico durante il dibattito sul cosiddetto diritto all’oblio (Floridi, 2015a). I motori di ricerca operano all’interno di uno spazio logico online fatto di nodi, collegamenti, protocolli, risorse, servizi, url, interfacce ecc. Ciò significa che qualsiasi cosa è a portata di clic. Perciò è difficile dare piena attuazione al diritto all’oblio, richiedendo a Google di rimuovere i collegamenti alle informazioni personali di qualcuno dalla sua versione .com negli Stati Uniti in forza di una decisione presa dalla Corte di Giustizia dell’Unione Europea, dal momento che tale decisione può risultare inutile se i collegamenti non sono rimossi da tutte le versioni del motore di ricerca. Si noti però che un tale disallineamento degli spazi non genera solo problemi, ma fornisce anche soluzioni. La non territorialità del digitale fa miracoli per la libera circolazione delle informazioni. In Cina, per esempio, il governo deve compiere uno sforzo costante e sostenuto per controllare le informazioni online. Parimenti, il Regolamento generale sulla protezione dei dati personali deve essere ammirato per la sua capacità di sfruttare la “saldatura” tra identità personale e informazioni personali per aggirare lo “scollamento” tra legge e territorialità, fondando la protezione dei dati personali sul legame tra persona e dato (che è ora fondamentale) piuttosto che sulla geografia (il luogo dove tali dati personali sono trattati non è più rilevante).

			Infine, possiamo considerare un incollamento che risulta essere più precisamente un ri-incollamento. Nel suo libro The Third Wave (La terza ondata) Alvin Toffler ha coniato il termine prosumer in riferimento all’affievolirsi della differenza e alla progressiva fusione tra il ruolo di produttore e quello di consumatore (Toffler, 1980). Egli ha attribuito questa tendenza a un mercato altamente saturo e alla produzione di massa di prodotti standardizzati, che ha stimolato un processo di personalizzazione di massa e quindi un crescente coinvolgimento dei consumatori come produttori dei propri prodotti personalizzati. Questa idea era stata anticipata da Marshall McLuhan e Barrington Nevitt (1972), che hanno attribuito il fenomeno alle tecnologie basate sull’elettricità. In seguito, si è fatto riferimento con ciò al consumo di informazioni prodotte dalla stessa popolazione di produttori, per esempio su YouTube. Ignaro di questi precedenti, quasi vent’anni dopo Toffler, io ho introdotto la parola produmer per cogliere lo stesso fenomeno.4 Tuttavia, in tutti questi casi, la posta in gioco non è tanto un nuovo incollamento, quanto piuttosto, per essere precisi, un ri-incollamento. Per la maggior parte della nostra storia (circa il 90% [Lee, Daly, 1999]) abbiamo vissuto in società di cacciatori-raccoglitori, che cercano cibo per sopravvivere. Durante questo lungo periodo, produttori e consumatori normalmente hanno coinciso. I prosumer che cacciavano animali selvatici e raccoglievano piante selvatiche erano, in altri termini, la normalità, non l’eccezione. È solo dopo lo sviluppo delle società agrarie, circa 10.000 anni fa, che abbiamo assistito a una separazione completa, e nel tempo culturalmente palese, tra produttori e consumatori. In alcuni angoli dell’infosfera, quella disgiunzione è ricongiunta. Su Instagram o TikTok, per esempio, consumiamo ciò che produciamo. Si può quindi rimarcare che, in alcuni casi, questa parentesi stia volgendo al termine e che i prosumer siano tornati, ricongiunti dal digitale. Di conseguenza, è perfettamente coerente che il comportamento umano in rete sia stato comparato e studiato in termini di modelli di foraggiamento sin dagli anni Novanta (Pirolli, Card, 1995, 1999; Pirolli, 2007).

			Il lettore può facilmente elencare ulteriori casi di incollamento, scollamento e ri-incollamento. Si pensi, per esempio, alla differenza tra realtà virtuale (scollamento) e realtà aumentata (incollamento); la consueta disgiunzione tra uso e proprietà nella share economy; la ricongiunzione di autenticità e memoria grazie alla blockchain; o l’attuale dibattito su un reddito di base universale, un caso di scollamento tra stipendio e lavoro. Ma è giunto il momento di procedere dalla domanda relativa al “come” a quella relativa al “perché”. Perché il digitale ha questo potere di scissione, vale a dire di incollare, scollare o ri-incollare il mondo? Perché altre innovazioni tecnologiche sembrano non avere un impatto simile? La risposta, suppongo, sta nella combinazione di due fattori.

			Da un lato, il digitale è una tecnologia di terzo ordine (Floridi, 2014a). Non è solo una tecnologia che sta tra noi e la natura, come un’ascia (primo ordine); o una tecnologia che sta tra noi e un’altra tecnologia, come un motore (secondo ordine). È piuttosto una tecnologia che sta tra una tecnologia e un’altra tecnologia, come un sistema computerizzato che controlla un robot che dipinge un’automobile (terzo ordine). A causa dell’autonoma potenza di calcolo del digitale, potremmo anche non avere controllo sul (per non parlare di essere parte del) processo.

			Dall’altro, il digitale non è semplicemente qualcosa che potenzia o aumenta una realtà, ma qualcosa che la trasforma radicalmente, perché crea nuovi ambienti che abitiamo e nuove forme di agire con cui interagiamo. Non c’è un termine specifico per descrivere questa profonda trasformazione, perciò in passato (Floridi, 2010b) ho utilizzato l’espressione re-ontologizzazione per fare riferimento a una radicale forma di re-ingegnerizzazione, che non consiste soltanto nel disegnare, costruire o strutturare un sistema (come una società, una macchina o un qualche artefatto) in modo nuovo, ma nel trasformare fondamentalmente la sua natura intrinseca, vale a dire la sua ontologia. In questo senso, per esempio, le nanotecnologie e le biotecnologie stanno non semplicemente re-ingegnerizzando, ma re-ontologizzando il nostro mondo. Attraverso la re-ontologizzazione della modernità, per dirlo in breve, il digitale sta anche ridefinendo dal punto di vista epistemologico la mentalità moderna, cioè molte delle nostre concezioni e idee consolidate.

			Considerati unitariamente, tutti questi fattori suggeriscono che il digitale deve il suo potere di scissione al suo essere una tecnologia di terzo ordine re-ontologizzante e ri-epistemologizzante. Questo è il motivo per cui fa ciò che fa e per cui nessun’altra tecnologia neppure si avvicina ad avere un effetto similare.





1.3 Nuove forme dell’agire


			Se quanto detto fin qui è a grandi linee corretto, allora ci può aiutare a dare un senso ad alcuni fenomeni attuali riguardanti la trasformazione della morfologia dell’agire nell’era digitale e, in tal modo, a rendere conto di alcune forme dell’agire che sono solo apparentemente non correlate. La loro trasformazione dipende dal potere di scissione del digitale, ma la loro interpretazione potrebbe essere dovuta a un implicito fraintendimento di tale potere e delle sue conseguenze ulteriori, profonde e durature. Mi riferisco all’agire politico in quanto democrazia diretta e all’agire artificiale in quanto ia. In sostanza, la re-ontologizzazione dell’agire non è stata ancora accompagnata da un’adeguata ri-epistemologizzazione della sua interpretazione. O per dirlo in modo meno preciso ma forse più intuitivo: il digitale ha cambiato la natura dell’agire, ma stiamo ancora interpretando l’esito di tali cambiamenti attraverso una mentalità moderna, e ciò genera qualche profondo malinteso. Dirò soltanto qualche parola sull’agire politico e la democrazia diretta perché, come anticipato, in questo libro mi concentrerò esclusivamente sull’agire artificiale, senza mettere a tema l’agire sociopolitico che è modellato e sostenuto dalle tecnologie digitali.

			Negli attuali dibattiti sulla democrazia diretta, talora siamo indotti erroneamente a credere che il digitale dovrebbe (si noti l’approccio normativo opposto a quello descrittivo) ricongiungere sovranità (il potere politico che può essere legittimamente delegato) e governance (il potere politico che è legittimamente delegato, temporaneamente, condizionatamente e responsabilmente, e che può essere in modo altrettanto legittimo ripreso: Floridi, 2016e). La democrazia rappresentativa è comunemente (benché erroneamente) concepita come un compromesso dovuto a vincoli pratici di comunicazione. La vera democrazia sarebbe quella diretta, in quanto basata sulla partecipazione immediata, costante e universale di tutti i cittadini alle questioni politiche. Purtroppo, così si è soliti argomentare, siamo troppo numerosi, e quindi la delega del potere politico è il male minore ma necessario (Mill, 1861, p. 69). È il mito della città-Stato, in particolare di Atene. Il compromesso a favore di una democrazia rappresentativa è sembrato inevitabile per secoli, fino all’avvento del digitale. Secondo taluni, il digitale promette ora di congiungere (o di ricongiungere, se si crede in qualche mitico tempo antico) sovranità e governance per offrire un nuovo tipo di democratica agorà digitale, che potrebbe infine rendere possibile il costante coinvolgimento diretto di ogni cittadino interessato. È la stessa promessa formulata dallo strumento referendario (soprattutto se vincolante, invece che consultivo). In entrambi i casi, agli elettori viene chiesto direttamente cosa si dovrebbe fare. Il solo compito lasciato alla classe politica, amministrativa e tecnica sarebbe di attuare la decisione popolare. I politici sarebbero funzionari delegati (e non rappresentanti) in un senso molto letterale. Eppure questo è un errore, perché la democrazia indiretta è sempre stata il vero progetto da realizzare. La disgiunzione è una caratteristica e non un difetto, per dirlo in modo esplicito. E ciò perché un regime democratico è prima di tutto caratterizzato non da talune procedure o da alcuni valori (elementi da cui pure è caratterizzato), ma da una chiara e netta separazione – cioè disgiunzione – tra coloro a cui appartiene il potere politico (sovranità) che delegano legittimamente con il voto (di tutti i cittadini che vi hanno diritto) e coloro a cui è affidato questo potere politico (governance) che esercitano in forza di tale mandato, governando in modo trasparente e responsabile, fintanto che vi sono legittimamente autorizzati. Per dirlo in termini più chiari, un regime democratico non è semplicemente un modo di esercitare e gestire il potere in base a determinate procedure e/o conformemente a determinati valori, ma prima di tutto un modo di strutturarlo: chi detiene il potere non lo esercita ma lo affida a chi lo esercita ma non lo detiene. La commistione tra queste due parti conduce a instabili forme di dittatura o di dominio delle masse. In tal senso la democrazia rappresentativa non è un compromesso ma in realtà la migliore forma di democrazia. Usare il digitale per congiungere (o, più miticamente, ricongiungere) sovranità e governance sarebbe un errore che si paga a caro prezzo. Brexit, Trump, Lega Nord e altri disastri populisti generati dalla “tirannia della maggioranza” (Adams, 1787) ne sono una prova sufficiente. Dobbiamo considerare quale sia il modo migliore per trarre vantaggio dal programmato disallineamento rappresentativo tra sovranità e governance, e non come cancellarlo. Il consenso è il problema, ma non è il tema di questo libro. Ciò che volevo suggerire con l’analisi precedente era solo un saggio del tipo di considerazioni sulle forme dell’agire che mettono in relazione l’impatto del digitale sulla politica con il modo in cui esaminiamo e valutiamo l’ia, come vedremo nel prossimo paragrafo.





1.4 ia: un ambito di ricerca in cerca di una definizione


			Alcune persone (forse molte) sembrano credere che l’ia riguardi la congiunzione tra agire artificiale e comportamento intelligente in nuovi artefatti. Questo è un malinteso. Come spiegherò più diffusamente nel prossimo capitolo, in realtà è vero il contrario: la rivoluzione digitale ha reso l’ia non solo possibile ma sempre più utile separando la capacità di risolvere un problema o di portare a termine un compito con successo dall’esigenza di essere intelligenti nel farlo. L’ia ha successo proprio quando è possibile realizzare tale separazione. Quindi, la consueta lamentela nota come “effetto dell’ia”5 – per cui non appena l’ia può eseguire un compito particolare, per esempio la traduzione automatica o il riconoscimento vocale, il bersaglio si sposta e quel compito non è più definito intelligente se eseguito dall’ia – è in realtà il riconoscimento corretto di come funzioni il processo in questione. L’ia esegue con successo un compito solo se può slegare la sua esecuzione dall’esigenza di essere intelligente nell’eseguirlo. Pertanto, se l’ia ha successo, allora la separazione ha avuto luogo e in effetti il compito si è dimostrato dissociabile dall’intelligenza che sembrava richiesta (per esempio in un essere umano) per portarlo a termine con successo. Ciò è meno sorprendente di quanto possa parere, e nel prossimo capitolo vedremo che è perfettamente coerente con le definizioni classiche, e probabilmente ancora tra le migliori, di ia, fornite da McCarthy, Minsky, Rochester e Shannon nella loro “Proposta per il progetto estivo di ricerca sull’intelligenza artificiale di Dartmouth”, il documento fondante e il successivo evento che hanno gettato le basi dei primi studi sull’ia nel 1955. Mi limito a citarlo di seguito, rimandandone la trattazione al prossimo capitolo:

			Per il presente scopo il problema dell’intelligenza artificiale è quello di far sì che una macchina agisca con modalità che sarebbero definite intelligenti se un essere umano si comportasse allo stesso modo. (Citazione dalla riedizione del 2006 in McCarthy, Minsky, Rochester, Shannon, 2006)

			Le conseguenze derivanti dal comprendere l’ia come un divorzio tra l’agire e l’intelligenza sono profonde, così come lo sono le sfide etiche a cui dà origine tale divorzio. Il resto del libro si concentra sulla loro analisi. Per concludere questo capitolo introduttivo, resta ancora da dare una risposta finale alla domanda “con quali conseguenze”, di cui parlavo all’inizio. Questo è il compito del prossimo paragrafo.





1.5 Conclusione: etica, governance e design


			Supponendo che le risposte precedenti alle domande “perché” e “come” siano plausibili, che differenza fa comprendere il potere del digitale come un potere di tagliare e incollare il mondo e la sua rappresentazione in modi senza precedenti? Un’analogia può aiutare a introdurre la risposta. Se qualcuno ha solo una pietra e nient’altro, nemmeno un’altra pietra da mettere accanto a essa, allora non c’è altro che si possa fare se non godersi la pietra stessa, magari guardandola o giocandoci. Ma se si può tagliare la pietra in due parti, ci sono già diverse possibilità di combinarle insieme. Due pietre forniscono più possibilità e meno vincoli di una singola pietra, e numerose pietre molte di più. Ma tagliare e incollare le pietre della modernità è, per così dire, proprio ciò che il digitale sa fare meglio, quando si tratta del mondo. Trarre vantaggio da tali possibilità e vincoli in vista della risoluzione di alcuni problemi è ciò che possiamo definire design. Quindi, la risposta ora dovrebbe essere chiara. Il potere di scissione del digitale riduce enormemente i vincoli della realtà e ne aumenta le possibilità. In tal modo, rende il design – l’arte di risolvere un problema sfruttando vincoli e possibilità, per soddisfare alcuni requisiti in vista di un obiettivo – l’attività innovativa che definisce la nostra epoca.

			Il nostro viaggio è adesso completo. Ogni epoca ha innovato la propria cultura, società e ambiente facendo affidamento su almeno tre elementi principali: la scoperta, l’invenzione e il design. Questi tre tipi di innovazione sono strettamente correlati, anche se l’innovazione è stata spesso più inclinata in una direzione rispetto alle altre, come uno sgabello con tre gambe, di cui una sia più lunga e pertanto più prominente rispetto alle altre. L’età post-rinascimentale e la prima modernità possono essere qualificate come l’epoca delle scoperte, soprattutto geografiche. La tarda modernità è ancora un’epoca di scoperte ma, con le sue innovazioni industriali e meccaniche, è forse in misura maggiore un’epoca di invenzioni. E, naturalmente, tutte le epoche sono state anche età del design, almeno in quanto le scoperte e le invenzioni richiedono modi ingegnosi per collegare e dare forma a nuove e vecchie realtà. Ma se è corretto quello che ho sostenuto finora, allora è davvero la nostra epoca che è in modo peculiare, e più di ogni altra, l’età del design. Perché il digitale sta riducendo i vincoli e aumentando le possibilità a nostra disposizione, offrendoci un’immensa e crescente libertà di strutturare e organizzare il mondo in una moltitudine di modi, per risolvere una varietà di problemi vecchi e nuovi. Naturalmente, ogni design richiede un progetto. E, nel nostro caso, si tratta di un progetto umano per la nostra era digitale che ancora ci manca. Tuttavia, non dovremmo lasciare che la forza di scissione del digitale plasmi il mondo senza un piano. Dobbiamo intraprendere ogni sforzo per decidere in quale direzione desideriamo sfruttarlo, per garantire che le società dell’informazione che stiamo costruendo grazie a esso siano aperte, tolleranti, eque, giuste e favorevoli all’ambiente e allo sviluppo umano. La conseguenza più rilevante del potere di scissione del digitale dovrebbe essere un design migliore del nostro mondo. E questo riguarda la definizione dell’ia come nuova forma dell’agire, come vedremo nel capitolo seguente.


2


			Presente: ia come nuova forma dell’agire e non dell’intelligenza

			Sommario In precedenza, nel primo capitolo, abbiamo visto come la rivoluzione digitale abbia tagliato e incollato la nostra realtà e le nostre idee sulla realtà, re-ontologizzando e ri-epistemologizzando la modernità. Ciò ha portato allo sviluppo dell’ia come nuova forma dell’agire, che può avere successo senza essere intelligente. Nel presente capitolo analizzo questa interpretazione. Nel primo paragrafo, mostro come l’assenza di una definizione di ia sia la prova che l’espressione non è un termine scientifico ma un’utile scorciatoia per fare riferimento a una famiglia di scienze, metodi, paradigmi, tecnologie, prodotti e servizi. Nel secondo paragrafo, faccio riferimento alla classica descrizione controfattuale dell’ia fornita da McCarthy, Minsky, Rochester e Shannon nella loro “Proposta per il progetto estivo di ricerca sull’intelligenza artificiale di Dartmouth”. L’abbiamo già incontrata nel capitolo precedente, ed è la descrizione che adotterò nel resto del libro. Esamino anche la celebre domanda di Turing: “Le macchine possono pensare?”. Nel terzo paragrafo, sulla base della precedente analisi delineo l’approccio ingegneristico e cognitivo all’ia, sostenendo che il primo si è tradotto in un grande successo mentre il secondo in un totale fallimento. L’interpretazione dell’ia come nuova forma dell’agire che non deve essere intelligente per avere successo si basa sulla tradizione ingegneristica: nel paragrafo seguente, il quarto, suggerisco che tale forma dell’agire può avere successo perché abbiamo trasformato il mondo (avvolgendolo) in un ambiente sempre più adattato al funzionamento dell’ia. In conclusione, sottolineo come tale processo generi il rischio di spingere l’umanità a adattarsi alle sue tecnologie intelligenti.





2.1 Introduzione: che cos’è l’ia? “La riconosco quando la vedo”


			L’ia è stata definita in molti modi, ma non esiste una sua definizione unitaria su cui tutti concordino. Un rapporto datato (Legg, Hutter, 2007) elencava già 53 definizioni di “intelligenza”, ciascuna delle quali in linea di principio può essere “artificiale”, e 18 definizioni di ia. Il numero è in crescita (Russell, Norvig, 2016). Di fronte a una sfida simile, Wikipedia risolve il problema optando per una tautologia:

			L’intelligenza artificiale (ia) […] è l’intelligenza mostrata dalle macchine, in contrasto con l’intelligenza naturale mostrata dagli esseri umani. (Wikipedia, “Artificial Intelligence”, 17 gennaio 2020)

			Ciò è al contempo assolutamente vero e totalmente inutile. Mi ricorda l’inattaccabile “Brexit significa Brexit”, meccanicamente ripetuto da Theresa May quando rivestiva il ruolo di primo ministro del Regno Unito.

			L’assenza di una definizione standard di ia può essere un problema perché, quasi inevitabilmente, in un seminario sull’etica dell’ia prima o poi qualche brillante partecipante non può fare a meno di chiedersi, pensieroso: “Ma cosa si intende veramente per ia?” (l’enfasi è presente nell’originale, poiché alcune persone riescono a parlare in corsivo). Di solito segue una discussione senza fine, che non raggiunge mai alcun consenso (né potrebbe essere altrimenti) e lascia tutti (inevitabilmente) frustrati. Il rischio è che, dopo una tale perdita di tempo, qualcuno possa concludere che non si possa avere un dibattito sull’etica di qualcosa che è indefinito e apparentemente indefinibile. Questo non ha senso. Certamente la mancanza della definizione di qualcosa di così importante solleva qualche sospetto. Tuttavia ciò non è dovuto al fatto che tutte le cose importanti nella vita siano sempre definibili. Spesso molte di esse non lo sono affatto. Per esempio, abbiamo un’idea assolutamente chiara di cosa sia l’amicizia, pur non disponendo delle condizioni necessarie e sufficienti per coglierne la natura. Un dibattito filosofico sulla sua definizione può facilmente terminare in un vicolo cieco, ma possiamo certamente discuterne in modo molto ragionevole e avere intuizioni preziose sull’etica dell’amicizia, sulla sua natura online, nonché sui suoi vantaggi o svantaggi. Lo stesso vale per “essere in buona salute” o “essere innamorati”. È così perché “la riconosciamo quando la vediamo”, per usare una frase diventata celebre nel 1964 (un anno piuttosto speciale, a quanto pare). Come il lettore probabilmente sa, questa espressione è stata utilizzata dal giudice della Corte Suprema degli Stati Uniti, Potter Stewart, nella decisione del caso Jacobellis v. Ohio, che verteva su ciò che poteva essere considerato osceno. Spiegando perché avesse deciso che il materiale in esame potesse essere considerato alla stregua di un discorso protetto dal Primo emendamento, da non reputarsi osceno e pertanto da non censurare, si era espresso in questi termini:

			Oggi non cercherò di definire ulteriormente i tipi di materiale che, a quanto mi risulta, devono essere inclusi in tale sintetica descrizione [pornografia hard-core], e forse non potrei mai riuscire a farlo in modo intellegibile. Ma la riconosco quando la vedo, e il film oggetto di questo caso non vi rientra. (378 Stati Uniti, p. 197; giudice Stewart, opinione concorrente [corsivo mio])

			L’amicizia, l’ia e molte altre cose nella vita sono come la pornografia: non sono definibili (nel senso stretto in cui l’acqua è definibile e definita come h2o) ma le riconosciamo quando le incontriamo. Ciò va bene nella quotidianità. Tuttavia riconosco che l’assenza di una definizione sia un po’ sospetta perché, nella scienza, anche cose insignificanti dovrebbero essere definibili con precisione, soprattutto dopo lunghi decenni di dibattiti. La conclusione è che probabilmente ia non è un termine scientifico, come “triangolo”, “pianeta” o “mammifero”, ma un’espressione generica, proprio come amicizia o pornografia. È una scorciatoia, usata per riferirsi approssimativamente a diverse discipline, servizi, prodotti tecnoscientifici talora solo genericamente correlati. L’ia è una famiglia in cui la somiglianza, e talvolta solo per pochi tratti, è il criterio di appartenenza.

			Alcuni membri della famiglia ia sono illustrati nella Figura 2.1.



			La mappa è utile ma alquanto controversa. Come mi ha fatto notare David Watson, sembra strano dedicare lo stesso spazio all’ia simbolica e probabilistica (“Sarebbe come tenere un corso di fisica nucleare che dedichi tanto tempo a Democrito quanto alla meccanica quantistica”); è fonte di confusione includere il deep learning (l’apprendimento profondo) in una sezione a sé stante distinta dalla visione artificiale e dall’elaborazione del linguaggio naturale, quando queste ultime due applicazioni sono condotte quasi esclusivamente, oggi, utilizzando il deep learning. Non è chiaro il motivo per cui le reti neurali vengono inserite in una colonna relativa all’apprendimento non supervisionato, quando sono più notoriamente associate a problemi supervisionati. Molti importanti metodi non supervisionati (come, per esempio, raggruppamento, proiezione, rilevamento di valori anomali) non sono nemmeno menzionati; e la terminologia del calcolo “sub-simbolico” non è standard, almeno se intende riferirsi a procedure di ottimizzazione come gli algoritmi evolutivi. David ha ragione. Ma mi piace ancora, perché aiuta a mettere a fuoco la questione ed è molto meglio di niente. Non mostro questa mappa perché è perfetta, ma perché, con tutti i suoi limiti, copre un bel po’ di territorio e mostra che anche due esperti possono facilmente essere in disaccordo pur appartenendo allo stesso ambito e condividendo lo stesso approccio. Come due esperti di etica che non siano d’accordo su che cosa sia veramente l’amicizia o la pornografia.

			Questo ci conduce alla domanda seguente: se l’ia non può essere definita elencando condizioni necessarie e sufficienti incontroverse, c’è qualcosa che queste o simili discipline, ambiti, paradigmi, metodi, tecniche o tecnologie riconducibili nell’alveo dell’ia hanno in comune? Direi di sì, e risale al 1955. È la definizione che abbiamo incontrato nel capitolo precedente.





2.2 ia come controfattuale


			Iniziamo richiamando tale definizione, così non occorre ricercarla:

			Per il presente scopo il problema dell’intelligenza artificiale è quello di far sì che una macchina agisca con modalità che sarebbero definite intelligenti se un essere umano si comportasse allo stesso modo. (Citazione dalla riedizione del 2006 in McCarthy, Minsky, Rochester, Shannon, 2006)

			Questo è chiaramente un controfattuale e non ha nulla a che vedere con il pensiero ma esclusivamente con il comportamento: se un essere umano si comportasse in quel modo, quel comportamento sarebbe definito intelligente. Non significa che la macchina sia intelligente o che addirittura stia pensando. La comprensione controfattuale dell’ia è alla base anche del test di Turing (Turing, 1950) e del premio Loebner (Floridi, Taddeo, Turilli, 2009). Turing comprese molto bene che non vi era modo di rispondere alla domanda se una macchina fosse in grado di pensare, perché, come ammise, entrambi i termini sono privi di definizione scientifica:

			Propongo di considerare la domanda: “Possono le macchine pensare?”. Questa indagine dovrebbe iniziare definendo il significato dei termini “macchina” e “pensare”. […] La domanda originaria, “Possono le macchine pensare?”, credo sia troppo insensata per meritare di essere discussa. (Turing, 1950, pp. 433, 442, corsivo mio)

			Per questo, ha elaborato invece un test: un po’ come decidere che il miglior modo di valutare se qualcuno sia in grado di guidare consista nel verificare le sue prestazioni su strada. Nel caso di Turing, il test verifica la capacità di una macchina di rispondere a domande in modo tale che il risultato sia indistinguibile, quanto alla sua fonte, dal risultato di un agente umano che si adopera nello stesso compito (ibidem). Questo è perfettamente ragionevole, ma riflettiamo su un punto: solo perché una lavastoviglie pulisce bene i piatti o meglio di quanto lo faccia io, non significa che li pulisca come me o che abbia bisogno di intelligenza (non importa se del mio tipo o di qualsiasi altro) nello svolgimento del suo compito. Ciò equivarrebbe a sostenere che, poiché (a) il fiume raggiunge il mare seguendo il miglior percorso possibile, rimuovendo gli ostacoli sul suo cammino; e (b) se ciò dovesse essere fatto da un essere umano, lo considereremmo un comportamento intelligente; allora (c) il comportamento del fiume è intelligente. Quest’ultimo scenario è fallace e sa di superstizione. Il solo aspetto rilevante è eseguire un compito con successo in modo tale che il risultato sia altrettanto buono o migliore di quello che l’intelligenza umana sarebbe stata in grado di ottenere. Il come non è in discussione, lo è solo il risultato. Questo punto è cruciale, logicamente e storicamente.

			Logicamente, persino l’identità (lasciando da parte la somiglianza) di un risultato non dice nulla circa l’identità dei processi che l’hanno generato e delle fonti dei processi stessi. Questo sembra indiscutibile, eppure Turing la pensava diversamente. In una trasmissione radiofonica della bbc, ha affermato:

			[…] [1.20] l’opinione che io stesso sostengo, che non è del tutto irragionevole descrivere i computer digitali come cervelli. […] [4.36-4.47] Se ora una macchina in particolare può essere descritta come un cervello non ci resta che programmare il nostro computer digitale per imitarlo e sarà anche un cervello. Se si accetta l’idea che il vero cervello, che si trova negli animali e in particolare negli esseri umani, sia una sorta di macchina, ne consegue che il nostro computer digitale opportunamente programmato si comporterà come un cervello. (Turing, 1951)

			Proviamo a dimenticare per un momento che la persona che sta parlando è un genio. Immaginiamo che sia scritto in un articolo di giornale, come il Daily Mail. Prendiamo la penna rossa. Prima cosa da sottolineare: “non è del tutto irragionevole”. Anche considerando l’inglese dell’epoca, questa doppia negazione costituisce la più debole premessa possibile che si possa immaginare per sostenere una qualsiasi tesi. In base allo stesso standard, “non del tutto irragionevole” è anche descrivere gli animali come automi che non possono ragionare o provare dolore, come ha sostenuto un altro genio, Cartesio. Naturalmente, pensiamo che Cartesio avesse torto. La successiva parola da sottolineare è “descrivere”. Qualsiasi cosa può essere descritta nei termini di qualsiasi altra cosa, a un certo livello di astrazione. La questione è se tale livello di astrazione sia quello corretto. Non è del tutto irragionevole descrivere una partita a scacchi come una battaglia tra due nemici. E se si tratta della finale del campionato mondiale del 1972 tra Spassky e Fischer, la descrizione potrebbe persino ricordare che le battaglie appartengono alle guerre, in questo caso la Guerra fredda. Ma se cerchiamo in essa sofferenza e morte, violenza e sangue, rimarremo delusi. È un “tiiipo” (le tre “i” sono importanti) di battaglia, ma non in senso letterale. Pertanto, consideriamo l’ipotesi che il cervello sia una macchina. Ciò pare o banalmente vero dal punto di vista metaforico o fattualmente sbagliato dal punto di vista non metaforico, cioè in termini di osservazione scientifica. Nell’articolo, Turing aveva ammesso che abbiamo una vaga idea di ciò che possiamo considerare macchina. Anche una burocrazia può essere descritta “non del tutto irragionevolmente” come una macchina, per dirla con un altro genio, Kafka. Un bollitore, un frigorifero, un treno, un computer, un tostapane, una lavastoviglie… sono tutte macchine, in un senso o nell’altro. Anche il nostro corpo è una macchina. Così il nostro cuore. Perché non il cervello? Assolutamente sì. Il vero problema è l’enorme margine di manovra. Perché tutto dipende da quanto rigorosamente interpretiamo “tipo di”. Se quasi tutto può qualificarsi come un tipo di macchina, allora sicuramente anche un cervello è un tipo di macchina. Ma non stiamo inserendo il piolo giusto nel foro giusto, abbiamo solo reso il foro così grande che qualsiasi piolo vi si adatterà. Finora tutto questo ha riguardato un modo di esprimersi approssimativo, ma il problema che segue è un vero errore. Se A e B producono entrambi lo stesso risultato R dato lo stesso input I (per usare l’esempio preferito di Turing, cioè fornire alle stesse domande risposte identiche o equivalenti per qualità, e comunque indistinguibili quanto alla loro fonte; cosa che, non dimentichiamolo, implica già la riduzione del cervello a un computer), anche in tal caso ciò non significa che (a) si comportino entrambi allo stesso modo o (b) siano la stessa cosa. Immaginiamo che Alice si rechi a casa di Roberto e trovi dei piatti puliti sul tavolo. Non sarebbe in grado di evincere dal risultato (piatti puliti) se sia stato utilizzato lo stesso procedimento (lavaggio meccanico o manuale), né quale agente li abbia effettivamente puliti (la lavastoviglie o Roberto), o quali capacità siano state adoperate per ottenere il risultato. Tuttavia, sarebbe sbagliato inferire da tale irreversibilità e opacità che Bob e la lavastoviglie siano, dunque, la stessa cosa o si comportino allo stesso modo, anche solo in termini di capacità di lavare i piatti. Il fatto è che a Alice probabilmente non importerebbe nulla, purché i piatti siano puliti.

			Temo che Turing fosse, metaforicamente, nel giusto ma, sostanzialmente, nel torto, o forse è stata la bbc che ha richiesto un abbassamento del livello di precisione (ci sono stato e l’ho fatto, per cui parlo da peccatore): non è del tutto irragionevole descrivere computer digitali come cervelli, o addirittura viceversa; ma non è utile perché tutto resta troppo metaforico e vago, e una volta che iniziamo a essere precisi le somiglianze scompaiono e tutte le differenze rilevanti diventano sempre più evidenti. Cervello e computer non sono la stessa cosa e non si comportano allo stesso modo. Entrambe le tesi non sono confutate dalla posizione di Turing. Tuttavia, ciò non costituisce un argomento positivo a loro favore. Temo che sostenere interamente tali tesi richiederebbe un diverso tipo di libro.1 In questo contesto, voglio solo rendere esplicita la prospettiva con cui può essere interpretata in modo più accurato la seconda parte di questo libro. I lettori che preferiscono pensarla come Turing non saranno d’accordo con me, ma spero che siano comunque in grado di convenire sulla seguente distinzione. Storicamente, la rappresentazione controfattuale dell’ia contiene i germi di un approccio ingegneristico, opposto a quello cognitivo, all’ia, come vedremo nel prossimo paragrafo.





2.3 Le due anime dell’ia: ingegneristica e cognitiva


			È un fatto risaputo, anche se talora sottostimato, che le ricerche sull’ia aspirino sia a riprodurre i risultati o l’esito positivo del nostro comportamento intelligente (o almeno di qualche tipo di comportamento animale) con mezzi non biologici, sia a produrre l’equivalente non biologico della nostra intelligenza, cioè la fonte di tale comportamento.

			Da un lato, come settore dell’ingegneria interessata alla riproduzione del comportamento intelligente, l’ia ha avuto un successo sbalorditivo, ben oltre le più rosee aspettative. Prendiamo un esempio piuttosto celebre, anche se un po’ vecchio. Deep Q-network (un sistema di algoritmi software) appartiene a questo genere di ia riproduttiva. Nel 2015, Deep Q-network ha imparato a giocare a 49 classici videogiochi vintage Atari da zero, basandosi solo sui dati relativi ai pixel su uno schermo e sul metodo di punteggio (Mnih, Kavukcuoglu, Silver et al., 2015). Impressionante? Sì, da un punto di vista ingegneristico. Non molto, per ciò che concerne avvicinarsi a una vera forma di intelligenza artificiale. In fin dei conti, ci vuole meno “intelligenza” per vincere giocando a Space Invaders o Breakout che per essere un campione di scacchi. Per questo era solo una questione di tempo prima che alcuni esseri umani ingegnosi trovassero il modo di rendere una macchina di Turing abbastanza intelligente da giocare con abilità ai giochi Atari. Oggi, facciamo sempre più affidamento su applicazioni basate sull’ia (definite talvolta come tecnologie smart, una terminologia che userò anch’io, sebbene l’espressione abbia una portata più ampia) per eseguire compiti che sarebbero semplicemente impossibili per un’intelligenza umana non aiutata o non aumentata. L’ia riproduttiva ottiene regolarmente risultati migliori e sostituisce l’intelligenza umana in un numero sempre maggiore di contesti. Per le tecnologie smart il cielo è il limite e Deep Q-network ha solo cancellato un altro ambito in cui gli esseri umani erano migliori delle macchine. La prossima volta che sperimenteremo un atterraggio un po’ accidentato, ricordiamoci che probabilmente ciò è accaduto perché al comando c’era un pilota e non un computer. Ciò non significa che un drone autonomo guidato dall’ia voli come un uccello. Il celebre commento attribuito a Edsger Wybe Dijkstra per cui “la questione se un computer possa pensare non è più interessante della questione se un sottomarino possa nuotare” illustra bene l’approccio pratico condiviso dall’ia riproduttiva.

			D’altro lato, come settore della scienza cognitiva interessata alla produzione di intelligenza, l’ia rimane fantascienza ed è stata una triste delusione. L’ia produttiva non si limita a prestazioni inferiori rispetto all’intelligenza umana; non ha ancora preso parte alla competizione. Il fatto che Watson, il sistema ibm in grado di rispondere alle domande poste in linguaggio naturale, possa sconfiggere i suoi avversari umani giocando a Jeopardy! dice di più sugli ingegneri umani, le loro incredibili capacità e competenze, e il gioco stesso, che sull’intelligenza biologica di qualsiasi tipo, inclusa quella di un topo. Il lettore non è costretto a credermi. John McCarthy – che, come detto, ha coniato l’espressione “intelligenza artificiale” ed era un autentico sostenitore della possibilità di creare una forma di ia nel senso cognitivo osservato poco sopra2 – aveva ben compreso tutto ciò. Le sue osservazioni deluse sulla vittoria di Deep Blue contro il campione del mondo Garry Kasparov nel 1997 (McCarthy, 1997) sono sintomatiche del tipo di ia produttiva e cognitiva che disapprova l’ia riproduttiva e ingegneristica. Questo è il motivo per cui non ha mai smesso di lamentarsi del fatto che il gioco degli scacchi fosse considerato un caso di autentica ia. Aveva ragione. Non lo è. Ma aveva torto nel ritenere che non fosse una buona alternativa. Lo stesso vale per AlphaGo (di cui dirò di più a breve).

			Le due anime dell’ia, quella ingegneristica (tecnologie intelligenti) e quella cognitiva (tecnologie realmente intelligenti), hanno spesso ingaggiato lotte fratricide per il primato intellettuale, il potere accademico e le risorse finanziarie. Ciò è in parte dovuto al fatto che entrambe rivendicano antenati comuni e un’unica eredità intellettuale: un evento fondante, la già citata conferenza estiva di ricerca di Dartmouth sull’ia nel 1956, e un padre fondatore, Turing, con la sua macchina, i suoi limiti computazionali e il suo celebre test. Non è di aiuto il fatto che una simulazione possa essere usata per verificare tanto se la fonte simulata (cioè l’intelligenza umana) sia stata prodotta, quanto se il comportamento o la prestazione della fonte che si hanno di mira (vale a dire, ciò che l’intelligenza umana consegue) siano stati riprodotti o addirittura superati. Le due anime dell’ia sono state denominate in modo diverso e non sempre coerente. Talora le distinzioni tra ia debole o forte, tradizionale o nuova, sono state utilizzate per cogliere tale differenza. Oggi, l’espressione intelligenza artificiale generale sembra più di moda, invece di quella di ia completa. In passato, ho preferito utilizzare la distinzione meno marcata tra ia leggera e forte (Floridi, 1999). Non importa. Il disallineamento dei loro caratteri, obiettivi e risultati ha causato infinite e per lo più inutili diatribe. I difensori dell’ia indicano i risultati impressionanti dell’ia riproduttiva e ingegneristica, che è un’ia davvero debole o leggera in termini di obiettivi. Mentre i detrattori dell’ia indicano i risultati deludenti dell’ia produttiva e cognitiva, che è un’ia veramente forte o generale in termini di obiettivi. Molte delle attuali speculazioni sulla cosiddetta questione della singolarità (di cui dirò di più nel decimo capitolo) e sugli effetti catastrofici di alcune presunte superintelligenze hanno le loro radici in tale confusione. A volte non posso fare a meno di sospettare che ciò sia stato fatto intenzionalmente, non per motivi maliziosi, ma perché la confusione è così intellettualmente piacevole. Alcune persone amano le diatribe inutili.

			I grandi campioni sanno come concludere la carriera al culmine del loro successo. Nel 2017, DeepMind, il laboratorio di ia di Alphabet (precedentemente Google), ha deciso che il suo programma per computer AlphaGo non si sarebbe più concentrato sulla vittoria del gioco Go. Invece, il ceo di DeepMind, Demis Hassabis, e il capo programmatore di AlphaGo, David Silver, hanno rivelato che l’attenzione sarebbe stata rivolta a

			sviluppare algoritmi generali avanzati che un giorno potrebbero aiutare gli scienziati nell’affrontare alcuni dei nostri problemi più complessi, come trovare nuove cure per le malattie, ridurre drasticamente il consumo di energia o inventare nuovi materiali rivoluzionari.3

			L’ambizione era giustificata. Tre anni dopo, nel 2020, il sistema di ia di DeepMind, AlphaFold 2, ha risolto il “problema del ripiegamento delle proteine”, una grande sfida della biologia che aveva tormentato gli scienziati per cinquant’anni:

			La capacità di prevedere con precisione le strutture proteiche a partire dalla loro sequenza di amminoacidi sarebbe un enorme vantaggio per le scienze della vita e la medicina. Velocizzerebbe notevolmente gli sforzi per comprendere gli elementi costitutivi delle cellule e per consentire una scoperta di farmaci più rapida e avanzata. (Callaway, 2020)

			L’ia porterà a nuove scoperte e svolte potenzialmente considerevoli, specialmente nelle mani di persone brillanti e riflessive. Sosterrà, inoltre, la gestione e il controllo di sistemi sempre più complessi. Tuttavia, tutti questi straordinari sviluppi potranno realizzarsi più facilmente se viene eliminato un malinteso. Abbiamo visto che l’ia di successo non riguarda la produzione ma la sostituzione dell’intelligenza umana. Una lavastoviglie non pulisce i piatti come lo facciamo noi, ma alla fine del processo i suoi piatti puliti sono indistinguibili dai nostri, anzi possono essere anche più puliti (efficacia), utilizzando meno risorse (efficienza). Lo stesso vale per l’ia. AlphaGo non ha giocato come il grande maestro cinese di Go, Ke Jie, numero uno al mondo, ma ha vinto comunque. Parimenti, le automobili autonome non sono auto guidate da robot umanoidi seduti al volante al nostro posto, ma sono modi per reinventare completamente l’auto e il suo ambiente. Nell’ia, è il risultato che conta, non se l’agente o il suo comportamento sia intelligente. Per questo, l’ia non concerne la capacità di riprodurre l’intelligenza umana, ma in realtà la capacità di farne a meno. Le macchine attuali hanno l’intelligenza di un tostapane e non abbiamo davvero la più pallida idea di come fare un passo avanti (Floridi, Taddeo, Turilli, 2009). Quando l’avviso “stampante non trovata” è visualizzato sullo schermo del computer, può risultare fastidioso ma non sorprendente, nonostante la stampante in questione sia effettivamente lì, accanto al computer. Ma, soprattutto, ciò non è un ostacolo perché gli artefatti possono essere smart senza essere intelligenti, e questo è il risultato davvero straordinario dell’ia riproduttiva, che è la continuazione riuscita dell’intelligenza umana con altri mezzi, per parafrasare Carl von Clausewitz.

			Oggi, l’ia scinde la risoluzione efficace dei problemi e l’esecuzione corretta dei compiti dal comportamento intelligente, ed è proprio grazie a tale scissione che può incessantemente colonizzare lo spazio sterminato di problemi e compiti, ogni volta che questi possono essere conseguiti senza comprensione, consapevolezza, acume, sensibilità, preoccupazioni, sensazioni, intuizioni, semantica, esperienza, bio-incorporazione, significato, persino saggezza e ogni altro ingrediente che contribuisca a creare l’intelligenza umana. In breve, è proprio quando smettiamo di cercare di produrre intelligenza umana che possiamo sostituirla con successo in un numero crescente di compiti. Se avessimo aspettato anche solo una scintilla di vera intelligenza artificiale, del tipo che troviamo in Star Wars, AlphaGo non sarebbe mai diventato più abile di chiunque altro nel giocare a Go. In effetti, avrei comunque vinto giocando a scacchi contro il mio smartphone.

			Se si comprende appieno il senso di questa scissione, si prospettano tre ovvi sviluppi. Li discuterò più in dettaglio nel prossimo capitolo, ma possono essere delineati qui. L’ia dovrebbe smettere di vincere i giochi e imparare a ludicizzare. Man mano che l’ia migliora nel giocare, tutto ciò che può essere trasformato in gioco rientra nel suo ambito. Se fossi DeepMind, assumerei un team di esperti che progettano giochi. In secondo luogo, in contesti ludificati, l’ia sarà abbinata soltanto all’ia e le sue interazioni interne potrebbero diventare troppo complesse per poter essere integralmente comprese da ammiratori esterni come noi. Potremmo allietarci nel guardare l’ia giocare così come ci allietiamo nell’ascoltare Bach. E, infine, possiamo aspettarci che l’intelligenza umana abbia un ruolo diverso ovunque l’ia sia il giocatore migliore. Perché si tratterà meno di risolvere alcuni problemi e più di decidere quali problemi valga la pena di risolvere, perché, per quali finalità, e con quali costi, trade-off e conseguenze accettabili.





2.4 ia: un divorzio riuscito nell’infosfera


			La classica definizione controfattuale e l’interpretazione dell’ia come divorzio, e non come matrimonio, tra l’agire e l’intelligenza, consentono di concepire l’ia come una risorsa crescente di capacità di agire interattiva, autonoma e spesso autoapprendente (nel senso dell’apprendimento automatico, di cui alla Figura 2.1), che può affrontare un numero sempre più elevato di problemi e attività che richiederebbero altrimenti l’intelligenza e l’intervento umani (e possibilmente una quantità illimitata di tempo) per essere eseguiti con successo. In breve, l’ia è definita sulla base di risultati e azioni ingegnerizzati e quindi, nel resto di questo libro, tratterò l’ia come una riserva di capacità di agire a portata di mano. Questa definizione è sufficientemente generale per cogliere i molti modi in cui l’ia è discussa in letteratura. Tuttavia, prima di esaminare il possibile sviluppo dell’ia nel prossimo capitolo, è necessario affrontare in questo contesto un’ultima questione. Il divorzio tra l’agire e l’intelligenza è problematico, in quanto è una delle fonti delle sfide etiche poste dall’ia, dal momento che gli agenti artificiali sono

			sufficientemente informati, “smart”, autonomi e in grado di compiere azioni moralmente rilevanti indipendentemente dagli esseri umani che li hanno creati […]. (Floridi, Sanders, 2004)

			Tuttavia, bisogna chiedersi, in primo luogo, come possa un divorzio tra l’agire e l’intelligenza avere successo in termini di efficacia. Non è, dunque, necessaria l’intelligenza perché un qualsiasi tipo di comportamento abbia successo? Ho già offerto l’esempio del fiume per illustrare una possibile fallacia. Occorre, però, fornire una spiegazione più articolata per rispondere alla domanda precedente, che può richiedere uno sforzo supplementare poiché costituisce un’obiezione ragionevole. Il successo dell’ia è in gran parte dovuto al fatto che stiamo costruendo un ambiente adattato a essa, in cui le tecnologie intelligenti si trovano a casa mentre noi siamo più simili a sommozzatori. È il mondo che si sta adattando all’ia e non viceversa. Vediamo cosa significa.

			L’ia non può essere ridotta a una “scienza della natura” o a una “scienza della cultura” (Ganascia, 2010) perché è una “scienza dell’artificiale”, per dirla con Herbert Simon (1996). In quanto tale, l’ia persegue un approccio al mondo che non è descrittivo né prescrittivo: indaga, piuttosto, le condizioni vincolanti che rendono possibile costruire e incorporare artefatti nel mondo, che sono in grado di interagire con esso con successo. In altre parole, inscrive il mondo, poiché questi artefatti sono nuovi pezzi di codice logico-matematico, cioè nuovi testi, scritti nel libro matematico della natura di Galileo:

			La filosofia è scritta in questo grande libro – intendo l’universo – che è continuamente aperto al nostro sguardo, ma non può essere compresa se non si impara prima a comprendere la lingua in cui è scritta. È scritto nel linguaggio della matematica e i suoi caratteri sono triangoli, cerchi e altre figure geometriche, senza le quali è umanamente impossibile comprenderne una sola parola; senza questi, si vaga in un labirinto oscuro. (Galileo, Il Saggiatore, 1623).

			Fino a poco tempo fa, l’impressione diffusa era che tale processo di addizione al libro matematico della natura (iscrizione) richiedesse la fattibilità di un’ia produttiva e cognitiva. Dopotutto, sviluppare anche una forma rudimentale di intelligenza non biologica può sembrare non solo il migliore ma forse l’unico modo per implementare tecnologie sufficientemente adattive e flessibili per affrontare efficacemente un ambiente complesso, in continua evoluzione e spesso imprevedibile, quando non ostile. Ciò che Cartesio, per esempio, riconosceva come un segno essenziale di intelligenza – la capacità di adattarsi a circostanze diverse e sfruttarle a proprio vantaggio – sarebbe una caratteristica inestimabile di qualsiasi dispositivo che cercasse di essere qualcosa di più che semplicemente smart.

			Questa impressione non è sbagliata, ma è sviante. Abbiamo osservato che il digitale sta re-ontologizzando la natura stessa (e quindi il significato) del nostro ambiente, l’infosfera, la quale al contempo sta progressivamente diventando il mondo in cui viviamo. Quindi, mentre stavamo perseguendo senza successo l’iscrizione dell’ia produttiva nel mondo, stavamo effettivamente modificando (re-ontologizzando) il mondo per adattarlo all’ia ingegneristica e riproduttiva. Il mondo sta diventando un’infosfera sempre meglio adattata alle delimitate capacità dell’ia (Floridi, 2003, 2014a). Per comprendere meglio come ciò accade, consideriamo brevemente l’industria automobilistica. Tale settore è stato in prima linea nella rivoluzione digitale e nell’ia fin dal principio, prima con la robotica industriale e ora con le automobili a guida autonoma basata sull’ia. I due fenomeni sono correlati e possono anche insegnarci una lezione molto importante allorché si tratti di comprendere come gli agenti umani e artificiali coabiteranno nei nostri ambienti.

			Consideriamo prima la robotica industriale: per esempio, un robot che dipinge il componente di un veicolo in una fabbrica. Lo spazio tridimensionale che definisce i confini entro i quali tale robot può lavorare con successo è definito l’involucro del robot. Alcune delle nostre tecnologie, come le lavastoviglie o le lavatrici, assolvono i loro compiti perché i loro ambienti sono strutturati (avvolti) attorno alle capacità elementari del robot al loro interno. Lo stesso vale, per esempio, per gli scaffali robotici nei magazzini di Amazon che sono “avvolti” attorno a loro. È l’ambiente che è progettato in modo tale da essere compatibile con i robot, non il contrario. Pertanto, non costruiamo droidi come il c-3po di Star Wars per lavare i piatti nel lavello esattamente come lo faremmo noi. Invece, avvolgiamo microambienti attorno a robot semplici per adattarli a essi e sfruttare le loro capacità limitate, in modo tale da ottenere comunque il risultato desiderato.

			L’avvolgimento era un fenomeno a sé stante (si poteva acquistare il robot con l’involucro richiesto, come una lavastoviglie o una lavatrice) o implementato all’interno delle mura di edifici industriali, attentamente ritagliati sui loro abitanti artificiali. Al giorno d’oggi, la pratica di avvolgere l’ambiente in un’infosfera adatta all’ia ha iniziato a pervadere tutti gli aspetti della realtà e sta prendendo piede quotidianamente ovunque, in casa, in ufficio e per strada. Quando parliamo di città smart, facciamo riferimento anche al fatto che stiamo trasformando gli habitat sociali in luoghi in cui i robot possono operare con successo. Da decenni avvolgiamo il mondo intorno alle tecnologie digitali in modo invisibile e senza rendercene interamente conto. Come vedremo nel terzo capitolo, il futuro dell’ia risiede anche in un maggiore avvolgimento, per esempio, in termini di 5G e Internet delle Cose (IoT), e in una crescente dimensione onlife, vale a dire nel fatto che siamo tutti costantemente connessi e trascorriamo sempre più tempo nell’infosfera, mentre tutte le nuove informazioni nascono sempre più digitali. Tornando all’industria automobilistica, le auto a guida autonoma diventeranno una merce il giorno in cui potremo avvolgere l’ambiente che le circonda.

			Negli anni Quaranta e Cinquanta, il computer era una stanza e vi camminavamo dentro per lavorare con esso e al suo interno. Programmare significava usare un cacciavite. L’interazione uomo-computer era una relazione somatica o fisica. Ricordiamoci del computer mostrato in Robinson Crusoe su Marte. Negli anni Settanta, siamo usciti dal computer, per sederci di fronte a esso. L’interazione uomo-computer divenne una relazione semantica, resa in seguito più facile dal sistema operativo per dischi, dalle righe di testo, dall’interfaccia utente grafica e dalle icone. Oggi, siamo entrati di nuovo nel computer, sotto forma di un’intera infosfera che ci circonda, spesso in modo impercettibile. Stiamo costruendo l’involucro definitivo in cui le interazioni uomo-computer sono diventate di nuovo somatiche, attraverso i touch screen, i comandi vocali, i dispositivi di ascolto, le applicazioni sensibili ai gesti, i dati di geolocalizzazione e così via. Come al solito, intrattenimento, sanità e applicazioni militari stanno guidando l’innovazione, ma il resto del mondo non è molto indietro. Se droni, veicoli a guida autonoma, tagliaerba robotici, ma anche bot e algoritmi di ogni tipo possono spostarsi “in giro” e interagire con i nostri ambienti con problemi decrescenti, non è perché è stata finalmente realizzata l’ia produttiva e cognitiva (di tipo hollywoodiano), ma perché ciò che sta “intorno” e gli ambienti con cui i nostri artefatti ingegnerizzati devono negoziare sono diventati sempre più adattati all’ia riproduttiva e ingegnerizzata e alle sue limitate capacità. In una tale infosfera adattata all’ia, l’assunto di base è che un agente può essere artificiale: questo è il motivo per cui ci viene chiesto regolarmente di dimostrare che non siamo robot, cliccando sul cosiddetto captcha, il test di Turing pubblico e completamente automatico, per distinguere computer e umani. Il test è rappresentato da stringhe di lettere leggermente alterate, eventualmente mescolate con altri segni grafici, che dobbiamo decifrare per dimostrare che siamo un umano e non un agente artificiale, per esempio, quando ci registriamo per un nuovo account online. Si tratta di un test banale per un essere umano ma realmente insormontabile per l’ia: ecco quanti pochi progressi ci sono stati nell’area cognitiva di produzione dell’intelligenza non biologica.

			Ogni giorno assistiamo all’incremento di potenza di calcolo, di dati, dispositivi (IoT), sensori, tag, satelliti, attuatori, servizi digitali, esseri umani connessi che vivono sempre di più onlife: in una parola, a un maggiore avvolgimento. Un numero crescente di lavori e attività sta diventando di natura digitale: giocare, educare, uscire con qualcuno, incontrarsi, litigare, prendersi cura, spettegolare, fare pubblicità. Facciamo tutto questo e molto di più in un’infosfera avvolta in cui siamo più ospitati analogici che ospiti digitali. Non c’è da stupirsi che i nostri agenti artificiali si comportino sempre meglio. È il loro ambiente. Come vedremo nella seconda parte, questa profonda trasformazione ontologica solleva importanti sfide etiche.





2.5 L’uso umano degli esseri umani e delle interfacce


			Avvolgere il mondo trasformando un ambiente ostile in un’infosfera adattata digitalmente significa che condivideremo i nostri habitat non solo con forze e fonti di azione naturali, animali e sociali, ma anche e talvolta principalmente con agenti artificiali. Questo non vuol dire che un vero agire artificiale di tipo intelligente sia prossimo. Non disponiamo di macchine competenti dal punto di vista semantico e realmente intelligenti che comprendono le cose, si preoccupano per esse, hanno preferenze o si appassionano a qualcosa. Abbiamo strumenti statistici così sofisticati che tecnologie puramente sintattiche possono aggirare i problemi di significato, pertinenza, comprensione, verità, intelligenza, intuizione, esperienza e così via, e fornire comunque ciò di cui abbiamo bisogno: una traduzione, la giusta immagine di un luogo, il ristorante preferito, un libro interessante, un biglietto a un prezzo migliore, un affare decisamente scontato, la canzone adatta alle nostre preferenze musicali, un film che ci piace, una soluzione più economica, una strategia più efficace, informazioni essenziali per nuovi progetti, il design per nuovi prodotti, la lungimiranza necessaria per anticipare i problemi, una migliore diagnosi, l’elemento inaspettato di cui non sapevamo neppure di aver bisogno, il supporto necessario per una scoperta scientifica o una cura medica e così via. Sono stupide come un vecchio frigorifero, eppure le nostre tecnologie smart giocano a scacchi, parcheggiano un’automobile o interpretano le scansioni mediche meglio di noi. La loro memoria (per ciò che concerne dati e algoritmi) supera l’intelligenza in un numero crescente e illimitato di compiti e problemi. Il cielo o meglio la nostra immaginazione su come sviluppare e distribuire tali tecnologie smart è il limite.

			Alcuni dei problemi che stiamo affrontando oggi, per esempio, nella sanità digitale o nei mercati finanziari, sorgono già in ambienti altamente avvolti in cui tutti i dati rilevanti (e talora gli unici disponibili) sono leggibili da macchine, cosicché decisioni e azioni possono essere compiute automaticamente da applicazioni e attuatori in grado di eseguire comandi e completare le corrispondenti procedure: dall’avvertire o esaminare un paziente all’acquistare o vendere obbligazioni. Gli esempi potrebbero facilmente moltiplicarsi. Le conseguenze dell’avvolgere il mondo per trasformarlo in un luogo adattato all’ia sono molte, e il resto del libro ne esplorerà alcune. Ma un esempio in particolare è molto significativo e ricco di conseguenze, e può essere discusso qui a titolo di conclusione: gli esseri umani possono diventare inavvertitamente parte del meccanismo. Questo è proprio ciò che Kant raccomandava di non fare mai: trattare gli esseri umani solo come mezzi anziché come fini. Eppure si sta già verificando, principalmente in due modi. Entrambi sono casi di un “uso umano degli esseri umani” (Wiener, 1954).

			In primo luogo, gli esseri umani stanno diventando nuovi mezzi di produzione digitale. Il punto è semplice: talvolta l’ia ha bisogno di capire e interpretare ciò che sta accadendo, per cui ha bisogno di motori semantici come noi per svolgere tale compito. Questa tendenza abbastanza recente è nota come computazione basata sull’umano. Un classico esempio è fornito da Amazon Mechanical Turk. Il nome deriva da un celebre automa capace di giocare a scacchi costruito da Wolfgang von Kempelen (1734-1804) alla fine del xviii secolo. L’automa divenne famoso battendo personaggi del calibro di Napoleone Bonaparte e Benjamin Franklin e ingaggiando una bella partita contro un campione come François-André Danican Philidor (1726-1795). Tuttavia, si trattava di un falso perché albergava al suo interno uno scomparto in cui si nascondeva un giocatore umano che ne controllava le operazioni meccaniche. Il Mechanical Turk adotta un trucco simile. Amazon lo descrive come un sistema di “intelligenza artificiale artificiale”. Il doppio “artificiale” è nell’originale. Si tratta di un servizio web di crowdsourcing che consente ai cosiddetti “richiedenti” di sfruttare l’intelligenza di lavoratori umani, noti come “fornitori” o, più informalmente, “turchi”, per realizzare compiti definiti hit (human intelligence tasks, compiti che richiedono intelligenza umana) che i computer non sono tuttora in grado di svolgere. Un richiedente pubblica un hit, come trascrivere registrazioni audio o taggare i contenuti negativi di un film (due esempi reali). I turchi possono sfogliare una lista, scegliere tra gli hit disponibili e completarli per una ricompensa stabilita dal richiedente. I richiedenti possono verificare se i turchi soddisfano alcune qualifiche specifiche prima di affidare loro un hit. Possono anche accettare o rifiutare il risultato inviato da un turco e questo incide sulla sua reputazione. “L’umano dentro” sta diventando il nuovo slogan. La formula vincente è semplice: macchina smart + intelligenza umana = sistema ingegnoso.

			Il secondo modo in cui gli esseri umani stanno diventando parte del meccanismo è come clienti influenzabili. Per il settore pubblicitario un cliente è un’interfaccia tra un fornitore e un conto bancario (più precisamente si dovrebbe parlare di “limite di credito”, che non coincide con il reddito disponibile, poiché i clienti possono spendere più di quanto hanno, per esempio utilizzando le loro carte di credito). Più il rapporto tra i due è fluido e privo di attriti e meglio è: per questo conviene manipolare l’interfaccia. Per manipolarla, il settore pubblicitario ha bisogno di avere quante più informazioni possibili sul cliente-interfaccia. Tuttavia, tali informazioni non possono essere ottenute a meno di fornire qualcosa in cambio al cliente. Fanno così il loro ingresso i servizi “gratuiti” online. Questi sono le valute con cui sono “acquistate” le informazioni sui clienti-interfaccia. L’obiettivo finale è quindi quello di fornire quel tanto che basta di “servizi gratuiti”, che sono costosi, per ottenere tutte le informazioni sul cliente-interfaccia che sono necessarie per garantire quel grado di manipolazione che fornisce, in relazione all’offerta, un accesso illimitato e non vincolato al conto bancario. A causa delle regole della concorrenza, un tale obiettivo non può essere conseguito da un singolo operatore. Tuttavia, lo sforzo congiunto dell’industria della pubblicità e dei fornitori fa sì che i clienti siano sempre più concepiti come un mezzo in direzione di un fine: come le interfacce di conti bancari da spingere e tirare, indirizzare e allettare. L’ia gioca un ruolo cruciale in questo contesto, ritagliando, ottimizzando e decidendo molti processi attraverso sistemi di raccomandazione (Milano, Taddeo, Floridi, 2019, 2020), un tema che sarà ulteriormente discusso nel settimo capitolo.





2.6 Conclusione: chi si adatterà a chi?


			I sistemi di ia saranno esponenzialmente più utili ed efficaci nella misura in cui ci inoltreremo nel percorso di digitalizzazione dei nostri ambienti e di espansione dell’infosfera. L’avvolgimento è una tendenza robusta, cumulativa e che si perfeziona progressivamente. Non ha nulla a che fare con una singolarità futura, perché non si basa su qualche speculazione su super ia che conquisteranno il mondo nel prossimo futuro (vedi capitolo 10). Nessuno Spartaco artificiale guiderà una rivolta digitale. Tuttavia, avvolgere il mondo e in tal modo agevolare l’emergere di agenti artificiali e il successo dei loro comportamenti è un processo che solleva sfide concrete e urgenti, che discuterò nella seconda metà di questo libro. Qui, mi sia concesso di illustrarne alcune affidandomi a una parodia.

			Immaginiamo due persone, A e H. Sono sposate e desiderano davvero far funzionare la loro relazione. A, che fa sempre di più in casa, è persona inflessibile, testarda, insofferente agli errori e restia a cambiare. H invece è esattamente l’opposto, ma sta anche diventando progressivamente più pigra e dipendente da A. Il risultato è una situazione squilibrata, in cui A finisce per plasmare la relazione e distorcere i comportamenti di H praticamente, se non intenzionalmente. Se la relazione funziona, è perché è attentamente ritagliata attorno a A. La relazione diviene interpretabile nei termini della dialettica hegeliana Servo-Padrone (Hegel, 1807). Ora, le tecnologie smart svolgono il ruolo di A nell’analogia precedente, mentre i loro utenti umani sono chiaramente H. Il rischio che corriamo è che, avvolgendo il mondo, le nostre tecnologie e in particolare l’ia possano plasmare i nostri ambienti fisici e concettuali e costringerci a adattarci a essi perché questo è il modo più semplice o migliore, e talvolta l’unico, per far funzionare le cose. In fin dei conti, dato che l’ia è il coniuge stupido ma laborioso e l’umanità quello intelligente ma pigro, chi si adatterà a chi? Il lettore probabilmente ricorderà molti episodi della vita reale in cui qualcosa non poteva assolutamente essere fatto, o doveva essere fatto in modo scomodo o sciocco, perché quello era il solo modo per far fare al sistema computerizzato quello che doveva fare. “Il computer dice no”, come risponderebbe il personaggio Carol Beer nella commedia britannica Little Britain a qualsiasi richiesta del cliente. Ecco un esempio più concreto, per quanto banale. Il rischio è che potremmo finire per costruire case con pareti rotonde e mobili con gambe abbastanza alte per adattarle alle capacità di Roomba4 in modo molto più efficace. Vorrei certamente che la nostra casa fosse più adatta a Roomba. Abbiamo adattato il nostro giardino per assicurarci che Ambrogio, un robot tagliaerba, possa lavorare con successo. Gli esempi sono utili per illustrare non solo il rischio ma anche l’opportunità rappresentata dal potere delle tecnologie digitali di re-ontologizzare e avvolgere il mondo.

			Sono tanti i luoghi “rotondi” in cui viviamo, dagli igloo alle torri medievali, dai bovindo agli edifici pubblici dove gli angoli delle stanze sono arrotondati per motivi sanitari. Se trascorriamo la maggior parte del nostro tempo all’interno di luoghi squadrati, è per un altro insieme di tecnologie legate alla produzione in serie di mattoni e infrastrutture in calcestruzzo e alla facilità dei tagli diritti del materiale da costruzione. È la sega circolare meccanica che, paradossalmente, genera un mondo ad angolo retto. In entrambi i casi, luoghi squadrati e rotondi sono stati costruiti seguendo le tecnologie predominanti, piuttosto che in forza delle scelte dei loro potenziali abitanti. Sulla base di questo esempio, è facile percepire come l’opportunità rappresentata dal potere di re-ontologizzazione del digitale si presenti in tre forme: rifiuto, accettazione critica e design proattivo. Diventando più criticamente consapevoli del potere re-ontologizzante dell’ia e delle applicazioni smart, potremmo essere in grado di evitare le peggiori forme di distorsione (rifiuto) o almeno essere coscientemente tolleranti nei loro confronti (accettazione), specialmente quando non è importante (penso alla lunghezza delle gambe del divano in casa nostra compatibili con Roomba) o quando si tratta di una soluzione temporanea, in attesa di un design migliore. In quest’ultimo caso, essere in grado di immaginare come sarà il futuro e quali esigenze di adattamento saranno poste dall’ia e dal digitale più in generale ai loro utenti umani può aiutarci a escogitare soluzioni tecnologiche capaci di diminuire i loro costi antropologici e accrescere i loro benefici ambientali. In breve, il design umano intelligente (il gioco di parole è voluto) dovrebbe svolgere un ruolo maggiore nel plasmare il futuro delle nostre interazioni con gli artefatti smart attuali e futuri, e gli ambienti che condividiamo con loro. Dopotutto, è un segno di intelligenza far lavorare la stupidità per noi.5 È giunto il momento di esaminare il presente e il futuro prevedibile dell’ia.


3


			Futuro: lo sviluppo prevedibile dell’ia

			Sommario In precedenza, nel secondo capitolo, ho sostenuto che l’ia non dovrebbe essere interpretata come un matrimonio tra un’intelligenza di tipo biologico e artefatti ingegnerizzati, ma come un divorzio tra l’agire e l’intelligenza, cioè una scissione tra la capacità di affrontare problemi e compiti con successo in vista di uno scopo e l’esigenza di essere intelligenti nel farlo. Nel presente capitolo, utilizzo questa interpretazione dell’ia come una nuova forma di agire efficace ma non-intelligente per scrutare il suo futuro. Dopo una breve introduzione nel primo paragrafo, relativa alle difficoltà che investono qualsiasi esercizio di previsione, nei paragrafi secondo e terzo sostengo che i probabili sviluppi e le possibili sfide dell’ia dipenderanno dalla spinta verso i dati sintetici, dalla crescente traduzione di problemi difficili in problemi complessi, dalla tensione tra regole regolative e costitutive alla base delle aree di applicazione dell’ia, e quindi dal progressivo adattamento dell’ambiente all’ia piuttosto che dell’ia all’ambiente (ciò che ho definito nel capitolo precedente come avvolgimento). Nel quarto paragrafo, ritorno sull’importanza del design e della responsabilità nel produrre il corretto tipo di ia per trarre vantaggio dagli sviluppi di cui sopra. Nella conclusione, discuto le stagioni dell’ia, e in particolare i suoi inverni, per sottolineare le lezioni che avremmo dovuto apprendere, e possiamo ancora assimilare e applicare, per sfruttare al meglio questa straordinaria tecnologia. Il capitolo conclude la prima parte del libro, con una breve introduzione filosofica a passato, presente e futuro dell’ia.





3.1 Introduzione: scrutare nei semi del tempo


			L’ia ha dominato i titoli dei giornali recenti, con le sue promesse e sfide, i suoi rischi, successi e fallimenti. Quale futuro possiamo prevedere per l’ia? Naturalmente, le previsioni più accurate vengono effettuate con il senno di poi. Ma se qualche trucco non è accettabile, allora le persone in gamba scommettono su ciò che non è controverso o non può essere verificato. Sul lato non controverso, si può menzionare la maggiore pressione che proverrà dai legislatori per garantire che le applicazioni di ia siano in linea con aspettative socialmente accettabili. Si veda, per esempio, la normativa proposta in merito dall’Unione Europea (Proposta di regolamento del Parlamento e del Consiglio che stabilisce regole armonizzate sull’intelligenza artificiale [com (2021) 206 definitivo]). Sul lato non verificabile, alcune persone continueranno a vendere previsioni catastrofiche, con scenari distopici che hanno luogo in un futuro sufficientemente distante da garantire che tali Geremia non saranno più in circolazione per essere smentiti. La paura vende sempre bene, come i film sui vampiri o sugli zombi. Perciò, dobbiamo aspettarci di più. Ciò che è difficile, e potrebbe risultare piuttosto imbarazzante in seguito, è cercare di “scrutare nei semi del tempo, e dire quali chicchi germoglieranno, e quali no” (Macbeth, atto i, scena iii), cioè tentare di capire in che direzione è più probabile che l’ia stia andando o dove potrebbe non andare, dato il suo stato attuale, e su questa base provare a tracciare la mappa delle sfide etiche che bisognerebbe prendere sul serio. È ciò che cercherò di fare in questo capitolo, dove sarò cauto nell’individuare i percorsi più probabili, ma non così cauto da evitare ogni rischio di essere smentito da qualcuno che leggerà questo libro tra pochi anni.

			Parte della difficoltà è individuare il corretto livello di astrazione (Floridi, 2008b, 2008c), vale a dire identificare l’insieme di osservabili rilevanti (“i semi del tempo”) su cui concentrarsi, poiché sono tali osservabili che faranno la vera, significativa differenza. Nel nostro caso, sosterrò che i migliori osservabili sono forniti da un’analisi della natura:

			a)	dei dati utilizzati dall’ia per realizzare le proprie prestazioni;

			b)	dei problemi che è ragionevole attendersi che l’ia sia in grado di risolvere.1

			Esaminiamo prima (a) e poi (b) nei prossimi due paragrafi.





3.2 Dati storici, ibridi e sintetici e il bisogno di ludicizzazione


			Dicono che i dati siano il nuovo petrolio. Non la penso così. I dati sono durevoli, riutilizzabili, rapidamente trasportabili, facilmente duplicabili e simultaneamente condivisibili (non rivali) senza fine, mentre il petrolio non ha alcuna di queste proprietà. Disponiamo di enormi quantità di dati che continuano a crescere, laddove il petrolio è, invece, una risorsa limitata e in diminuzione. Il petrolio ha un prezzo ben distinto, mentre la monetizzazione degli stessi dati dipende quantomeno da chi li utilizza e per quale scopo, per non parlare di circostanze come quando, dove e così via. E tutto questo ancor prima di introdurre le questioni giuridiche ed etiche che emergono quando sono in gioco i dati personali, o l’intero dibattito sulla proprietà dei dati (“i miei dati” è un’espressione molto più simile alle “mie mani” che non al “mio petrolio”: Floridi, 2013). Dunque, l’analogia è a dir poco forzata. Ciò non significa che sia totalmente inutile. Perché è vero che i dati, come il petrolio, sono una risorsa preziosa e devono essere raffinati per estrarne il valore. In particolare, senza dati, gli algoritmi – inclusa l’ia – non vanno da nessuna parte, come un motore con un serbatoio vuoto. L’ia ha bisogno di dati per essere addestrata e pertanto di dati per applicare il suo addestramento. Naturalmente, l’ia può essere estremamente flessibile: sono i dati che determinano il suo ambito di applicazione e grado di successo. Per esempio, nel 2016 Google ha utilizzato il sistema di apprendimento automatico di DeepMind per ridurre il proprio consumo energetico:

			Poiché l’algoritmo è uno strumento concettuale che si applica a finalità diverse per comprendere dinamiche complesse, prevediamo di applicarlo ad altre sfide nell’ambiente dei data center e oltre nei prossimi mesi. Possibili applicazioni di questa tecnologia includono il miglioramento dell’efficienza di conversione delle centrali elettriche (ottenendo più energia dalla stessa unità di input), la riduzione dell’energia di produzione di semiconduttori e dell’utilizzo di acqua o l’ottimizzazione degli impianti di produzione per aumentare la produttività.2

			È noto che l’ia, intesa come Machine Learning (apprendimento automatico), apprende dai dati che riceve e migliora progressivamente i suoi risultati. Se mostriamo un grandissimo numero di foto di cani a una rete neurale, alla fine imparerà a riconoscere i cani in modo sempre migliore, compresi i cani che non ha mai visto prima. Per ottenere tale risultato, di solito sono necessarie enormi quantità di dati, e di regola quanti più sono i dati, tanto migliore è il risultato. Per esempio, in test recenti un team di ricercatori dell’Università della California a San Diego ha addestrato un sistema di ia su 101,6 milioni di punti dati di cartelle cliniche elettroniche (incluso il testo scritto da medici e i risultati dei test di laboratorio) sulla base di 1.362.559 visite di pazienti pediatrici in un importante centro medico a Guangzhou, in Cina. Una volta addestrato, il sistema ia è stato in grado di mostrare:

			[…] un’elevata accuratezza diagnostica su più sistemi di organi ed è paragonabile a pediatri esperti nella diagnosi di malattie infantili comuni. Il nostro studio fornisce una verifica di funzionamento per l’implementazione di un sistema basato su ia come strumento di ausilio per aiutare i medici ad affrontare grandi quantità di dati, incrementare le valutazioni diagnostiche e fornire supporto decisionale clinico in casi di incertezza o complessità diagnostica. Sebbene questo impatto possa risultare più evidente nelle aree in cui gli operatori sanitari sono relativamente carenti, è probabile che i vantaggi di un tale sistema di ia siano universali. (Liang, Tsui, Ni et al., 2019)

			Tuttavia, recentemente l’ia è talmente migliorata che, in taluni casi, si sta passando da un’enfasi sulla quantità di grandi masse di dati, a volte impropriamente chiamati Big Data (Floridi, 2012a), a un’enfasi sulla qualità di insiemi di dati ben curati. Per esempio, nel 2018, DeepMind, in collaborazione con l’ospedale Moorfields Eye di Londra, ha addestrato un sistema di ia per identificare sintomi di malattie degli occhi pericolose per la vista utilizzando i dati della tomografia a coerenza ottica, una tecnica di elaborazione di immagini che genera immagini 3D della parte posteriore dell’occhio. Alla fine, il team è riuscito a

			dimostrare una capacità di prestazione nel formulare raccomandazioni ai pazienti equivalente o superiore a quella degli esperti su una gamma di malattie retiniche pericolose per la vista a seguito di un addestramento su solo 14.884 scansioni. (De Fauw, Ledsam, Romera-Paredes et al., 2018, p. 1342, corsivo mio)

			Sottolineo “solo 14.884 scansioni” perché i “piccoli dati” di alta qualità costituiscono uno degli scenari futuri dell’ia. L’ia avrà maggiori possibilità di successo ogni volta che insiemi di dati ben curati, aggiornati e completamente affidabili saranno disponibili e accessibili per addestrare un sistema in un’area specifica di applicazione. Ciò è piuttosto ovvio e a stento rappresenta una nuova previsione. Ma è un passo avanti concreto, che ci aiuta a guardare più lontano, oltre la narrativa dei “Big Data”. Se la qualità è importante, la provenienza è fondamentale. Da dove provengono i dati? Nell’esempio precedente, sono stati forniti dall’ospedale. Tali dati sono talvolta noti come storici, autentici o provenienti dalla vita reale (d’ora in poi li chiamerò semplicemente storici). Ma sappiamo anche che l’ia può generare i propri dati. Non parlo di metadati o dati secondari sui loro utilizzi (Floridi, 2010b). Faccio riferimento al loro input principale. Chiamerò sintetici i dati interamente generati dall’ia. Purtroppo, il termine ha un’etimologia ambigua, dal momento che ha iniziato a essere utilizzato negli anni Novanta in riferimento a dati storici resi anonimi prima di essere utilizzati, di regola per proteggere privacy e riservatezza. Questi dati sono sintetici solo nel senso che sono stati sintetizzati da dati storici, per esempio attraverso il “mascheramento”.3 Hanno una risoluzione inferiore, ma non sono generati da una fonte artificiale. La distinzione tra i dati storici e quelli da essi sintetizzati è utile, ma non è ciò che intendo in questo contesto, dove voglio sottolineare la provenienza interamente ed esclusivamente artificiale dei dati in questione. È una distinzione ontologica, che può avere importanti implicazioni in termini epistemologici, soprattutto quando è in gioco la nostra capacità di spiegare i dati sintetici prodotti e l’addestramento raggiunto dall’ia che li utilizza (Watson, Krutzinna, Bruce et al., 2019). Un celebre esempio può aiutare a spiegare la differenza.

			In passato, giocare a scacchi contro un computer significava giocare contro i migliori giocatori umani che avessero mai preso parte al gioco. Perciò, una delle caratteristiche di Deep Blue, il programma di scacchi della ibm che aveva sconfitto il campione del mondo Garry Kasparov, consisteva in

			un uso efficace di un database delle partite di un grande maestro. (Campbell, Hoane Jr, Hsu, 2002, p. 57)

			Ma AlphaZero, l’ultima versione del sistema di ia sviluppato da DeepMind, ha imparato a giocare meglio di chiunque altro, e in effetti di qualsiasi altro software, facendo affidamento soltanto sulle regole del gioco, senza alcun input di dati da alcuna fonte esterna. Non aveva alcuna memoria storica:

			Il gioco degli scacchi ha rappresentato l’apice della ricerca sull’intelligenza artificiale per diversi decenni. I programmi all’avanguardia si basano su potenti motori che ricercano molti milioni di posizioni, sfruttando competenze proprie dell’ambito degli scacchi e sofisticati adattamenti a esso [corsivo mio, questi sono i dati non sintetici]. AlphaZero è un generico algoritmo di ricerca e di apprendimento per rinforzo, ideato originariamente per il gioco Go, che ha ottenuto risultati superiori in poche ore […] senza alcuna conoscenza dell’ambito eccetto le regole degli scacchi. (Silver, Hubert, Schrittwieser et al., 2018, p. 1144, corsivo mio)

			AlphaZero ha imparato giocando contro se stesso, generando così i propri dati sintetici relativi agli scacchi. Non sorprende che il grande maestro di scacchi Matthew Sadler e la maestra internazionale Natasha Regan,

			che hanno analizzato migliaia di partite di scacchi di AlphaZero per il loro libro Game Changer (gennaio 2019), affermano che il suo stile è diverso da qualsiasi motore scacchistico tradizionale. “È come scoprire i taccuini segreti di un grande giocatore del passato”, dice Matthew.4

			AlphaZero ha generato i propri dati sintetici, e questo è stato sufficiente per il suo addestramento. Questo è ciò che intendo per dati sintetici.

			I dati realmente sintetici, nel senso in cui li ho definiti in questo contesto, hanno alcune straordinarie proprietà. Non solo condividono quelle elencate all’inizio del paragrafo (sono durevoli, riutilizzabili, rapidamente trasportabili, facilmente duplicabili, simultaneamente condivisibili senza fine ecc.). Sono anche puliti e affidabili (in termini di accuratezza), non violano privacy o riservatezza nella fase di sviluppo (sebbene i problemi persistano nella fase di implementazione, a causa di possibili violazioni della privacy su base predittiva (Crawford, Schultz, 2014), non sono immediatamente sensibili (la sensibilità durante la fase di implementazione è tuttora rilevante), se vengono persi non è un disastro perché possono essere ricreati e sono perfettamente formattati per essere utilizzati dal sistema che li genera. Con i dati sintetici l’ia non è mai costretta ad abbandonare il suo spazio digitale, dove può esercitare il controllo completo su qualsiasi input e output dei suoi processi. In termini più epistemologici, con i dati sintetici l’ia gode della posizione privilegiata della conoscenza del costruttore, che conosce la natura intrinseca e il funzionamento di qualcosa perché lo ha costruito (Floridi, 2018). Ciò spiega perché sono così popolari, per esempio, nei contesti di sicurezza, dove l’ia viene impiegata per testare i sistemi digitali. Inoltre, i dati sintetici possono essere anche prodotti, talvolta, in modo più rapido ed economico rispetto ai dati storici. AlphaZero è diventato il miglior giocatore di scacchi del globo in nove ore (ci sono volute dodici ore per Shogi e tredici giorni per Go).

			Tra dati storici più o meno mascherati (impoveriti attraverso una risoluzione inferiore, per esempio tramite l’anonimizzazione) e dati puramente sintetici, esiste una varietà di dati più o meno ibridi, che possiamo raffigurare come un prodotto di dati storici e sintetici. L’idea di base è utilizzare i dati storici per ottenere alcuni nuovi dati sintetici che non sono semplicemente dati storici impoveriti. Un buon esempio, introdotto da Goodfellow e coautori (2014), è fornito dalle reti generative avverse:

			Due reti neurali – un Generatore e un Discriminatore [mie le maiuscole nel testo] – competono l’una contro l’altra per avere successo in un gioco. Lo scopo del gioco per il Generatore è di trarre in inganno il Discriminatore con esempi che paiono simili al set di addestramento. […] Quando il Discriminatore rigetta un esempio prodotto dal Generatore, il Generatore impara qualcosa di più su come si presenta un buon esempio. […] In altri termini, il Discriminatore fa trapelare informazioni su quanto il Generatore fosse vicino e su come dovrebbe procedere per avvicinarsi. […] Col passare del tempo, il Discriminatore impara dal set di addestramento e invia segnali sempre più significativi al Generatore. Quando ciò si verifica, il Generatore si avvicina sempre di più all’apprendimento dell’aspetto degli esempi dal set di addestramento. Ancora una volta, gli unici input che il Generatore ha sono una distribuzione iniziale di probabilità (spesso la distribuzione normale) e l’indicatore che riceve dal Discriminatore. Non vede mai alcun esempio reale [corsivo mio].5

			Il Generatore impara a creare dati sintetici che sono equivalenti a dati di input conosciuti. Per questo c’è qui un po’ di natura ibrida, perché il Discriminatore deve avere accesso ai dati storici per “addestrare” il Generatore. Ma i dati generati dal Generatore sono nuovi, e non semplicemente un’astrazione a partire dai dati di addestramento. Pertanto, non si tratta di un caso di partenogenesi, come AlphaZero che dà alla luce i propri dati, ma vi si avvicina abbastanza da produrre comunque alcune delle caratteristiche molto interessanti dei dati sintetici. Per esempio, i volti umani sintetici creati da un Generatore non sollevano problemi in termini di privacy, consenso o riservatezza nella fase di sviluppo.6

			Molti metodi per generare dati ibridi o sintetici sono già disponibili o in fase di sviluppo, spesso con caratteristiche specifiche per settore. Esistono anche tendenze altruistiche per rendere pubblicamente disponibili tali insiemi di dati (Howe, Stoyanovich, Ping et al., 2017). Chiaramente, il futuro dell’ia non risiede soltanto nei “piccoli dati” ma anche, o forse principalmente, nella sua crescente capacità di generare i propri dati. Si tratterebbe di uno sviluppo notevole e ci si può aspettare che vengano compiuti sforzi significativi in tale direzione. La domanda seguente è: quale fattore può far spostare l’indicatore, nella Figura 3.1, da sinistra a destra?



			La differenza è costituita dal processo genetico, cioè dalle regole usate per creare i dati. I dati storici sono ottenuti tramite regole di registrazione, in quanto sono il risultato di osservazioni del comportamento di un sistema. I dati sintetizzati sono ottenuti tramite regole di astrazione, che eliminano, mascherano o offuscano alcuni gradi di risoluzione a partire dai dati storici, per esempio mediante l’anonimizzazione. Dati ibridi e realmente sintetici possono essere generati tramite regole vincolanti o costitutive. Non esiste una mappatura in scala uno a uno, ma è utile considerare i dati ibridi come i dati su cui dobbiamo fare affidamento, utilizzando regole vincolanti, quando non disponiamo di regole costitutive in grado di generare dati sintetici da zero. Occorre chiarire questo punto.

			L’indicatore si sposta facilmente verso i dati sintetici ogniqualvolta l’ia si occupa di “giochi” – intesi come qualsiasi interazione formale in cui i giocatori competono secondo regole e in vista del raggiungimento di un obiettivo – le cui regole sono costitutive e non semplicemente vincolanti. La differenza che intendo tracciare7 diventa chiara se si mettono a confronto gli scacchi e il calcio. Entrambi sono giochi, ma negli scacchi le regole stabiliscono le mosse valide e non valide prima che sia possibile una qualsiasi attività di tipo scacchistico; perciò, generano tutte e solo le mosse accettabili. Mentre nel calcio un’attività precedente – come calciare un pallone – è “regolamentata” o strutturata da regole che arrivano dopo l’attività. Le regole non determinano né possono determinare le mosse dei giocatori, ma pongono semplicemente limiti a quali mosse siano “valide”. Negli scacchi, come in tutti i giochi da tavolo le cui regole sono costitutive (Dama, Go, Monopoli, Shogi ecc.), l’ia può utilizzare le regole per provare qualsiasi possibile mossa valida che vuole esplorare. In nove ore, AlphaZero ha giocato 44 milioni di partite di addestramento. Per avere un’idea dell’entità del risultato si consideri che la Opening Encyclopedia 2018 contiene circa 6,3 milioni di partite, selezionate dall’intera storia degli scacchi. Ma nel calcio questo non avrebbe senso perché le regole non costituiscono il gioco ma si limitano a modellarlo. Ciò non significa che l’ia non possa giocare a calcio virtuale; contribuire a identificare la migliore strategia per vincere contro una squadra di cui siano registrati i dati relativi a partite e strategie precedenti; aiutare a identificare potenziali giocatori o contribuire ad allenarli meglio. Naturalmente, tutte queste applicazioni sono ora banalmente fattibili e già presenti. Quello che voglio dire è che in tutti questi casi sono richiesti dati storici. Invece, quando

			1. 	un processo o un’interazione può essere trasformata in un gioco e

			2. 	il gioco può essere trasformato in un gioco formato da regole costitutive, allora

			3. 	l’ia sarà in grado di generare i propri dati, completamente sintetici, ed essere il miglior “giocatore” su questo pianeta, realizzando ciò che ha fatto AlphaZero con gli scacchi (questo processo è parte dell’avvolgimento descritto nel secondo capitolo).

			Per dirla con Wiener:

			Il miglior modello materiale di un gatto è un altro, o preferibilmente lo stesso, gatto. (Rosenblueth, Wiener, 1945, p. 316)

			Idealmente, i dati migliori su cui addestrare un’ia sono dati completamente storici o dati completamente sintetici generati dalle stesse regole che hanno generato i dati storici. In qualsiasi gioco da tavolo, questo accade per impostazione predefinita. Ma nella misura in cui uno qualsiasi dei due passaggi (1)-(2), di cui sopra, sia difficile da raggiungere, è probabile che l’assenza di regole o la presenza di regole meramente vincolanti rappresentino un limite. Non abbiamo il gatto vero e proprio, ma solo un suo modello più o meno affidabile. Le cose possono diventare più complicate quando ci rendiamo conto che, nei giochi reali, le regole vincolanti sono semplicemente imposte convenzionalmente su un’attività che si è verificata in precedenza, mentre nella vita reale, quando osserviamo alcuni fenomeni, per esempio il comportamento di una tipologia di tumore in uno specifico insieme di pazienti in determinate circostanze, le regole genetiche devono essere estratte dal “gioco” reale attraverso la ricerca scientifica (oggi possibilmente basata sull’ia). Per esempio, non conosciamo e forse non conosceremo mai quali siano le esatte “regole” che presiedono allo sviluppo dei tumori cerebrali. Disponiamo di alcuni principi e teorie generali in base ai quali comprendiamo il loro sviluppo. Perciò, in questa fase (che potrebbe essere una fase permanente), non c’è modo di “ludicizzare” (cioè trasformare in un gioco nel senso già specificato; evito l’espressione “gamificare” che ha una diversa e consolidata accezione) tumori cerebrali in un “gioco di regole costitutive” (si pensi agli scacchi), in modo tale che un sistema di ia, giocando secondo le regole identificate, possa generare propri dati sintetici sui tumori cerebrali che sarebbero equivalenti ai dati storici che potremmo raccogliere, facendo per tali tumori quanto AlphaZero ha fatto per le partite di scacchi. Ciò non è necessariamente un problema. Al contrario l’ia, basandosi su dati storici o ibridi (per esempio, scansioni cerebrali) e apprendendo da essi, può ottenere risultati ancora migliori di quelli degli esperti ed espandere le proprie capacità oltre i set finiti di dati storici esistenti (per esempio, scoprendo nuovi modelli di correlazioni) o fornire servizi accessibili dove difettano le competenze. È già un grande successo se si può estrarre un numero sufficiente di regole vincolanti per produrre dati affidabili in silico. Tuttavia, senza un sistema affidabile di regole costitutive, alcuni dei vantaggi sopra menzionati dei dati sintetici non sarebbero pienamente disponibili. La vaghezza di questa affermazione è dovuta al fatto che possiamo ancora utilizzare dati ibridi.

			La ludicizzazione e la presenza o assenza di regole vincolanti/costitutive non sono rigidi limiti mutualmente esclusivi. Non dimentichiamo che i dati ibridi possono contribuire a sviluppare dati sintetici. È probabile che, in futuro, diventerà sempre più chiaro quando database di dati storici di alta qualità risultino essere assolutamente necessari e inevitabili, cioè quando è indispensabile il gatto reale, per parafrasare Wiener, e pertanto quando dovremo gestire questioni relative a disponibilità, accessibilità, rispetto delle norme giuridiche e, nel caso di dati personali, privacy, consenso, sensibilità e altre questioni etiche. Tuttavia, la tendenza verso la generazione di dati quanto più possibile sintetici (sintetizzati, più o meno ibridi, fino a diventare completamente sintetici) è probabilmente un Sacro Graal dell’ia. Per questo mi aspetto che la comunità dell’ia spinga in modo convinto in quella direzione: il modello del gatto senza il gatto, per riferirsi ancora una volta all’immagine di Wiener. Generare sempre più dati non storici, spostando il più possibile l’indicatore verso destra, richiederà una “ludicizzazione” dei processi, e per tale motivo mi aspetto anche che la comunità dell’ia sia sempre più interessata all’industria dei giochi, perché è lì che probabilmente si trovano le migliori competenze in materia di “ludicizzazione”. Inoltre, in termini di risultati negativi, le prove matematiche dell’impossibilità di ludicizzare intere tipologie o aree di processi o interazioni dovrebbero essere le benvenute al fine di chiarire dove o fino a che punto un approccio del tipo di AlphaZero potrebbe non essere mai realizzabile dall’ia.





3.3 Problemi difficili, problemi complessi e il bisogno di avvolgimento


			Nel secondo capitolo, ho affermato che probabilmente è preferibile comprendere l’ia come una riserva di capacità di agire che può essere usata per risolvere problemi ed eseguire compiti con successo. L’ia consegue i propri obiettivi scindendo la capacità di eseguire un compito con successo da qualsiasi esigenza di essere intelligente nel farlo. L’app nel mio cellulare non deve essere intelligente per giocare a scacchi meglio di me. Ogni volta che questa scissione è realizzabile, in linea di principio diventa possibile qualche soluzione di ia. Questa è la ragione per cui comprendere il futuro dell’ia significa anche comprendere la natura dei problemi in relazione ai quali tale scissione può essere tecnicamente realizzabile, almeno in teoria, ed economicamente attuabile nella pratica. Ora, molti dei problemi che cerchiamo di risolvere con l’ia si verificano nel mondo fisico: dalla guida alla scansione di etichette in un supermercato, dalla pulizia di pavimenti o finestre al taglio dell’erba in giardino. Perciò, il lettore potrebbe pensare all’ia in termini di robotica nel resto di questo paragrafo. Tuttavia, non sto parlando solo di robotica: per esempio, anche applicazioni smart per allocare prestiti o interfacce smart che facilitano e migliorano le interazioni con l’Internet delle Cose sono oggetto dell’analisi. Con ciò intendo suggerire che, allo scopo di comprendere gli sviluppi dell’ia in relazione ad ambienti analogici e digitali, è utile mappare i problemi in base alle risorse che sono necessarie per risolverli e capire in che misura l’ia può disporre di tali risorse. Mi riferisco alle risorse computazionali e, pertanto, ai gradi di complessità; e alle risorse relative alle abilità e, pertanto, ai gradi di difficoltà.

			I gradi di complessità di un problema sono ben noti e ampiamente studiati nella teoria computazionale (Arora, Barak, 2009; Sipser, 2012). Non dirò molto su questa dimensione ma mi limito a rimarcare che è altamente quantitativa e che la trattabilità matematica che offre è dovuta alla disponibilità di criteri standard di confronto, forse anche idealizzati ma chiaramente definiti, come le risorse computazionali di una macchina di Turing. Se disponiamo di un “metro”, possiamo misurare le lunghezze. Parimenti, se adottiamo una macchina di Turing come punto di partenza, possiamo calcolare quanto tempo, in termini di passaggi, e quanto spazio, in termini di memoria o nastro, “consuma” un problema computazionale per essere risolto. Per motivi di semplicità e tenendo presente che, se necessario, è possibile ottenere gradi di precisione finemente granulari e sofisticati utilizzando strumenti della teoria della complessità, conveniamo di mappare la complessità di un problema (trattato dall’ia in termini di spazio-tempo = memoria e passaggi richiesti) da 0 (semplice) a 1 (complesso).

			I gradi di difficoltà di un problema, intesi in termini di abilità richieste per risolverlo, dall’accendere e spegnere una luce a stirare delle camicie, richiedono in questo caso qualcosa di più di una stipulazione per essere mappati, perché di solito la letteratura pertinente, per esempio, sullo sviluppo motorio umano, non si concentra su una tassonomia dei problemi basata sulle risorse necessarie, ma su una tassonomia basata sulla valutazione delle prestazioni degli agenti umani e delle loro capacità o abilità dimostrate nella risoluzione di un problema o nell’esecuzione di un compito. Si tratta anche di una letteratura più qualitativa. In particolare, ci sono molte maniere per valutare una prestazione e quindi svariati modi per catalogare i problemi relativi alle abilità, ma una distinzione standard è tra abilità motorie grossolane e fini. Le abilità grosso-motorie richiedono l’uso di grandi gruppi muscolari, per eseguire attività come camminare o saltare, prendere o calciare una palla. Le abilità motorie fini richiedono l’uso di gruppi muscolari più piccoli, nei polsi, nelle mani, nelle dita, nei piedi e nelle dita dei piedi, per svolgere compiti come lavare i piatti, scrivere, digitare, usare o suonare uno strumento. Nonostante le precedenti difficoltà, possiamo riconoscere immediatamente che abbiamo a che fare con diversi gradi di difficoltà. Ancora una volta, per motivi di semplicità e ricordando che anche in questo caso gradi di precisione finemente granulari e sofisticati possono essere ottenuti, se necessario, utilizzando strumenti della psicologia dello sviluppo, conveniamo di mappare la difficoltà di un problema (trattato dall’ia in termini di abilità richieste) da 0 = facile, a 1 = difficile. Siamo adesso pronti per mappare le due dimensioni, nella Figura 3.2, dove ho aggiunto quattro esempi.



			Accendere la luce è un problema la cui soluzione ha un grado molto basso di complessità (pochissimi passaggi e stati) e di difficoltà (anche un bambino può farlo). Tuttavia, allacciarsi le scarpe richiede capacità motorie avanzate, così come allacciare quelle altrui, per cui si tratta di un’attività con bassa complessità (facile), ma che richiede un’abilità elevata (difficile). Come ha osservato il ceo di Adidas Kasper Rørsted nel 2017:

			La sfida più grande per il settore calzaturiero è come creare un robot che inserisca il laccio nella scarpa. Non sto scherzando. Oggi è un processo interamente manuale. Non esiste una tecnologia per questo.8

			Lavare i piatti è l’opposto: può richiedere molti passaggi e spazio, anzi tanti più quanti più sono i piatti da lavare, ma non è difficile (persino un filosofo come me può farlo). E, naturalmente, in alto a destra troviamo lo stirare le camicie, che è sia un’attività che consuma risorse, come lavare i piatti, sia un’attività impegnativa in termini di abilità. Si tratta, dunque, di un’attività al contempo complessa e difficile, che è di regola la mia scusa per cercare di evitarla. Usando gli esempi precedenti del calcio e degli scacchi, è possibile dire che giocare a calcio è semplice ma difficile, mentre giocare a scacchi è facile (puoi imparare le regole in pochi minuti) ma molto complesso: ecco perché l’ia può vincere contro chiunque a scacchi, ma una squadra di androidi che vinca la coppa del mondo di calcio è fantascienza.

			Abbiamo visto che avvolgere significa adattare l’ambiente e le attività alle capacità dell’ia. Più sofisticate sono queste capacità, meno necessario è l’avvolgimento, ma stiamo cercando un compromesso, una sorta di equilibrio, tra robot in grado di cucinare9 e robot in grado di cucinare gli hamburger.10 Parimenti, in un aeroporto, che è un ambiente altamente controllato e perciò più facilmente “avvolgibile”, una navetta potrebbe essere un veicolo autonomo, ma sembra improbabile modificare lo scuolabus di una cittadina, dato che il suo autista deve essere in grado di operare in circostanze estreme e difficili (campagna, neve, nessun segnale, nessuna copertura satellitare ecc.), prendersi cura dei bambini, aprire porte, spostare biciclette ecc., tutte attività che sono più improbabili (attenzione, non logicamente impossibili; su questa distinzione si dirà di più nel decimo capitolo) da avvolgere.

			In una prospettiva simile, nel 2016, Nike ha lanciato HyperAdapt 1.0, le sue scarpe automatiche elettroniche che si allacciano da sole, non sviluppando un’ia che le allacci al posto nostro, ma reinventando il concetto di cosa significa adattare le scarpe ai piedi: ogni scarpa ha un sensore, una batteria, un motore e un sistema di cavi che, insieme, possono regolare l’adattamento tramite un’equazione algoritmica di pressione. Succedono cose strane quando il software non funziona correttamente.11

			Potrebbero esserci problemi, e quindi compiti relativi che li risolvono, che non si prestano facilmente a essere avvolti. Eppure qui non si tratta di prove, ma piuttosto di ingegno, costi economici, esperienze e preferenze dell’utente o del cliente. Per esempio, un robot che stira le camicie può essere progettato. Nel 2012, un team dell’Università Carlos iii di Madrid, in Spagna, ha costruito teo, un robot che pesa circa 80 chili ed è alto un metro e ottanta. teo può salire le scale, aprire le porte e, più recentemente, ha mostrato di essere in grado di stirare le camicie (Estevez, Victores, Fernandez-Fernandez et al., 2017), anche se occorre appoggiare gli indumenti sull’asse da stiro e poi ritirarli. L’opinione, abbastanza diffusa, è che:

			“teo è costruito per fare ciò che fanno gli esseri umani come lo fanno gli esseri umani”, afferma Juan Victores, membro del team dell’Università Carlos iii di Madrid. Lui e i suoi colleghi vogliono che teo sia in grado di eseguire altri compiti domestici, come dare una mano in cucina. Il loro obiettivo finale è di far sì che teo sia in grado di imparare a svolgere un compito semplicemente osservando persone senza specifiche competenze che l’eseguono. “Avremo robot come teo nelle nostre case. È soltanto questione di chi lo farà per primo”, afferma Victores. (Ibidem)

			Come è intuibile, penso che questo sia esattamente l’opposto di ciò che è destinato ad accadere. Dubito fortemente che questo sia il futuro. Si tratta di una visione che non riesce a cogliere la distinzione tra compiti difficili e complessi, e l’enorme vantaggio di avvolgere i compiti per renderli facili (con una difficoltà molto bassa), per quanto altamente complessi. Non dimentichiamoci del fatto che non stiamo costruendo veicoli autonomi mettendo i robot al posto di guida, ma al contrario stiamo ripensando l’intero ecosistema di veicoli e ambienti, rimuovendo del tutto il posto di guida. Perciò, se la mia analisi è corretta, il futuro dell’ia non è popolato da androidi simili a teo che imitano il comportamento umano, ma è rappresentato più probabilmente da Effie,12 Foldimate13 e altre simili macchine automatiche domestiche che asciugano e stirano i vestiti. Non sono androidi, come teo, ma sistemi simili a scatole che possono essere piuttosto sofisticati dal punto di vista computazionale. Assomigliano più a lavastoviglie e lavatrici, con la differenza che, nei loro ambienti avvolti, il loro input è costituito da vestiti stropicciati e il loro output da vestiti stirati. Forse macchine simili saranno costose, forse non funzioneranno sempre nel modo desiderato, forse potrebbero essere implementate in modi che non riusciamo a immaginare ora, ma è percepibile che si tratti della logica corretta. Stiamo trasformando che cosa sia e come appaia un tagliaerba, non stiamo costruendo androidi che spingono il mio vecchio tagliaerba in giro come farei io. La lezione? Non dobbiamo cercare di imitare gli esseri umani attraverso l’ia. Dobbiamo sfruttare, invece, ciò che le macchine, inclusa l’ia, fanno meglio. La difficoltà è nemica delle macchine, la complessità il loro alleato: per questo, occorre avvolgere il mondo che le circonda, disegnare nuove forme di implementazione per incorporarle con successo nel loro involucro. A quel punto diventerà ragionevole ottenere una serie di perfezionamenti progressivi, dimensioni di mercato, marketing e prezzi adeguati, seguiti da nuovi miglioramenti.





3.4 Il design come futuro dell’ia


			I due futuri che ho delineato in questa sede sono complementari e basati sulla nostra attuale e prevedibile comprensione dell’ia. Ci sono incognite sconosciute, ovviamente, ma tutto quello che si può dire al loro riguardo è esattamente questo: esistono, ma non ne sappiamo alcunché. È un po’ come dire che sappiamo che vi sono domande che non ci stiamo ponendo, ma non siamo in grado di dire quali siano. Il futuro dell’ia è pieno di incognite sconosciute. Ciò che ho cercato di fare in questo capitolo è “scrutare nei semi del tempo” che abbiamo già seminato. Mi sono concentrato sulla natura dei dati e dei problemi perché i primi consentono all’ia di funzionare e i secondi delineano i confini entro i quali l’ia può operare con successo. A questo livello di astrazione, due inferenze appaiono molto plausibili. Cercheremo di sviluppare l’ia utilizzando dati il più possibile ibridi e preferibilmente sintetici, attraverso un processo di ludicizzazione di interazioni e compiti. In altri termini, tenteremo di allontanarci dai dati puramente storici, per quanto possibile. In settori come la sanità e l’economia, è ben possibile che i dati storici o al massimo ibridi restino necessari, a causa della differenza tra regole vincolanti e costitutive. Faremo tutto questo traducendo il più possibile problemi difficili in problemi complessi, attraverso l’avvolgimento della realtà attorno alle capacità dei nostri artefatti. In sintesi, cercheremo di creare dati ibridi o sintetici per affrontare problemi complessi, ludicizzando compiti e interazioni in ambienti avvolti. Quanto più questo è possibile, tanto più l’ia avrà successo. Questo è il motivo per cui una tendenza come lo sviluppo delle città gemelle digitali è molto interessante. Il che mi porta ad altri due commenti.

			Ludicizzare e avvolgere sono questione di disegnare, e talvolta ridisegnare, le realtà con cui ci confrontiamo (Floridi, 2019c). Pertanto, il futuro prevedibile dell’ia dipenderà dalle nostre capacità di design e ingegno. Dipenderà anche dalla nostra capacità di negoziare le derivanti (e serie) questioni etiche, giuridiche e sociali, dalle nuove forme di privacy (tramite proxy [cioè dati vicarianti], predittiva o di gruppo) alle forme di nudging (spinta gentile) e autodeterminazione. Abbiamo visto nel secondo capitolo che l’idea stessa di plasmare sempre più i nostri ambienti (analogici o digitali) per renderli adatti all’ia dovrebbe far riflettere chiunque. Prevedere tali questioni, per facilitarne la soluzione ed evitarne o mitigarne gli aspetti negativi, è il vero valore di qualsiasi analisi rivolta al futuro. È interessante cercare di capire quali possano essere i percorsi più probabili nell’evoluzione dell’ia. Tuttavia, sarebbe piuttosto sterile tentare di prevedere “quale grano crescerà e quale no” e poi non fare nulla per garantire che i chicchi buoni crescano e quelli cattivi no (Floridi, 2014d). Il futuro non è del tutto aperto perché il passato lo plasma, ma non è neppure del tutto determinato, perché il passato può essere indirizzato in una direzione diversa. Ecco perché la sfida che ci attende non sarà tanto quella dell’innovazione digitale di per sé, quanto piuttosto quella della governance del digitale, compresa l’ia.





3.5 Conclusione: l’ia e le sue stagioni


			Il problema con le metafore stagionali è che sono cicliche. Se diciamo che l’ia ha trascorso un brutto inverno, dobbiamo anche ricordarci che l’inverno farà ritorno, ed è meglio farsi trovare pronti. L’inverno dell’ia è quella fase in cui la tecnologia, gli affari e i media escono dalla loro calda e confortevole bolla, si raffreddano, temperano le loro speculazioni fantascientifiche e le loro esagerazioni irragionevoli, e fanno i conti con ciò che l’ia può o non può davvero fare come tecnologia (Floridi, 2019d), in modo misurato. Gli investimenti diventano più attenti e i giornalisti smettono di scrivere di ia, per inseguire altri temi in voga e alimentare la moda seguente.

			L’ia ha conosciuto diversi inverni.14 Tra i più rilevanti, ce n’è stato uno alla fine degli anni Settanta e un altro a cavallo degli anni Ottanta e Novanta. Oggi parliamo di un altro prevedibile inverno (Nield, 2019; Walch, 2019; Schuchmann, 2019).15 L’ia è soggetta a questi cicli di esagerazioni perché è una speranza o una paura che abbiamo nutrito da quando siamo stati cacciati dal paradiso: qualcosa che fa tutto per noi, al nostro posto, meglio di noi, con tutti i vantaggi sognati (saremo in vacanza per sempre) e i rischi paventati (saremo ridotti in schiavitù) che ne derivano. Per alcune persone, speculare su tutto questo è irresistibile. È il selvaggio West delle “ipotesi” e degli scenari del “come se”. Ma spero che il lettore mi perdonerà, se mi permetterò di dire: “Te l’avevo detto”. Da tempo ho messo in guardia contro commentatori ed “esperti” che facevano a gara per vedere chi la raccontava più grossa (Floridi, 2016d). Ne è seguita una pletora di miti. Parlavano dell’ia come se fosse l’ultima panacea, che avrebbe risolto e superato tutto; o come la catastrofe finale, una superintelligenza che avrebbe distrutto milioni di posti di lavoro, sostituendo avvocati e medici, giornalisti e ricercatori, camionisti e tassisti, e finendo per dominare gli esseri umani come se fossero animali domestici.

			Molti hanno seguito Elon Musk nel dichiarare che lo sviluppo dell’ia rappresentava il più grande rischio esistenziale corso dall’umanità. Come se la maggior parte dell’umanità non vivesse nella miseria e nella sofferenza. Come se guerre, carestie, inquinamento, riscaldamento globale, ingiustizia sociale e fondamentalismo fossero fantascienza o solo seccature trascurabili, che non meritano considerazione. Oggi, la pandemia da Covid-19 ha posto fine a queste sciocche affermazioni. Alcuni hanno insistito sul fatto che leggi e regolamenti giungerebbero sempre troppo tardi senza tenere mai il passo dell’ia, quando in realtà le norme non riguardano il ritmo ma la direzione dell’innovazione, poiché dovrebbero guidare il corretto sviluppo di una società. Se ci piace dove stiamo andando, possiamo andarci alla velocità che vogliamo. È a causa della nostra mancanza di visione che abbiamo paura. Oggi sappiamo che una normativa è in corso di elaborazione, almeno nell’Unione Europea. Altri (non necessariamente diversi dai precedenti) hanno affermato che l’ia fosse una scatola nera magica che non potremmo mai spiegare, quando in realtà si tratta di individuare il corretto livello di astrazione al quale interpretare le complesse interazioni ingegnerizzate: anche il traffico automobilistico nel centro di una città diventa una scatola nera se pretendiamo di scoprire perché ogni singolo individuo si trovi lì in quel momento. Oggigiorno, c’è un crescente sviluppo di strumenti adeguati per monitorare e capire come i sistemi di apprendimento automatico (ml) raggiungono i loro risultati (Watson, Krutzinna, Bruce et al., 2019; Watson, Floridi, 2020; Watson, Gultchin, Taly et al., 2021). Inoltre, tali persone diffondono scetticismo sulla possibilità di delineare un quadro etico che sintetizzi ciò che intendiamo per ia socialmente buona, laddove in realtà la ue, l’ocse e la Cina convergono su principi molto simili, che offrono una piattaforma comune per ulteriori accordi, come vedremo nel quarto capitolo. Si tratta di irresponsabili in cerca di titoli. Dovrebbero vergognarsi e chiedere scusa. Non solo per i loro commenti insostenibili, ma anche per la grande trascuratezza e l’allarmismo, che hanno tratto in inganno l’opinione pubblica sia su una tecnologia potenzialmente utile – che può fornire e difatti fornisce soluzioni utili, dalla medicina ai sistemi di sicurezza e monitoraggio (Taddeo, Floridi, 2018a) – sia sui rischi reali, che sappiamo essere concreti ma molto meno fantasiosi, dalla manipolazione quotidiana delle scelte (Milano, Taddeo, Floridi, 2019, 2020) all’aumento della pressione sulla privacy individuale e di gruppo (Floridi, 2014c), dai conflitti informatici all’uso dell’ia da parte della criminalità organizzata per riciclaggio di denaro e furto di identità, come vedremo nei capitoli settimo e ottavo.

			Il rischio insito in ogni estate dell’ia è che le aspettative esagerate si trasformino in una distrazione di massa. Il rischio insito in ogni inverno dell’ia è che il contraccolpo sia eccessivo, la delusione troppo profonda, cosicché soluzioni potenzialmente preziose vengono buttate via con l’acqua delle illusioni. Gestire il mondo è un compito sempre più complesso: le megalopoli e la loro “trasformazione in città smart” ne sono un buon esempio. Inoltre, siamo posti a confronto con problemi planetari – pandemie, cambiamenti climatici, ingiustizie sociali, migrazioni – che richiedono livelli di coordinamento sempre più elevati per essere risolti. Abbiamo bisogno naturalmente di tutta la buona tecnologia che possiamo disegnare, sviluppare e implementare per affrontare queste sfide, e di tutta l’intelligenza umana che possiamo esercitare per mettere tale tecnologia al servizio di un futuro migliore. L’ia può svolgere un ruolo importante in tutto questo perché abbiamo bisogno di modalità sempre più intelligenti per elaborare immense quantità di dati, in maniera efficiente, efficace, sostenibile ed equa. Ma l’ia deve essere trattata come una normale tecnologia, non come un miracolo né come una piaga, bensì come una delle tante soluzioni che l’ingegno umano è riuscito a escogitare. Questa è anche la ragione per cui il dibattito etico resta sempre una questione interamente umana.

			Ora che il nuovo inverno potrebbe arrivare, possiamo provare a imparare alcune lezioni ed evitare di allignare in questo andirivieni di illusioni irragionevoli e disillusioni esagerate. Non dimentichiamo che l’inverno dell’ia non dovrebbe essere l’inverno delle sue opportunità. Certamente non sarà l’inverno dei suoi rischi o delle sue sfide. Dobbiamo chiederci se le soluzioni di ia rimpiazzeranno davvero le soluzioni precedenti, come ha fatto l’automobile con la carrozza, le diversificheranno, come ha fatto la moto con la bicicletta, o le integreranno ed espanderanno, come ha fatto lo smartwatch digitale con l’orologio analogico. Quale sarà il livello di sostenibilità, accettabilità sociale o preferibilità di ogni ia che emergerà in futuro, forse dopo un nuovo inverno? Indosseremo davvero degli strani occhiali per vivere in un mondo virtuale o aumentato creato e abitato da sistemi di ia? Oggi molte persone sono restie a indossare occhiali anche quando ne hanno davvero bisogno, solo per motivi estetici. E poi ci sono soluzioni ia fattibili nella vita di tutti i giorni? Sono disponibili le competenze, gli insiemi di dati, l’infrastruttura e i modelli di business necessari per garantire il successo dell’implementazione dell’ia? I futurologi trovano queste domande noiose. A loro piace un’idea unica, semplice, che interpreta e cambia tutto, che può essere dispiegata con leggerezza in un libro facile che fa sentire il lettore intelligente, un libro che deve essere letto da tutti oggi e ignorato da tutti domani. È la cattiva alimentazione del cibo spazzatura per i pensieri e la maledizione del bestseller da aeroporto. Dobbiamo resistere all’eccesso di semplificazione. Questa volta, dobbiamo pensare in modo più approfondito ed esteso a ciò che stiamo facendo e pianificando con l’ia. Questo esercizio si chiama filosofia, non futurologia. Ed è ciò a cui spero di contribuire nella seconda parte di questo libro.